{"cells":[{"cell_type":"markdown","metadata":{"id":"8-OyeUC3W-nh"},"source":["# Modern Optimization: SOAP â€” Student Notebook\n","\n","Deep Learning optimization is moving beyond scalar-adaptive methods like **Adam** and **AdamW**. While these optimizers scale updates based on the magnitude of gradients (element-wise), they ignore the *correlations* between parameters.\n","\n","**SOAP (ShampoO with Adam in the Preconditioner's eigenbasis)** belongs to a family of matrix-oriented optimizers (like Shampoo) that attempt to capture the geometry of the loss landscape more accurately.\n","\n","The core insight of SOAP is to perform updates in the **eigenbasis** of the gradient correlations. This allows the optimizer to \"rotate\" the gradient into a coordinate system where the parameters are decoupled, apply adaptive scaling, and then rotate back.\n","\n","## Learning Objectives\n","- Understand the concept of **Eigenbasis Rotation** in optimization.\n","- Implement the core mathematical operation of SOAP: **Gradient Projection**.\n","- Integrate this projection into a working optimizer.\n","- Compare SOAP against AdamW on a Recurrent Neural Network task."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qo5CRu48W-ni","executionInfo":{"status":"ok","timestamp":1764902056564,"user_tz":480,"elapsed":10786,"user":{"displayName":"Winnie the Xi","userId":"11308985818502310829"}},"outputId":"e4673ccd-050a-4b8c-fca9-e10b7ad63af1"},"source":["import torch\n","import torch.nn as nn\n","import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from typing import Tuple, Optional, Callable\n","\n","# Set random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"markdown","metadata":{"id":"-RV0039NW-ni"},"source":["# Part 1: Gradient Rotation in SOAP\n","\n","In standard Adam, we divide the gradient by $\\sqrt{v_t}$, where $v_t$ is the squared gradient history. This is effectively a diagonal approximation of the curvature.\n","\n","In SOAP, we maintain preconditioners (approximations of the Hessian/Covariance) $L$ and $R$. To update the weights correctly, we need to:\n","1.  Compute the eigenvectors of these preconditioners: $Q_L$ and $Q_R$.\n","2.  **Project** the gradient $G$ into this eigenbasis.\n","3.  Perform the update (scaling).\n","4.  Project back.\n","\n","### Task 1: Implement Gradient Projection\n","Your task is to implement the projection step.\n","\n","Given:\n","- Gradient $G$ of shape $(M \\times N)$\n","- Left Eigenvectors $Q_L$ of shape $(M \\times M)$\n","- Right Eigenvectors $Q_R$ of shape $(N \\times N)$\n","\n","The projection formula is:\n","$$ G_{projected} = Q_L^T \\cdot G \\cdot Q_R $$\n","\n","*(Note: In PyTorch, `torch.matmul` or the `@` operator handles matrix multiplication)*"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"sY6v7kJhW-ni","executionInfo":{"status":"ok","timestamp":1764902060808,"user_tz":480,"elapsed":5,"user":{"displayName":"Winnie the Xi","userId":"11308985818502310829"}}},"source":["class MockSOAPContext:\n","    \"\"\"A simplified context to test your rotation logic.\"\"\"\n","    def __init__(self, m, n):\n","        # Random orthogonal matrices to simulate eigenvectors\n","        self.QL = torch.randn(m, m)\n","        self.QR = torch.randn(n, n)\n","        self.QL, _ = torch.linalg.qr(self.QL)\n","        self.QR, _ = torch.linalg.qr(self.QR)\n"],"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"S3Hi3id8W-nj","executionInfo":{"status":"ok","timestamp":1764902064241,"user_tz":480,"elapsed":6,"user":{"displayName":"Winnie the Xi","userId":"11308985818502310829"}}},"source":["def project_gradient(grad, Q_L, Q_R):\n","    \"\"\"\n","    Projects the gradient G into the eigenbasis defined by Q_L and Q_R.\n","\n","    Args:\n","        grad: Tensor of shape (M, N)\n","        Q_L: Tensor of shape (M, M) (Left Eigenvectors)\n","        Q_R: Tensor of shape (N, N) (Right Eigenvectors)\n","\n","    Returns:\n","        projected_grad: Tensor of shape (M, N)\n","    \"\"\"\n","    # TODO: Implement G_proj = Q_L^T @ G @ Q_R\n","    # Remember: transpose in PyTorch is .t() or .T\n","\n","    # YOUR CODE HERE\n","    pass\n"],"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91RoCLuqW-nj"},"source":["# Verify Dimensions\n","M, N = 10, 20\n","ctx = MockSOAPContext(M, N)\n","grad = torch.randn(M, N)\n","\n","try:\n","    proj = project_gradient(grad, ctx.QL, ctx.QR)\n","\n","    if proj is not None:\n","        print(f\"Original shape: {grad.shape}\")\n","        print(f\"Projected shape: {proj.shape}\")\n","        assert proj.shape == (M, N), f\"Shape mismatch! Expected ({M}, {N}), got {proj.shape}\"\n","        print(\"Verification Passed!\")\n","    else:\n","        print(\"Function returned None. Please implement the logic.\")\n","except Exception as e:\n","    print(f\"Error: {e}\")"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-3njAieFW-nj"},"source":["## The SimpleSOAP Optimizer\n","\n","Below is a simplified implementation of a SOAP-like optimizer. It calculates the running statistics of $G G^T$ and $G^T G$ to find the eigenvectors, then uses **your** `project_gradient` function to rotate the update.\n","\n","*You do not need to modify this cell, but read it to understand how your code is used.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_XJhxNhiW-nj"},"source":["class SimpleSOAP(torch.optim.Optimizer):\n","    \"\"\"A simplified SOAP implementation for demonstration purposes.\"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.95), weight_decay=0.01, precondition_frequency=10):\n","        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay,\n","                        precondition_frequency=precondition_frequency)\n","        super(SimpleSOAP, self).__init__(params, defaults)\n","\n","    @torch.no_grad()\n","    def step(self, closure=None):\n","        loss = None\n","        if closure is not None:\n","            with torch.enable_grad():\n","                loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None: continue\n","                grad = p.grad\n","                state = self.state[p]\n","\n","                # Initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    state['exp_avg'] = torch.zeros_like(p)\n","                    # For simplicity, we only precondition 2D weights (matrices)\n","                    if p.ndim == 2:\n","                        M, N = p.shape\n","                        state['Q_L'] = torch.eye(M, device=p.device)\n","                        state['Q_R'] = torch.eye(N, device=p.device)\n","\n","                state['step'] += 1\n","\n","                # Update momentum (standard Adam-style)\n","                beta1, beta2 = group['betas']\n","                exp_avg = state['exp_avg']\n","                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n","\n","                # --- SOAP LOGIC ---\n","                if p.ndim == 2:\n","                    # Periodic Eigendecomposition (Simulated for this toy example)\n","                    if state['step'] % group['precondition_frequency'] == 0:\n","                        # In real SOAP, we accumulate G*G^T. Here we just take SVD of current grad\n","                        # to simulate finding the 'correct' basis for the current landscape\n","                        try:\n","                            U, S, Vh = torch.linalg.svd(grad, full_matrices=False)\n","                            state['Q_L'] = U\n","                            state['Q_R'] = Vh.T\n","                        except:\n","                            pass # Fallback if SVD fails\n","\n","                    # USE STUDENT FUNCTION\n","                    # Rotate momentum into eigenbasis\n","                    proj_update = project_gradient(exp_avg, state['Q_L'], state['Q_R'])\n","\n","                    # Apply scale (simplified adaptive element-wise in eigenbasis)\n","                    # In full SOAP, this involves Shampoo-style preconditioning\n","                    # Here we just pass through for the structural rotation effect\n","\n","                    # Rotate back\n","                    # update = Q_L @ proj_update @ Q_R^T\n","                    update = state['Q_L'] @ proj_update @ state['Q_R'].T\n","                else:\n","                    # Fallback for 1D vectors (biases)\n","                    update = exp_avg\n","\n","                # Weight Decay\n","                if group['weight_decay'] != 0:\n","                    p.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                # Step\n","                p.add_(update, alpha=-group['lr'])\n","\n","        return loss"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_5JL1lLW-nk"},"source":["# Part 2: Comparison on RNNs (The Adding Problem)\n","\n","We will now compare **AdamW** against our **SimpleSOAP** implementation on the \"Adding Problem\", a standard stress test for RNNs.\n","\n","**The Adding Problem**:\n","- Input: Sequence of pairs $(x_t, i_t)$. $x_t$ is a random number, $i_t$ is a binary mask.\n","- Target: Sum of $x_t$ where $i_t = 1$. Only two entries have $i_t=1$.\n","- Difficulty: The network must remember information over long durations.\n","\n","### Task 2: Training Loop and Comparison\n","Complete the training loop below. We will run the experiment for both optimizers across a range of learning rates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGouKaZyW-nk"},"source":["# --- PROVIDED UTILS ---\n","class SimpleRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super().__init__()\n","        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        out, _ = self.rnn(x)\n","        # Use last hidden state\n","        return self.fc(out[:, -1, :])\n","\n","def get_adding_problem_data(batch_size=64, seq_len=20):\n","    x = torch.rand(batch_size, seq_len, 2)\n","    x[:, :, 1] = 0\n","    indices = torch.randint(0, seq_len, (batch_size, 2))\n","    for i in range(batch_size):\n","        x[i, indices[i, 0], 1] = 1\n","        x[i, indices[i, 1], 1] = 1\n","    y = (x[:, :, 0] * x[:, :, 1]).sum(dim=1).unsqueeze(1)\n","    return x.to(device), y.to(device)"],"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCwSKgXWW-nk"},"source":["def train_experiment(optimizer_name, lr, steps=500):\n","    model = SimpleRNN(input_size=2, hidden_size=32, output_size=1).to(device)\n","    criterion = nn.MSELoss()\n","\n","    # Initialize optimizer based on name\n","    if optimizer_name == \"AdamW\":\n","        opt = torch.optim.AdamW(model.parameters(), lr=lr)\n","    elif optimizer_name == \"SOAP\":\n","        # Uses the SimpleSOAP class which calls YOUR project_gradient function\n","        opt = SimpleSOAP(model.parameters(), lr=lr)\n","\n","    losses = []\n","\n","    # TODO: Write the training loop\n","    # 1. Loop over 'steps'\n","    # 2. Get data: x, y = get_adding_problem_data()\n","    # 3. Forward pass, Loss, Backward, Optimizer Step\n","    # 4. Append loss.item() to losses\n","\n","    for step in range(steps):\n","        # YOUR CODE HERE\n","        pass\n","\n","    return np.mean(losses[-50:]) # Return final converged loss\n","\n","# Experiment Configuration\n","lrs = [5e-4, 1e-3, 5e-3, 1e-2, 5e-2]\n","results = {\"AdamW\": [], \"SOAP\": []}\n","\n","print(\"Running Experiments...\")\n","# TODO: Uncomment below to run after implementing train_experiment\n","# for lr in lrs:\n","#     results[\"AdamW\"].append(train_experiment(\"AdamW\", lr))\n","#     results[\"SOAP\"].append(train_experiment(\"SOAP\", lr))"],"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBw1S_lZW-nk"},"source":["# Plotting the results\n","plt.figure(figsize=(10, 6))\n","plt.plot(lrs, results[\"AdamW\"], marker='o', label='AdamW')\n","plt.plot(lrs, results[\"SOAP\"], marker='s', label='SOAP')\n","plt.xscale('log')\n","plt.xlabel('Learning Rate (Log Scale)')\n","plt.ylabel('Final Loss (MSE)')\n","plt.title('Hyperparameter Sensitivity: AdamW vs SOAP')\n","plt.legend()\n","plt.grid(True, which=\"both\", ls=\"-\")\n","plt.show()\n","\n","print(\"Analysis:\")\n","print(\"Comparison of how matrix-oriented optimization (SOAP) behaves compared to standard AdamW.\")"],"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}
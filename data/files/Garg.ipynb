{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Maximal Update Parameterization (μP) for RNNs — Student Notebook\n",
        "\n",
        "In this notebook, you will learn how to apply μP (Maximal Update Parameterization) to Recurrent Neural Networks. You'll discover:\n",
        "\n",
        "1. How different parameterization schemes affect training dynamics\n",
        "2. Why RNNs require special consideration for width scaling\n",
        "3. The relationship between spectral radius and gradient flow\n",
        "4. How to implement μP correctly for RNNs\n",
        "\n",
        "## Instructions\n",
        "\n",
        "- **Code cells** marked with `# TODO` or `# YOUR CODE HERE` require you to fill in the missing code\n",
        "- **Question cells** ask you to write your answer in the provided space\n",
        "- Run the verification cells to check your answers\n",
        "- Some cells have hints — try without them first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch as th\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "th.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: Understanding Parameterization Schemes\n",
        "\n",
        "When scaling neural networks to different widths, initialization and forward pass scaling dramatically affect training. There are two schemes we consider:\n",
        "\n",
        "## 1.1 Standard Parameterization (SP)\n",
        "\n",
        "For weight matrix $W \\in \\mathbb{R}^{n_{\\text{out}} \\times n_{\\text{in}}}$:\n",
        "- **Initialization**: $W_{ij} \\sim \\mathcal{N}(0, 1/n_{\\text{in}})$ (Xavier/He)\n",
        "- **Forward pass**: $y = Wx$ (no additional scaling)\n",
        "\n",
        "## 1.2 Maximal Update Parameterization (μP)\n",
        "\n",
        "- **Initialization**: $W_{ij} \\sim \\mathcal{N}(0, 1)$ (unit variance)\n",
        "- **Forward pass scaling**: Depends on layer type\n",
        "- **Learning rate scaling**: Different multipliers for different layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Question 1.1: μP Forward Scaling\n",
        "\n",
        "In μP, different layer types use different forward pass multipliers. Fill in the table:\n",
        "\n",
        "| Layer Type | Forward Multiplier | LR Multiplier |\n",
        "|------------|-------------------|---------------|\n",
        "| Input → Hidden | ??? | 1 |\n",
        "| Hidden → Hidden | ??? | $1/\\text{width\\_mult}$ |\n",
        "| Hidden → Output | ??? | $1/\\text{width\\_mult}$ |\n",
        "\n",
        "**Your Answer:**\n",
        "\n",
        "```\n",
        "Input → Hidden:   Forward multiplier = _______________\n",
        "Hidden → Hidden:  Forward multiplier = _______________\n",
        "Hidden → Output:  Forward multiplier = _______________\n",
        "```\n",
        "\n",
        "*Hint: The multipliers involve $n$ (hidden width), $d$ (input dimension), or both. Think about what keeps activations O(1).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: The RNN Challenge — Spectral Radius\n",
        "\n",
        "RNNs have a unique challenge: the hidden-to-hidden weight matrix $W_{hh}$ is applied **repeatedly**:\n",
        "\n",
        "$$h_t = \\phi(W_{xh} x_t + W_{hh} h_{t-1} + b)$$\n",
        "\n",
        "The **spectral radius** $\\rho(W_{hh})$ — the largest absolute eigenvalue — controls whether signals grow or shrink over time.\n",
        "\n",
        "## 2.1 Random Matrix Theory\n",
        "\n",
        "For a random matrix $W \\in \\mathbb{R}^{n \\times n}$ with i.i.d. entries $W_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$:\n",
        "\n",
        "$$\\rho(W) \\approx \\sigma \\sqrt{n}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Question 2.1: Spectral Radius under Different Parameterizations\n",
        "\n",
        "Use the formula $\\rho(W) \\approx \\sigma\\sqrt{n}$ to fill in the effective spectral radius:\n",
        "\n",
        "**Standard Parameterization (SP):**\n",
        "- Init: $W_{ij} \\sim \\mathcal{N}(0, 1/n)$, so $\\sigma = 1/\\sqrt{n}$\n",
        "- Forward: Use $W$ directly (no scaling)\n",
        "- Effective spectral radius $\\rho(W) = $ _______________\n",
        "\n",
        "**μP with $1/\\sqrt{n}$ scaling:**\n",
        "- Init: $W_{ij} \\sim \\mathcal{N}(0, 1)$, so $\\sigma = 1$\n",
        "- Forward: Use $\\frac{1}{\\sqrt{n}}W$\n",
        "- Effective spectral radius $\\rho(\\frac{1}{\\sqrt{n}}W) = $ _______________\n",
        "\n",
        "**WRONG scaling with $1/n$:**\n",
        "- Init: $W_{ij} \\sim \\mathcal{N}(0, 1)$, so $\\sigma = 1$\n",
        "- Forward: Use $\\frac{1}{n}W$\n",
        "- Effective spectral radius $\\rho(\\frac{1}{n}W) = $ _______________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 2.2: Verify Spectral Radius Empirically\n",
        "\n",
        "Complete the function to compute the spectral radius of the effective recurrence matrix under different parameterizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_spectral_radius(n, parameterization='sp', num_samples=50):\n",
        "    \"\"\"\n",
        "    Compute spectral radius of the effective recurrence matrix.\n",
        "    \n",
        "    Args:\n",
        "        n: Matrix dimension (hidden size)\n",
        "        parameterization: 'sp', 'mup_correct', or 'mup_wrong'\n",
        "        num_samples: Number of random matrices to average over\n",
        "    \n",
        "    Returns:\n",
        "        mean_radius, std_radius\n",
        "    \"\"\"\n",
        "    radii = []\n",
        "    \n",
        "    for _ in range(num_samples):\n",
        "        if parameterization == 'sp':\n",
        "            # SP: W ~ N(0, 1/n), used directly\n",
        "            # TODO: Initialize W with variance 1/n\n",
        "            W = th.randn(n, n) / ...  # YOUR CODE HERE\n",
        "            effective_W = W\n",
        "            \n",
        "        elif parameterization == 'mup_correct':\n",
        "            # μP (CORRECT): W ~ N(0, 1), scaled by 1/√n in forward pass\n",
        "            W = th.randn(n, n)\n",
        "            # TODO: Apply correct μP scaling\n",
        "            effective_W = W / ...  # YOUR CODE HERE\n",
        "            \n",
        "        elif parameterization == 'mup_wrong':\n",
        "            # WRONG: W ~ N(0, 1), scaled by 1/n\n",
        "            W = th.randn(n, n)\n",
        "            # TODO: Apply wrong 1/n scaling\n",
        "            effective_W = W / ...  # YOUR CODE HERE\n",
        "        \n",
        "        # Compute spectral radius (max absolute eigenvalue)\n",
        "        eigs = th.linalg.eigvals(effective_W)\n",
        "        radii.append(eigs.abs().max().item())\n",
        "    \n",
        "    return np.mean(radii), np.std(radii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test your implementation\n",
        "widths = [32, 64, 128, 256, 512]\n",
        "print(\"Effective Spectral Radius:\")\n",
        "print(f\"{'Width':>6} | {'SP':>12} | {'μP (1/√n)':>12} | {'WRONG (1/n)':>12} | {'Theory 1/√n':>12}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for n in widths:\n",
        "    sp_rho, _ = compute_spectral_radius(n, 'sp')\n",
        "    mup_correct_rho, _ = compute_spectral_radius(n, 'mup_correct')\n",
        "    mup_wrong_rho, _ = compute_spectral_radius(n, 'mup_wrong')\n",
        "    theoretical_wrong = 1 / math.sqrt(n)\n",
        "    print(f\"{n:>6} | {sp_rho:>12.4f} | {mup_correct_rho:>12.4f} | {mup_wrong_rho:>12.4f} | {theoretical_wrong:>12.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Question 2.3: Interpreting the Results\n",
        "\n",
        "Based on the table above, answer:\n",
        "\n",
        "**A)** Which parameterizations give a spectral radius that is approximately constant (≈1) regardless of width?\n",
        "\n",
        "**Your Answer:** _______________\n",
        "\n",
        "**B)** For the \"WRONG (1/n)\" scaling, how does the spectral radius change as width increases?\n",
        "\n",
        "**Your Answer:** _______________\n",
        "\n",
        "**C)** If the spectral radius is $\\rho$ and we have $T=100$ timesteps, the gradient magnitude scales roughly as $\\rho^T$. For width $n=256$ with wrong 1/n scaling, approximately what is the gradient ratio? (Use $\\rho \\approx 1/\\sqrt{256} = 1/16$)\n",
        "\n",
        "**Your Answer:** $(1/16)^{100} = $ _______________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 3: Implementing RNN Layers\n",
        "\n",
        "Now let's implement RNN layers with different parameterizations and measure their gradient flow.\n",
        "\n",
        "## Exercise 3.1: Standard Parameterization RNN\n",
        "\n",
        "Complete the `StandardRNNLayer` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StandardRNNLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard Parameterization RNN.\n",
        "    \n",
        "    - Init: Xavier (1/√fan_in variance)\n",
        "    - Forward: No additional scaling\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # TODO: Initialize W_xh with Xavier scaling (divide by √input_size)\n",
        "        self.W_xh = nn.Parameter(th.randn(hidden_size, input_size) / ...)  # YOUR CODE HERE\n",
        "        \n",
        "        # TODO: Initialize W_hh with Xavier scaling (divide by √hidden_size)\n",
        "        self.W_hh = nn.Parameter(th.randn(hidden_size, hidden_size) / ...)  # YOUR CODE HERE\n",
        "        \n",
        "        self.b = nn.Parameter(th.zeros(hidden_size))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, T, _ = x.shape\n",
        "        h = th.zeros(B, self.hidden_size, device=x.device)\n",
        "        h_list = []\n",
        "        \n",
        "        for t in range(T):\n",
        "            # Standard forward pass: no additional scaling\n",
        "            h = th.tanh(F.linear(x[:, t], self.W_xh) + F.linear(h, self.W_hh) + self.b)\n",
        "            h_list.append(h)\n",
        "            h.retain_grad()\n",
        "        \n",
        "        self.h_list = h_list\n",
        "        return th.stack(h_list, dim=1), h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3.2: Correct μP RNN Layer\n",
        "\n",
        "This is the key exercise! Complete the `MuPRNNLayer` with **correct** μP scaling.\n",
        "\n",
        "Remember:\n",
        "- Input → Hidden: scale by $1/\\sqrt{d}$ where $d$ is input dimension\n",
        "- Hidden → Hidden: scale by $1/\\sqrt{n}$ where $n$ is hidden size (**NOT** $1/n$!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MuPRNNLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    CORRECT μP RNN Layer.\n",
        "    \n",
        "    - Init: Unit variance for all weights\n",
        "    - Forward: 1/√d for input, 1/√n for recurrence\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, base_hidden_size=64):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.base_hidden_size = base_hidden_size\n",
        "        self.width_mult = hidden_size / base_hidden_size\n",
        "        \n",
        "        # μP: Unit variance initialization (no scaling here)\n",
        "        self.W_xh = nn.Parameter(th.randn(hidden_size, input_size))\n",
        "        self.W_hh = nn.Parameter(th.randn(hidden_size, hidden_size))\n",
        "        self.b = nn.Parameter(th.zeros(hidden_size))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, T, d = x.shape\n",
        "        n = self.hidden_size\n",
        "        h = th.zeros(B, n, device=x.device)\n",
        "        h_list = []\n",
        "        \n",
        "        for t in range(T):\n",
        "            # TODO: Complete the μP forward pass\n",
        "            # - Input projection should be scaled by 1/√d\n",
        "            # - Recurrence should be scaled by 1/√n (NOT 1/n!)\n",
        "            h = th.tanh(\n",
        "                F.linear(x[:, t], self.W_xh) / ... +  # YOUR CODE HERE: input scaling\n",
        "                F.linear(h, self.W_hh) / ... +        # YOUR CODE HERE: recurrence scaling\n",
        "                self.b\n",
        "            )\n",
        "            h_list.append(h)\n",
        "            h.retain_grad()\n",
        "        \n",
        "        self.h_list = h_list\n",
        "        return th.stack(h_list, dim=1), h\n",
        "    \n",
        "    def get_lr_multipliers(self):\n",
        "        \"\"\"μP learning rate multipliers.\"\"\"\n",
        "        return {\n",
        "            'W_xh': 1.0,\n",
        "            'W_hh': 1.0 / self.width_mult,  # Key μP LR scaling\n",
        "            'b': 1.0,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3.3: Wrong Scaling RNN (for comparison)\n",
        "\n",
        "Complete this **incorrectly** parameterized RNN to see what goes wrong with 1/n scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WrongScalingRNNLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    INCORRECTLY parameterized RNN — DO NOT USE IN PRACTICE!\n",
        "    \n",
        "    This uses 1/n scaling for recurrence, which causes vanishing gradients.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.W_xh = nn.Parameter(th.randn(hidden_size, input_size))\n",
        "        self.W_hh = nn.Parameter(th.randn(hidden_size, hidden_size))\n",
        "        self.b = nn.Parameter(th.zeros(hidden_size))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, T, d = x.shape\n",
        "        n = self.hidden_size\n",
        "        h = th.zeros(B, n, device=x.device)\n",
        "        h_list = []\n",
        "        \n",
        "        for t in range(T):\n",
        "            # WRONG: 1/n scaling causes vanishing gradients!\n",
        "            h = th.tanh(\n",
        "                F.linear(x[:, t], self.W_xh) / math.sqrt(d) +\n",
        "                F.linear(h, self.W_hh) / n +  # WRONG: should be √n\n",
        "                self.b\n",
        "            )\n",
        "            h_list.append(h)\n",
        "            h.retain_grad()\n",
        "        \n",
        "        self.h_list = h_list\n",
        "        return th.stack(h_list, dim=1), h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 3.4: Measuring Gradient Flow\n",
        "\n",
        "Complete the function to measure how well gradients flow backward through time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def measure_gradient_flow(rnn_class, hidden_size, seq_len=10, num_trials=20, **kwargs):\n",
        "    \"\"\"\n",
        "    Measure gradient flow through time.\n",
        "    \n",
        "    Returns the ratio: ||∇_{h_0} L|| / ||∇_{h_T} L||\n",
        "    \n",
        "    A ratio close to 1 means gradients flow well.\n",
        "    A ratio << 1 means gradients are vanishing.\n",
        "    \"\"\"\n",
        "    grad_ratios = []\n",
        "    \n",
        "    for _ in range(num_trials):\n",
        "        rnn = rnn_class(input_size=8, hidden_size=hidden_size, **kwargs)\n",
        "        x = th.randn(4, seq_len, 8)\n",
        "        \n",
        "        _, last_h = rnn(x)\n",
        "        \n",
        "        # TODO: Compute a simple loss at the final timestep\n",
        "        loss = ...  # YOUR CODE HERE (hint: sum of last_h)\n",
        "        loss.backward()\n",
        "        \n",
        "        # TODO: Get gradient norms at first and last timestep\n",
        "        grad_first = rnn.h_list[0].grad.norm().item()  # Gradient at t=0\n",
        "        grad_last = rnn.h_list[-1].grad.norm().item()  # Gradient at t=T\n",
        "        \n",
        "        # TODO: Compute the ratio (with numerical stability check)\n",
        "        if grad_last > 1e-10:\n",
        "            ratio = ...  # YOUR CODE HERE\n",
        "            grad_ratios.append(ratio)\n",
        "    \n",
        "    return np.mean(grad_ratios), np.std(grad_ratios)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test gradient flow across different widths\n",
        "widths = [32, 64, 128, 256]\n",
        "seq_len = 10\n",
        "\n",
        "print(f\"Gradient Flow Ratio (∇h₀/∇h_T) for T={seq_len}:\")\n",
        "print(f\"{'Width':>6} | {'SP':>16} | {'μP (correct)':>16} | {'1/n (WRONG)':>16}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for n in widths:\n",
        "    sp_mean, sp_std = measure_gradient_flow(StandardRNNLayer, n, seq_len)\n",
        "    mup_mean, mup_std = measure_gradient_flow(MuPRNNLayer, n, seq_len)\n",
        "    wrong_mean, wrong_std = measure_gradient_flow(WrongScalingRNNLayer, n, seq_len)\n",
        "    \n",
        "    print(f\"{n:>6} | {sp_mean:>7.4f} ± {sp_std:.4f} | {mup_mean:>7.4f} ± {mup_std:.4f} | {wrong_mean:>7.4f} ± {wrong_std:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Question 3.5: Analyzing Gradient Flow Results\n",
        "\n",
        "Based on the gradient flow measurements:\n",
        "\n",
        "**A)** For SP and correct μP, does the gradient ratio change significantly as width increases?\n",
        "\n",
        "**Your Answer:** _______________\n",
        "\n",
        "**B)** For the wrong 1/n scaling, what happens to the gradient ratio as width doubles?\n",
        "\n",
        "**Your Answer:** _______________\n",
        "\n",
        "**C)** Why is this problematic for training wide RNNs with wrong scaling?\n",
        "\n",
        "**Your Answer:** _______________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 4: Controlling the Spectral Radius with α\n",
        "\n",
        "We can introduce a learnable parameter $\\alpha$ to control the effective spectral radius:\n",
        "\n",
        "$$h_t = \\phi\\left(\\frac{1}{\\sqrt{d}}W_{xh}x_t + \\frac{\\alpha}{\\sqrt{n}}W_{hh}h_{t-1} + b\\right)$$\n",
        "\n",
        "The effective spectral radius becomes $\\alpha$ (independent of width!).\n",
        "\n",
        "## Exercise 4.1: μP RNN with Learnable α"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MuPRNNLayerWithAlpha(nn.Module):\n",
        "    \"\"\"\n",
        "    μP RNN with learnable spectral radius control.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, base_hidden_size=64, init_alpha=0.95):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.base_hidden_size = base_hidden_size\n",
        "        self.width_mult = hidden_size / base_hidden_size\n",
        "        \n",
        "        self.W_xh = nn.Parameter(th.randn(hidden_size, input_size))\n",
        "        self.W_hh = nn.Parameter(th.randn(hidden_size, hidden_size))\n",
        "        self.b = nn.Parameter(th.zeros(hidden_size))\n",
        "        \n",
        "        # TODO: Initialize log_alpha so that alpha = init_alpha\n",
        "        # Hint: if alpha = exp(log_alpha), what should log_alpha be?\n",
        "        self.log_alpha = nn.Parameter(th.tensor(...))  # YOUR CODE HERE\n",
        "    \n",
        "    @property\n",
        "    def alpha(self):\n",
        "        return th.exp(self.log_alpha)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, T, d = x.shape\n",
        "        n = self.hidden_size\n",
        "        h = th.zeros(B, n, device=x.device)\n",
        "        h_list = []\n",
        "        \n",
        "        for t in range(T):\n",
        "            # TODO: Include alpha in the recurrence scaling\n",
        "            # The recurrence should be: alpha * W_hh @ h / √n\n",
        "            h = th.tanh(\n",
        "                F.linear(x[:, t], self.W_xh) / math.sqrt(d) +\n",
        "                ... * F.linear(h, self.W_hh) / math.sqrt(n) +  # YOUR CODE HERE\n",
        "                self.b\n",
        "            )\n",
        "            h_list.append(h)\n",
        "            h.retain_grad()\n",
        "        \n",
        "        self.h_list = h_list\n",
        "        return th.stack(h_list, dim=1), h\n",
        "    \n",
        "    def get_lr_multipliers(self):\n",
        "        return {\n",
        "            'W_xh': 1.0,\n",
        "            'W_hh': 1.0 / self.width_mult,\n",
        "            'b': 1.0,\n",
        "            'log_alpha': 1.0,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify that α controls gradient flow independently of width\n",
        "def analyze_alpha_effect(widths, alphas, seq_len=10):\n",
        "    results = {}\n",
        "    \n",
        "    for alpha in alphas:\n",
        "        results[alpha] = []\n",
        "        for n in widths:\n",
        "            mean, _ = measure_gradient_flow(\n",
        "                MuPRNNLayerWithAlpha, n, seq_len, \n",
        "                num_trials=15, init_alpha=alpha\n",
        "            )\n",
        "            results[alpha].append(mean)\n",
        "    \n",
        "    return results\n",
        "\n",
        "widths = [32, 64, 128, 256]\n",
        "alphas = [0.8, 0.9, 0.95, 1.0]\n",
        "seq_len = 10\n",
        "\n",
        "results = analyze_alpha_effect(widths, alphas, seq_len)\n",
        "\n",
        "print(f\"Gradient Flow Ratio for Different α Values (T={seq_len}):\")\n",
        "print(f\"{'Width':>6}\", end='')\n",
        "for a in alphas:\n",
        "    print(f\" | {'α='+str(a):>12}\", end='')\n",
        "print()\n",
        "print(\"-\" * (8 + 15 * len(alphas)))\n",
        "\n",
        "for i, n in enumerate(widths):\n",
        "    print(f\"{n:>6}\", end='')\n",
        "    for a in alphas:\n",
        "        print(f\" | {results[a][i]:>12.4f}\", end='')\n",
        "    print()\n",
        "\n",
        "print(f\"\\nTheoretical α^T:\")\n",
        "for a in alphas:\n",
        "    print(f\"  α={a}: {a**seq_len:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Question 4.2: Understanding α\n",
        "\n",
        "**A)** For a fixed α, does the gradient ratio depend on width?\n",
        "\n",
        "**Your Answer:** _______________\n",
        "\n",
        "**B)** How does the empirical gradient ratio compare to the theoretical prediction $\\alpha^T$?\n",
        "\n",
        "**Your Answer:** _______________\n",
        "\n",
        "**C)** If you need gradients at $t=0$ to be at least 10% of gradients at $t=T$ for a sequence of length $T=50$, what is the minimum α you need?\n",
        "\n",
        "*Hint: Solve $\\alpha^{50} \\geq 0.1$*\n",
        "\n",
        "**Your Answer:** $\\alpha \\geq $ _______________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 5: Width × Time Interaction\n",
        "\n",
        "With correct μP, width and time become **orthogonal** concerns:\n",
        "- Width affects optimization (LR transfer)\n",
        "- Time affects gradient flow (via $\\alpha^T$)\n",
        "\n",
        "## Exercise 5.1: Verify Orthogonality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_width_time_interaction(widths, seq_lens, alpha=0.95, num_trials=10):\n",
        "    \"\"\"\n",
        "    Measure gradient flow for different (width, seq_len) combinations.\n",
        "    \n",
        "    Expected behavior with correct μP:\n",
        "    - Rows (fixed T) should be constant (width-independent)\n",
        "    - Columns (fixed n) should decay as α^T\n",
        "    \"\"\"\n",
        "    results = np.zeros((len(widths), len(seq_lens)))\n",
        "    \n",
        "    for i, n in enumerate(widths):\n",
        "        for j, T in enumerate(seq_lens):\n",
        "            mean, _ = measure_gradient_flow(\n",
        "                MuPRNNLayerWithAlpha, n, T, \n",
        "                num_trials=num_trials, init_alpha=alpha\n",
        "            )\n",
        "            results[i, j] = mean\n",
        "    \n",
        "    return results\n",
        "\n",
        "widths = [32, 64, 128, 256]\n",
        "seq_lens = [2, 5, 10, 15, 20]\n",
        "alpha = 0.95\n",
        "\n",
        "results = analyze_width_time_interaction(widths, seq_lens, alpha)\n",
        "\n",
        "print(f\"Gradient Flow Matrix (α={alpha}):\")\n",
        "print(f\"{'Width':>6}\", end='')\n",
        "for T in seq_lens:\n",
        "    print(f\" | T={T:>3}\", end='')\n",
        "print()\n",
        "print(\"-\" * (8 + 8 * len(seq_lens)))\n",
        "\n",
        "for i, n in enumerate(widths):\n",
        "    print(f\"{n:>6}\", end='')\n",
        "    for j in range(len(seq_lens)):\n",
        "        print(f\" | {results[i,j]:>5.3f}\", end='')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the orthogonality\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Heatmap\n",
        "ax = axes[0]\n",
        "im = ax.imshow(results, cmap='viridis', aspect='auto')\n",
        "ax.set_xticks(range(len(seq_lens)))\n",
        "ax.set_xticklabels(seq_lens)\n",
        "ax.set_yticks(range(len(widths)))\n",
        "ax.set_yticklabels(widths)\n",
        "ax.set_xlabel('Sequence Length T')\n",
        "ax.set_ylabel('Hidden Width n')\n",
        "ax.set_title(f'Gradient Flow Ratio (α={alpha})')\n",
        "plt.colorbar(im, ax=ax, label='Gradient Ratio')\n",
        "\n",
        "for i in range(len(widths)):\n",
        "    for j in range(len(seq_lens)):\n",
        "        ax.text(j, i, f'{results[i,j]:.2f}', ha='center', va='center', \n",
        "                color='white', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Line plot\n",
        "ax = axes[1]\n",
        "for i, n in enumerate(widths):\n",
        "    ax.semilogy(seq_lens, results[i, :], 'o-', label=f'n={n}', linewidth=2, markersize=8)\n",
        "\n",
        "theoretical = [alpha**T for T in seq_lens]\n",
        "ax.semilogy(seq_lens, theoretical, 'k--', linewidth=3, label=f'Theory: {alpha}^T')\n",
        "\n",
        "ax.set_xlabel('Sequence Length T')\n",
        "ax.set_ylabel('Gradient Flow Ratio (log scale)')\n",
        "ax.set_title('All widths follow the same α^T decay')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Question 5.2: Interpreting the Heatmap\n",
        "\n",
        "**A)** Looking across each row (fixed T, varying n), what do you observe about the gradient ratios?\n",
        "\n",
        "**Your Answer:** _______________\n",
        "\n",
        "**B)** Looking down each column (fixed n, varying T), what pattern do you see?\n",
        "\n",
        "**Your Answer:** _______________\n",
        "\n",
        "**C)** Why is this orthogonality useful for hyperparameter tuning?\n",
        "\n",
        "**Your Answer:** _______________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 6: Learning Rate Transfer — The Core Benefit of μP\n",
        "\n",
        "The fundamental promise of μP is **zero-shot hyperparameter transfer**: tune your learning rate on a small model, then use the *same* LR on a large model.\n",
        "\n",
        "## Why SP Fails at LR Transfer\n",
        "\n",
        "With Standard Parameterization:\n",
        "- Optimal learning rate **changes** with width\n",
        "- Typically, optimal LR $\\propto 1/\\sqrt{n}$ or smaller\n",
        "- Must re-tune hyperparameters for each model size\n",
        "\n",
        "## Why μP Succeeds\n",
        "\n",
        "With μP's adaptive learning rates:\n",
        "- Hidden→hidden weights get LR multiplier: $1/\\text{width\\_mult}$\n",
        "- This compensates for the larger number of neurons\n",
        "- Result: **Same effective learning rate works across all widths**\n",
        "\n",
        "Let's empirically verify this with learning rate sweeps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, lets look at what MuP tells us the learning rate multipliers should be. Fill in the blanks\n",
        "\n",
        "| Layer Type | LR Multiplier |\n",
        "|------------|-------------------|\n",
        "| Input → Hidden | TODO |\n",
        "| Hidden → Hidden | TODO |\n",
        "| Hidden → Output | TODO |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Task:* Implement the learning rate scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " def get_lr_multipliers(self):\n",
        "    \"\"\"\n",
        "    μP learning rate multipliers.\n",
        "    \n",
        "    For hidden→hidden weights, we scale LR by 1/width_mult\n",
        "    to ensure relative updates are O(1).\n",
        "    \"\"\"\n",
        "    # TODO: Complete the learning rate multipliers\n",
        "    return {\n",
        "        'W_xh': ...,                    # Input weights: no scaling\n",
        "        'W_hh': ...,                    # Hidden weights: scale down\n",
        "        'b': ...,                       # Biases: no scaling\n",
        "    }\n",
        "\n",
        "MuPRNNLayer.get_lr_multipliers = get_lr_multipliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_sequence_task(batch_size=32, seq_len=20, input_dim=8):\n",
        "    \"\"\"\n",
        "    Generate a simple sequence task: predict cumulative sum.\n",
        "    Input: random noise, Target: running average\n",
        "    \"\"\"\n",
        "    x = th.randn(batch_size, seq_len, input_dim)\n",
        "    # Target is cumulative mean at each timestep\n",
        "    cumsum = th.cumsum(x.mean(dim=2, keepdim=True), dim=1)\n",
        "    targets = cumsum / th.arange(1, seq_len + 1).view(1, -1, 1)\n",
        "    return x, targets\n",
        "\n",
        "\n",
        "def train_rnn_with_lr(rnn_class, hidden_size, lr, num_steps=100, \n",
        "                      base_hidden_size=64, use_mup_lr_scaling=False):\n",
        "    \"\"\"\n",
        "    Train an RNN with a specific learning rate.\n",
        "    \n",
        "    Args:\n",
        "        rnn_class: RNNLayer class to instantiate\n",
        "        hidden_size: Hidden dimension\n",
        "        lr: Base learning rate\n",
        "        num_steps: Number of training steps\n",
        "        base_hidden_size: Base width for μP scaling\n",
        "        use_mup_lr_scaling: If True, apply μP LR multipliers to different params\n",
        "    \n",
        "    Returns:\n",
        "        final_loss: Loss after training\n",
        "        loss_curve: List of losses during training\n",
        "    \"\"\"\n",
        "    if rnn_class == MuPRNNLayer:\n",
        "        rnn = rnn_class(input_size=8, hidden_size=hidden_size, \n",
        "                       base_hidden_size=base_hidden_size)\n",
        "    else:\n",
        "        rnn = rnn_class(input_size=8, hidden_size=hidden_size)\n",
        "    \n",
        "    readout = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "    # Setup optimizer with μP LR scaling if requested\n",
        "    if use_mup_lr_scaling and hasattr(rnn, 'get_lr_multipliers'):\n",
        "        lr_mults = rnn.get_lr_multipliers()\n",
        "        param_groups = []\n",
        "        \n",
        "        for name, param in rnn.named_parameters():\n",
        "            mult = lr_mults.get(name, 1.0)\n",
        "            param_groups.append({'params': [param], 'lr': lr * mult})\n",
        "        \n",
        "        # Readout layer: μP uses 1/width_mult for output layers\n",
        "        width_mult = hidden_size / base_hidden_size\n",
        "        param_groups.append({'params': readout.parameters(), 'lr': lr / width_mult})\n",
        "        \n",
        "        optimizer = th.optim.Adam(param_groups)\n",
        "    else:\n",
        "        # Standard: same LR for all parameters\n",
        "        optimizer = th.optim.Adam(list(rnn.parameters()) + list(readout.parameters()), lr=lr)\n",
        "    \n",
        "    losses = []\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        x, y = simple_sequence_task(batch_size=32, seq_len=20, input_dim=8)\n",
        "        \n",
        "        h_seq, _ = rnn(x)\n",
        "        pred = readout(h_seq)\n",
        "        \n",
        "        loss = F.mse_loss(pred, y)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "    \n",
        "    return losses[-1], losses\n",
        "\n",
        "\n",
        "def run_lr_sweep(rnn_class, widths, learning_rates, num_steps=100, \n",
        "                 use_mup_lr_scaling=False, base_hidden_size=64):\n",
        "    \"\"\"\n",
        "    Run learning rate sweep for different widths.\n",
        "    \n",
        "    Returns:\n",
        "        results: Dict[width][lr] = (final_loss, loss_curve)\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for width in widths:\n",
        "        print(f\"  Width {width}...\", end='', flush=True)\n",
        "        results[width] = {}\n",
        "        \n",
        "        for lr in learning_rates:\n",
        "            th.manual_seed(42)  # Fixed seed for fair comparison\n",
        "            np.random.seed(42)\n",
        "            \n",
        "            final_loss, loss_curve = train_rnn_with_lr(\n",
        "                rnn_class, width, lr, num_steps,\n",
        "                base_hidden_size=base_hidden_size,\n",
        "                use_mup_lr_scaling=use_mup_lr_scaling\n",
        "            )\n",
        "            results[width][lr] = (final_loss, loss_curve)\n",
        "        \n",
        "        print(\" done\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Run experiments\n",
        "widths = [32, 64, 128, 256, 512]\n",
        "learning_rates = [1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1]\n",
        "num_steps = 150\n",
        "base_hidden_size = 64\n",
        "\n",
        "print(\"Running LR sweep for Standard Parameterization...\")\n",
        "sp_results = run_lr_sweep(\n",
        "    StandardRNNLayer, widths, learning_rates, num_steps,\n",
        "    use_mup_lr_scaling=False\n",
        ")\n",
        "\n",
        "print(\"\\nRunning LR sweep for μP (with adaptive LR scaling)...\")\n",
        "mup_results = run_lr_sweep(\n",
        "    MuPRNNLayer, widths, learning_rates, num_steps,\n",
        "    use_mup_lr_scaling=True, base_hidden_size=base_hidden_size\n",
        ")\n",
        "\n",
        "print(\"\\nDone!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize LR sweep results\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Color map for widths\n",
        "colors = plt.cm.viridis(np.linspace(0, 0.8, len(widths)))\n",
        "\n",
        "# ===== Top Row: Loss vs LR curves =====\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "\n",
        "for i, width in enumerate(widths):\n",
        "    sp_losses = [sp_results[width][lr][0] for lr in learning_rates]\n",
        "    mup_losses = [mup_results[width][lr][0] for lr in learning_rates]\n",
        "    \n",
        "    ax1.plot(learning_rates, sp_losses, 'o-', color=colors[i], \n",
        "             label=f'n={width}', linewidth=2, markersize=6)\n",
        "    ax2.plot(learning_rates, mup_losses, 's-', color=colors[i], \n",
        "             label=f'n={width}', linewidth=2, markersize=6)\n",
        "\n",
        "ax1.set_xscale('log')\n",
        "ax1.set_yscale('log')\n",
        "ax1.set_xlabel('Learning Rate', fontsize=11)\n",
        "ax1.set_ylabel('Final Loss', fontsize=11)\n",
        "ax1.set_title('Standard Parameterization\\n(Optimal LR shifts with width)', fontsize=12, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.set_xscale('log')\n",
        "ax2.set_yscale('log')\n",
        "ax2.set_xlabel('Learning Rate', fontsize=11)\n",
        "ax2.set_ylabel('Final Loss', fontsize=11)\n",
        "ax2.set_title('μP with Adaptive LR Scaling\\n(Optimal LR constant across width)', fontsize=12, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# ===== Middle Row: Optimal LR vs Width =====\n",
        "ax3 = fig.add_subplot(gs[1, :])\n",
        "\n",
        "# Find optimal LR for each width\n",
        "sp_optimal_lrs = []\n",
        "mup_optimal_lrs = []\n",
        "\n",
        "for width in widths:\n",
        "    sp_losses = [sp_results[width][lr][0] for lr in learning_rates]\n",
        "    mup_losses = [mup_results[width][lr][0] for lr in learning_rates]\n",
        "    \n",
        "    sp_optimal_lrs.append(learning_rates[np.argmin(sp_losses)])\n",
        "    mup_optimal_lrs.append(learning_rates[np.argmin(mup_losses)])\n",
        "\n",
        "ax3.plot(widths, sp_optimal_lrs, 'o-', color='#e74c3c', \n",
        "         label='Standard Param', linewidth=3, markersize=10)\n",
        "ax3.plot(widths, mup_optimal_lrs, 's-', color='#27ae60', \n",
        "         label='μP (adaptive LR)', linewidth=3, markersize=10)\n",
        "\n",
        "# Add theoretical SP line (optimal LR ∝ 1/√n)\n",
        "sp_theory = [sp_optimal_lrs[0] * np.sqrt(widths[0] / w) for w in widths]\n",
        "ax3.plot(widths, sp_theory, '--', color='#e74c3c', alpha=0.5, \n",
        "         linewidth=2, label='SP Theory (∝ 1/√n)')\n",
        "\n",
        "ax3.axhline(y=mup_optimal_lrs[0], color='#27ae60', linestyle='--', \n",
        "            alpha=0.5, linewidth=2, label='μP Theory (constant)')\n",
        "\n",
        "ax3.set_xscale('log', base=2)\n",
        "ax3.set_yscale('log')\n",
        "ax3.set_xlabel('Hidden Width n', fontsize=12)\n",
        "ax3.set_ylabel('Optimal Learning Rate', fontsize=12)\n",
        "ax3.set_title('Optimal LR vs Width: SP Requires Retuning, μP Transfers', \n",
        "              fontsize=13, fontweight='bold')\n",
        "ax3.legend(fontsize=10)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# ===== Bottom Row: Training curves for specific widths =====\n",
        "ax4 = fig.add_subplot(gs[2, 0])\n",
        "ax5 = fig.add_subplot(gs[2, 1])\n",
        "\n",
        "# Show training curves for smallest and largest width at their optimal LRs\n",
        "small_width, large_width = widths[0], widths[-1]\n",
        "sp_small_lr = sp_optimal_lrs[0]\n",
        "sp_large_lr = sp_optimal_lrs[-1]\n",
        "mup_optimal_lr = mup_optimal_lrs[0]  # Same for all widths!\n",
        "\n",
        "_, sp_small_curve = sp_results[small_width][sp_small_lr]\n",
        "_, sp_large_curve = sp_results[large_width][sp_large_lr]\n",
        "\n",
        "ax4.semilogy(sp_small_curve, color='#3498db', linewidth=2, \n",
        "             label=f'n={small_width}, LR={sp_small_lr:.4f}')\n",
        "ax4.semilogy(sp_large_curve, color='#e74c3c', linewidth=2, \n",
        "             label=f'n={large_width}, LR={sp_large_lr:.4f}')\n",
        "ax4.set_xlabel('Training Step', fontsize=11)\n",
        "ax4.set_ylabel('Loss', fontsize=11)\n",
        "ax4.set_title('Standard Param: Different LRs per Width', fontsize=12, fontweight='bold')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "_, mup_small_curve = mup_results[small_width][mup_optimal_lr]\n",
        "_, mup_large_curve = mup_results[large_width][mup_optimal_lr]\n",
        "\n",
        "ax5.semilogy(mup_small_curve, color='#3498db', linewidth=2, \n",
        "             label=f'n={small_width}, LR={mup_optimal_lr:.4f}')\n",
        "ax5.semilogy(mup_large_curve, color='#27ae60', linewidth=2, \n",
        "             label=f'n={large_width}, LR={mup_optimal_lr:.4f}')\n",
        "ax5.set_xlabel('Training Step', fontsize=11)\n",
        "ax5.set_ylabel('Loss', fontsize=11)\n",
        "ax5.set_title('μP: Same LR Works for All Widths!', fontsize=12, fontweight='bold')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"=\" * 60)\n",
        "print(\"LEARNING RATE TRANSFER SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n{'Width':>6} | {'SP Optimal LR':>15} | {'μP Optimal LR':>15}\")\n",
        "print(\"-\" * 60)\n",
        "for i, width in enumerate(widths):\n",
        "    print(f\"{width:>6} | {sp_optimal_lrs[i]:>15.6f} | {mup_optimal_lrs[i]:>15.6f}\")\n",
        "\n",
        "print(f\"\\n{'Metric':>30} | {'SP':>12} | {'μP':>12}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'LR range (max/min)':>30} | {max(sp_optimal_lrs)/min(sp_optimal_lrs):>12.2f}x | {max(mup_optimal_lrs)/min(mup_optimal_lrs):>12.2f}x\")\n",
        "print(f\"{'LR std deviation':>30} | {np.std(sp_optimal_lrs):>12.6f} | {np.std(mup_optimal_lrs):>12.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"KEY INSIGHT: μP's adaptive LR scaling enables zero-shot transfer!\")\n",
        "print(\"The same base LR works across all widths with μP.\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We should see that MuP leads to better learning rate transfer!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

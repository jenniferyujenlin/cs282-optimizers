{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6238e2ab",
   "metadata": {},
   "source": [
    "# HW03 Participation D – Lion and SOAP Solutions\n",
    "\n",
    "This notebook contains staff solutions for the **Lion** (part c) and **SOAP-style** (part h) optimizer questions from `q_mup_coding.ipynb`. It is not student-facing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aed713",
   "metadata": {},
   "source": [
    "## Part c – Lion (staff solution)\n",
    "\n",
    "This section records a reference implementation of `SimpleLion` and brief notes on expected behavior for parts c.1 (activation deltas) and c.2 (hyperparameter sweep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class SimpleLion(Optimizer):\n",
    "    \"\"\"Reference implementation matching the student TODO for part c.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Any,\n",
    "        lr: float = 1e-1,\n",
    "        b1: float = 0.9,\n",
    "    ):\n",
    "        defaults = dict(lr=lr, b1=b1)\n",
    "        super(SimpleLion, self).__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            b1 = group[\"b1\"]\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:  # initialization\n",
    "                    state[\"momentum\"] = torch.zeros_like(p)\n",
    "\n",
    "                m = state[\"momentum\"]\n",
    "                # Exponential-moving-average momentum\n",
    "                m.lerp_(grad, 1 - b1)\n",
    "                # Sign-based update\n",
    "                u = m.sign()\n",
    "                # Parameter update\n",
    "                p.add_(u, alpha=-lr)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a65ff",
   "metadata": {},
   "source": [
    "### Notes for c.1 (activation deltas)\n",
    "\n",
    "- Compared to `SimpleAdam`, `SimpleLion` often produces:\n",
    "  - Slightly larger or more uniform activation deltas across layers, because sign-based updates do not shrink when gradients are small.\n",
    "  - Less variation in per-layer delta magnitudes when some layers have much smaller raw gradients.\n",
    "- Students should comment qualitatively on which layers change most and how sign-based updates affect the pattern.\n",
    "\n",
    "### Notes for c.2 (hyperparameter sweep)\n",
    "\n",
    "- A small grid over `lr \\in {0.001, 0.003, 0.01, 0.03}` and `b1 \\in {0.8, 0.9, 0.95}` is sufficient.\n",
    "- Reasonable settings typically look like:\n",
    "  - `lr` around `0.003` or `0.01`.\n",
    "  - `b1` around `0.9`.\n",
    "- Too large a learning rate (e.g. `0.03`) can be unstable for Lion because update magnitudes do not shrink with small gradients.\n",
    "- The goal is a brief qualitative statement about sensitivity to `lr` and `b1`, not finding a single perfect pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f70ce",
   "metadata": {},
   "source": [
    "## Part h – SOAP-style Optimizer (staff solution)\n",
    "\n",
    "This section records a reference implementation of `SimpleSOAP` and notes on expected behavior for the update-norm comparison (h.1) and the learning-rate comparison (h.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c864c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class SimpleSOAP(Optimizer):\n",
    "    \"\"\"Reference implementation matching the student TODO for part h.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Any,\n",
    "        lr: float = 1e-1,\n",
    "        b1: float = 0.9,\n",
    "    ):\n",
    "        defaults = dict(lr=lr, b1=b1)\n",
    "        super(SimpleSOAP, self).__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            b1 = group[\"b1\"]\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:  # initialization\n",
    "                    state[\"step\"] = torch.tensor(0.0)\n",
    "                    state[\"momentum\"] = torch.zeros_like(p)\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "                m = state[\"momentum\"]\n",
    "                m.lerp_(grad, 1 - b1)\n",
    "\n",
    "                if len(m.shape) == 1:\n",
    "                    # For biases, just use the momentum directly\n",
    "                    u = m\n",
    "                else:\n",
    "                    # SOAP-style matrix update: orthogonalize, then match Frobenius norm\n",
    "                    U, S, Vh = torch.linalg.svd(m, full_matrices=False)\n",
    "                    u0 = U @ Vh\n",
    "                    m_frob = torch.norm(m, p=\"fro\")\n",
    "                    u0_frob = torch.norm(u0, p=\"fro\") + 1e-16\n",
    "                    scale = m_frob / u0_frob\n",
    "                    u = u0 * scale\n",
    "\n",
    "                p.add_(u, alpha=-lr)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab8370",
   "metadata": {},
   "source": [
    "### Notes for h.1 (update norms)\n",
    "\n",
    "- **Adam**:\n",
    "  - Frobenius and spectral norms reflect raw gradient magnitudes and vary by layer.\n",
    "- **Shampoo**:\n",
    "  - Spectral norms are roughly 1 for matrix parameters (due to orthogonalization).\n",
    "  - Frobenius norms scale like `sqrt(rank)` and can differ substantially from Adam.\n",
    "- **SOAP**:\n",
    "  - By construction, Frobenius norms of updates closely match those of the original momentum (and so are closer to Adam), while spectral norms behave similarly to Shampoo.\n",
    "  - Students should notice that SOAP preserves overall update \"energy\" per layer while still enforcing orthogonality.\n",
    "\n",
    "### Notes for h.2 (learning-rate comparison)\n",
    "\n",
    "- A small sweep over `lr \\in {0.001, 0.003, 0.01, 0.03}` for `SimpleShampoo` and `SimpleSOAP` is sufficient.\n",
    "- Typical qualitative observations:\n",
    "  - SOAP often behaves more like Adam in terms of sensible learning-rate scales, because update magnitudes are preserved.\n",
    "  - Shampoo can require more careful tuning of `lr` due to fixed spectral norms.\n",
    "  - There is usually an overlapping range of good learning rates; SOAP may degrade less sharply when `lr` is slightly mis-specified.\n",
    "- Students only need to report a brief comparison, not an exhaustive grid search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

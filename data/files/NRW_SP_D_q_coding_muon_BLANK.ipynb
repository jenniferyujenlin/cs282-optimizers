{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t6tuD6vGUsz"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssD7ZMwsRHTr"
      },
      "source": [
        "# Problem Intro\n",
        "\n",
        "In this problem, we will implement key components of the Muon optimizer in PyTorch. Then,  we will compare Muon against SGD and AdamW empirically to demonstrate the benefits of Muon.\n",
        "\n",
        "We will train a sample CNN architecture on CIFAR10 image data (initialized below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp4WlumrKZMn"
      },
      "outputs": [],
      "source": [
        "# Define a sample CNN for CIFAR-10\n",
        "class CIFAR10CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Dataset & loader with augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Increase batch size for more realistic training\n",
        "batch_size = 128\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "test_ds = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjpGbmJRTPaJ"
      },
      "source": [
        "## Part 1: Implementing Newton-Schulz\n",
        "\n",
        "The core of Muon is using matrix orthogonalization on a typical gradient update matrix. After orthogonalization:\n",
        "* Singular values become more uniform\n",
        "* Updates act across all directions in parameter space\n",
        "* The neural network can utilize its full parameter capacity (because parameters are all receiving nontrivial gradients)\n",
        "\n",
        "In Muon, matrix orthogonalization is done using Newton-Schulz. Newton-Schulz relies on iteration using an odd matrix polynomial. In this problem, we consider a very simple cubic polynomial, but many odd polynomials can be used (state-of-the-art implementations use specific, tuned quintic polynomial).\n",
        "\n",
        "**Complete the following code** to implement Newton-Schulz, using the matrix polynomial in the comments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm5I-C5HIJQt"
      },
      "outputs": [],
      "source": [
        "def newton_schulz_orthogonalize(X: torch.Tensor, num_iters: int):\n",
        "    \"\"\"\n",
        "    Apply Newton-Schulz iterations to approximate orthogonalization.\n",
        "\n",
        "    This function applies the polynomial f(X) = (3X - X^3)/2 repeatedly to a normalized matrix,\n",
        "    which gradually forces all singular values to 1 while preserving singular vectors.\n",
        "\n",
        "    Args:\n",
        "      X (torch.Tensor): Input matrix to orthogonalize\n",
        "      num_iters (int): Number of Newton-Schulz iterations\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: Orthogonalized matrix\n",
        "    \"\"\"\n",
        "    dtype = X.dtype\n",
        "    # Use bfloat16 for potential speed/memory savings during the iterations\n",
        "    X = X.bfloat16()\n",
        "    # Recall from prior homeworks that we can transpose the matrix to speed up computation.\n",
        "    transposed = False\n",
        "    if X.size(-2) < X.size(-1):\n",
        "        transposed = True\n",
        "        X = X.mT\n",
        "\n",
        "    # Ensure spectral norm is at most sqrt(3)\n",
        "    norm = torch.linalg.norm(X, dim=(-2, -1), keepdim=True)\n",
        "    X = torch.div(X, norm + 1e-7) * (3**0.5)\n",
        "\n",
        "    ################################################################################\n",
        "    # TODO: YOUR CODE HERE\n",
        "    ################################################################################\n",
        "    def f(X):\n",
        "      return (3*X - X @ X.T @ X) / 2\n",
        "    for _ in range(num_iters):\n",
        "      X = f(X)\n",
        "    ################################################################################\n",
        "\n",
        "    if transposed:\n",
        "        X = X.mT\n",
        "\n",
        "    return X.to(dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YEjXmzqS_Ms"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "Notice that in the above implementation, we scale the spectral norm to be at most sqrt(3). **Can you explain why we choose this particular scaling?**\n",
        "\n",
        "*(Hint: Inspect the roots of the cubic polynomial. What is the connection between the roots and the convergence properties of the singular values? You can refer to Discussion 4 for the answer)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKyN6DSfaA7k"
      },
      "source": [
        "## Part 2: Implementing Muon Update\n",
        "\n",
        "Now, we implement the update in a Muon optimizer. Given parameter matrix $W$ with momentum matrix $M$, the pseudocode for the Muon update proceeds as follows:\n",
        "\n",
        "```\n",
        "d_out, d_in = M.shape\n",
        "\n",
        "# Apply Newton-Schulz orthogonalization\n",
        "M ← newton_schulz_orthogonalize(M, ns_iters)\n",
        "        \n",
        "# Apply muP scaling factor for consistent update magnitude\n",
        "M ← M · sqrt(max(1, d_out / d_in))\n",
        "```\n",
        "\n",
        "Then, the Muon update is used later to update the parameters W:\n",
        "```\n",
        "# Update the parameter matrix\n",
        "W  ← W - lr * M\n",
        "```\n",
        "\n",
        "**Complete the following code** to implement the Muon update following the above pseudocode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-m7S4cJnKx2"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def muon_update(grad, momentum, beta=0.95, ns_iters=5):\n",
        "    momentum.lerp_(grad, 1 - beta)  # momentum = beta * momentum + (1-beta) * grad\n",
        "    update = momentum.clone()\n",
        "\n",
        "    # If the parameter is a convolutional kernel, then flatten to a 2D matrix\n",
        "    original_shape = update.shape\n",
        "    reshaped = False\n",
        "    if update.ndim > 2:\n",
        "        reshaped = True\n",
        "        update = update.view(update.size(0), -1)\n",
        "\n",
        "    ################################################################################\n",
        "    # TODO: YOUR CODE HERE\n",
        "    ################################################################################\n",
        "    d_out, d_in = update.shape\n",
        "    update = newton_schulz_orthogonalize(update, ns_iters)\n",
        "    update = update * torch.sqrt(torch.tensor(max(1, d_out / d_in)))\n",
        "    ################################################################################\n",
        "\n",
        "    # Restore shape if needed\n",
        "    if reshaped:\n",
        "        update = update.view(original_shape)\n",
        "\n",
        "    return update\n",
        "\n",
        "\n",
        "class Muon(optim.Optimizer):\n",
        "    def __init__(self, params, lr=0.01, beta=0.95, ns_iters=5,\n",
        "                weight_decay=0):\n",
        "\n",
        "        defaults = dict(lr=lr, beta=beta, ns_iters=ns_iters,\n",
        "                        weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            beta = group['beta']\n",
        "            ns_iters = group['ns_iters']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                # Get state for this parameter\n",
        "                state = self.state[p]\n",
        "                # Initialize momentum buffer if it doesn't exist\n",
        "                if 'momentum' not in state:\n",
        "                    state['momentum'] = torch.zeros_like(grad)\n",
        "\n",
        "                # Apply weight decay directly to parameters (AdamW style)\n",
        "                if weight_decay != 0:\n",
        "                    p.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                # Apply newton_schulz if parameter is a matrix\n",
        "                if p.ndim >= 2:\n",
        "                    update = muon_update(grad, state['momentum'],\n",
        "                                         beta=beta, ns_iters=ns_iters)\n",
        "                    # Apply update to parameters\n",
        "                    p.add_(update, alpha=-lr)\n",
        "                else:\n",
        "                    # For non-matrix parameters, i.e. bias, use standard momentum update\n",
        "                    momentum = state['momentum']\n",
        "                    momentum.mul_(beta).add_(grad)\n",
        "                    p.add_(momentum, alpha=-lr)\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6V1-eIIsPrJ"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "Note that Muon requires that parameters are 2D matrices of shape $d_{out} \\times d_{in}$. However, we know that parameters that are convolutional kernels have shape $c_{out} \\times c_{in} \\times k \\times k$ where $c$ denotes number of channels and $k$ is kernel size.\n",
        "\n",
        "Modern implementations of convolutional layers will transform an input image $\\mathbf{x}$ of shape $c_{in} \\times h \\times w$ to $\\mathbf{x}'$ such that each column has size $c_{in} \\cdot k \\cdot k$ and corresponds to one flattened \"receptive field\" of the image (or one patch of the image that a convolutional filter passes over to compute one pixel in the output).\n",
        "\n",
        "Given this fact, **how do we modify the convolutional kernel into a $d_{out} \\times d_{in}$ matrix $C$ such that the output of the convolutional layer can be expressed as $C \\mathbf{x}'$**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOa1BoaWeJ1R"
      },
      "source": [
        "## Part 3: Empirical Evaluation of Muon\n",
        "\n",
        "Now, we'll train the CNN network on the CIFAR10 dataset using our Muon implementation, comparing performance on the test set against other popular optimizers in SGD and AdamW.\n",
        "\n",
        "First, in addition to SGD and AdamW, we consider two additional baseline optimizers that will help us better interpret our results:\n",
        "\n",
        "\n",
        "*   MuonSVD: Rather than using Newton-Schulz, we orthogonalize the momentum using SVD on the momentum matrix $M = U\\Sigma V^T$ and computing $UV^T$.\n",
        "*   AdamWMuP: We add the muP scaling on top of the AdamW optimizer. This is meant the help us better understand how much of the Muon performance is due to the orthogonalization step, and how much is simply from the muP scaling.\n",
        "\n",
        "The cell below implements these two additional optimizers. You do not have to implement anything.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix6NZCLkF7qK"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def muon_update_svd(grad, momentum, beta=0.95):\n",
        "    momentum.lerp_(grad, 1 - beta)  # momentum = beta * momentum + (1-beta) * grad\n",
        "    update = momentum.clone()\n",
        "\n",
        "    # If the parameter is a convolutional kernel, then flatten to a 2D matrix\n",
        "    original_shape = update.shape\n",
        "    reshaped = False\n",
        "    if update.ndim > 2:\n",
        "        reshaped = True\n",
        "        update = update.view(update.size(0), -1)\n",
        "\n",
        "    # Orthogonalization via SVD - specify full_matrices=False for reduced SVD\n",
        "    U, _, Vh = torch.linalg.svd(update, full_matrices=False)\n",
        "    update = torch.matmul(U, Vh)\n",
        "\n",
        "    # Apply muP scaling\n",
        "    update.mul_(max(1, update.size(-2) / update.size(-1))**0.5)\n",
        "\n",
        "    # Restore shape if needed\n",
        "    if reshaped:\n",
        "        update = update.view(original_shape)\n",
        "\n",
        "    return update\n",
        "\n",
        "\n",
        "class MuonSVD(optim.Optimizer):\n",
        "    def __init__(self, params, lr=0.01, beta=0.95, weight_decay=0):\n",
        "\n",
        "        defaults = dict(lr=lr, beta=beta, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            beta = group['beta']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                # Get state for this parameter\n",
        "                state = self.state[p]\n",
        "                # Initialize momentum buffer if it doesn't exist\n",
        "                if 'momentum' not in state:\n",
        "                    state['momentum'] = torch.zeros_like(grad)\n",
        "\n",
        "                # Apply weight decay directly to parameters (AdamW style)\n",
        "                if weight_decay != 0:\n",
        "                    p.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                # Apply newton_schulz if parameter is a matrix\n",
        "                if p.ndim >= 2:\n",
        "                    update = muon_update_svd(grad, state['momentum'], beta=beta)\n",
        "                    # Apply update to parameters\n",
        "                    p.add_(update, alpha=-lr)\n",
        "                else:\n",
        "                    # For non-matrix parameters, i.e. bias, use standard momentum update\n",
        "                    momentum = state['momentum']\n",
        "                    momentum.mul_(beta).add_(grad)\n",
        "                    p.add_(momentum, alpha=-lr)\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "class AdamWMuP(optim.Optimizer):\n",
        "    def __init__(self,  params, lr=0.01, betas=(0.9, 0.999), weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0: # Initialization\n",
        "                    state[\"step\"] = torch.tensor(0.0)\n",
        "                    state['momentum'] = torch.zeros_like(p)\n",
        "                    state['variance'] = torch.zeros_like(p)\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    p.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                state['step'] += 1\n",
        "                m = state['momentum']\n",
        "                m.lerp_(grad, 1 - group[\"betas\"][0])\n",
        "                v = state['variance']\n",
        "                v.lerp_(grad**2, 1 - group[\"betas\"][1])\n",
        "\n",
        "                m_hat = m / (1 - group[\"betas\"][0]**state['step'])\n",
        "                v_hat = v / (1 - group[\"betas\"][1]**state['step'])\n",
        "                u = m_hat / (torch.sqrt(v_hat) + 1e-16)\n",
        "\n",
        "                if p.ndim >= 2:\n",
        "                    # If the parameter is a convolutional kernel, then flatten to a 2D matrix\n",
        "                    original_shape = u.shape\n",
        "                    reshaped = False\n",
        "                    if u.ndim > 2:\n",
        "                        u = u.view(u.shape[0], -1)  # keep first dim, flatten the rest\n",
        "                        reshaped = True\n",
        "\n",
        "                    u.mul_(max(1, u.size(-2) / u.size(-1))**0.5)\n",
        "\n",
        "                    # Unflatten back to convolutional kernel\n",
        "                    if reshaped:\n",
        "                        u = u.view(original_shape)\n",
        "\n",
        "                p.add_(u, alpha=-lr)\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd48iWs2EoF2"
      },
      "source": [
        "\n",
        "If you performed hyperparameter sweeping (optional part below), then replace the default hyperparameters with the values you found from your sweep. Then, run the following cell to investigate how good Muon is relative to other baseline optimizers. The cell should take less than 10 minutes if you use a GPU runtime on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2SVu7okFWVE"
      },
      "outputs": [],
      "source": [
        "# # --- Define the optimizers you want to compare ---\n",
        "# optimizers_dict = {\n",
        "#     \"Muon\": lambda params: Muon(params, lr=1e-2, weight_decay=0),\n",
        "#     \"SGD\": lambda params: torch.optim.SGD(params, lr=1e-2, momentum=0.9, weight_decay=1e-4),\n",
        "#     \"AdamW\": lambda params: torch.optim.AdamW(params, lr=1e-3, weight_decay=1e-3),\n",
        "#     \"MuonSVD\": lambda params: MuonSVD(params, lr=1e-2, weight_decay=0),\n",
        "#     \"AdamWMuP\": lambda params: AdamWMuP(params, lr=1e-3, weight_decay=1e-3)\n",
        "# }\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device = torch.device(\"cuda:1\")\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # Function to evaluate model\n",
        "# def evaluate(model, dataloader):\n",
        "#     model.eval()\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     with torch.no_grad():\n",
        "#         for images, labels in dataloader:\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             outputs = model(images)\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "#     return 100 * correct / total\n",
        "\n",
        "\n",
        "# results = {}  # store loss curves\n",
        "# accuracy_results = {}  # store accuracy curves\n",
        "\n",
        "# # --- Train for each optimizer ---\n",
        "# for opt_name, opt_fn in optimizers_dict.items():\n",
        "#     print(f\"\\n--- Training with {opt_name} ---\")\n",
        "#     model = CIFAR10CNN().to(device)  # re-init model each time\n",
        "#     optimizer = opt_fn(model.parameters())\n",
        "\n",
        "#     # Learning rate scheduler\n",
        "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
        "\n",
        "#     losses = []\n",
        "#     accuracies = []\n",
        "#     epoch_times = []\n",
        "\n",
        "#     for epoch in range(1, 6):  # Train for 5 epochs\n",
        "#         model.train()\n",
        "#         epoch_start_time = time.time()\n",
        "#         total_loss = 0\n",
        "\n",
        "#         for i, (x, y) in enumerate(train_loader):\n",
        "#             x, y = x.to(device), y.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "#             out = model(x)\n",
        "#             loss = criterion(out, y)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#             # Print progress\n",
        "#             if (i+1) % 100 == 0:\n",
        "#                 print(f'Epoch [{epoch}/5], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "#         epoch_end_time = time.time()\n",
        "#         epoch_duration = epoch_end_time - epoch_start_time\n",
        "#         epoch_times.append(epoch_duration)\n",
        "\n",
        "#         # Evaluate\n",
        "#         test_acc = evaluate(model, test_loader)\n",
        "\n",
        "#         avg_loss = total_loss / len(train_loader)\n",
        "#         losses.append(avg_loss)\n",
        "#         accuracies.append(test_acc)\n",
        "\n",
        "#         print(f\"{opt_name} | Epoch {epoch}, avg loss: {avg_loss:.4f}, test acc: {test_acc:.2f}%, time: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "#         # Update learning rate\n",
        "#         scheduler.step()\n",
        "\n",
        "#     results[opt_name] = losses\n",
        "#     accuracy_results[opt_name] = accuracies\n",
        "\n",
        "#     # Calculate and print total training time\n",
        "#     total_time = sum(epoch_times)\n",
        "#     print(f\"{opt_name} | Total training time: {total_time:.2f} seconds\")\n",
        "\n",
        "# # --- Plot results ---\n",
        "# # 1. Loss vs Epoch\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.subplot(1, 2, 1)\n",
        "# for opt_name, losses in results.items():\n",
        "#     plt.plot(range(1, len(losses)+1), losses, label=opt_name, marker=\"o\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Average Loss\")\n",
        "# plt.title(\"Training Loss vs Epoch for Different Optimizers\")\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "\n",
        "# # 2. Accuracy vs Epoch\n",
        "# plt.subplot(1, 2, 2)\n",
        "# for opt_name, accuracies in accuracy_results.items():\n",
        "#     plt.plot(range(1, len(accuracies)+1), accuracies, label=opt_name, marker=\"o\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Accuracy (%)\")\n",
        "# plt.title(\"Test Accuracy vs Epoch for Different Optimizers\")\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCOdMfEFfHmo"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "**Which optimizer performed best between Muon, SGD, and AdamW?** Also **copy the resulting plots** into the submission as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqoNyPtLMf3H"
      },
      "source": [
        "### Question 4\n",
        "\n",
        "Compare the loss curves between Muon and MuonSVD. **Are the results expected? Explain why.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RVJi_lXM6JD"
      },
      "source": [
        "### Question 5\n",
        "\n",
        "The Muon optimizer contains two key differences: (1) orthogonalization of the momentum, and (2) muP scaling of the momentum. **Between orthogonalization and muP scaling, which seemed to matter more?** Reference the loss curves to justify your answer.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyAeAbB2bssc"
      },
      "source": [
        "### Question 6 (Optional)\n",
        "\n",
        "Our implementation of Newton-Schulz is suboptimal in the polynomial used for convergence.\n",
        "The community has developed quintic polynomials that converge faster while still being efficient. **Implement an improved Newton-Schulz and compare. Comment on the speed advantage of the improved Muon relative to the MuonSVD.**\n",
        "\n",
        "*(Hint: You can modify the number of iterations by setting the ns_iters parameter in the Muon optimizer)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ9TX-EqwdfU"
      },
      "source": [
        "## Part 4 (Optional): Hyperparameter sweeps (Muon & AdamW)\n",
        "\n",
        "To ensure that we are making fair comparisons, we will sweep over both learning rate and weight decay for both Muon and AdamW (the likely strongest competing optimizer). We choose these two parameters because empirically, they have the greatest effect on training.\n",
        "\n",
        "Running the sweep should take less than 1 hour total on a GPU runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLfdpF5h_ReG"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data import random_split\n",
        "\n",
        "# # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # Function to evaluate model\n",
        "# def evaluate(model, dataloader):\n",
        "#     model.eval()\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     with torch.no_grad():\n",
        "#         for images, labels in dataloader:\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             outputs = model(images)\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "#     return 100 * correct / total\n",
        "\n",
        "\n",
        "# def train_and_evaluate(optimizer_fn, lr, weight_decay, num_epochs=5):\n",
        "#     \"\"\"Trains and evaluates the CIFAR10CNN with a given optimizer and hyperparameters.\"\"\"\n",
        "#     model = CIFAR10CNN().to(device)\n",
        "#     optimizer = optimizer_fn(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "#     best_accuracy = 0\n",
        "\n",
        "#     # Split training data into training and validation sets\n",
        "#     train_size = int(0.8 * len(train_ds))\n",
        "#     val_size = len(train_ds) - train_size\n",
        "#     train_dataset, val_dataset = random_split(train_ds, [train_size, val_size])\n",
        "\n",
        "#     train_loader_split = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "#     for epoch in range(1, num_epochs + 1):\n",
        "#         model.train()\n",
        "#         for i, (x, y) in enumerate(train_loader_split):\n",
        "#             x, y = x.to(device), y.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "#             out = model(x)\n",
        "#             loss = criterion(out, y)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#         # Evaluate on the validation set\n",
        "#         val_acc = evaluate(model, val_loader)\n",
        "#         best_accuracy = max(best_accuracy, val_acc)\n",
        "\n",
        "#         scheduler.step()\n",
        "\n",
        "#     return best_accuracy\n",
        "\n",
        "# # Define the optimizers and their hyperparameter search spaces\n",
        "# optimizers_to_sweep = {\n",
        "#     \"Muon\": {\n",
        "#         \"optimizer_fn\": lambda params, lr, weight_decay: Muon(params, lr=lr, weight_decay=weight_decay),\n",
        "#         \"lr_values\": [1e-2, 5e-3, 1e-3, 5e-4],\n",
        "#         \"weight_decay_values\": [0, 1e-4, 1e-3]\n",
        "#     },\n",
        "#     \"AdamW\": {\n",
        "#         \"optimizer_fn\": lambda params, lr, weight_decay: torch.optim.AdamW(params, lr=lr, weight_decay=weight_decay),\n",
        "#         \"lr_values\": [1e-2, 5e-3, 1e-3, 5e-4],\n",
        "#         \"weight_decay_values\": [0, 1e-4, 1e-3]\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# results = {}\n",
        "\n",
        "# # Perform the hyperparameter sweep for each optimizer\n",
        "# for opt_name, opt_info in optimizers_to_sweep.items():\n",
        "#     print(f\"\\n--- Hyperparameter sweep for {opt_name} ---\")\n",
        "#     optimizer_fn = opt_info[\"optimizer_fn\"]\n",
        "#     lr_values = opt_info[\"lr_values\"]\n",
        "#     weight_decay_values = opt_info[\"weight_decay_values\"]\n",
        "\n",
        "#     results[opt_name] = {}\n",
        "\n",
        "#     for lr in lr_values:\n",
        "#         for weight_decay in weight_decay_values:\n",
        "#             print(f\"Training with lr={lr}, weight_decay={weight_decay}\")\n",
        "#             accuracy = train_and_evaluate(optimizer_fn, lr, weight_decay)\n",
        "#             results[opt_name][(lr, weight_decay)] = accuracy\n",
        "#             print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# # Print the results and best hyperparameters for each optimizer\n",
        "# print(\"\\n--- Hyperparameter Sweep Results ---\")\n",
        "# for opt_name, opt_results in results.items():\n",
        "#     print(f\"\\n{opt_name}:\")\n",
        "#     for (lr, weight_decay), accuracy in opt_results.items():\n",
        "#         print(f\"  (lr={lr}, weight_decay={weight_decay}): {accuracy:.2f}%\")\n",
        "\n",
        "#     best_params = max(opt_results, key=opt_results.get)\n",
        "#     print(f\"Best hyperparameters for {opt_name}: (lr={best_params[0]}, weight_decay={best_params[1]}) with validation accuracy {opt_results[best_params]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVQE4jRzDkU9"
      },
      "source": [
        "### Question 7 (Optional)\n",
        "\n",
        "**What were the best choices for hyperparameters for Muon? What about for AdamW?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aQqzm9BGTmX"
      },
      "source": [
        "# New questions by (Nicolas Rault-Wang, nraultwang@berkeley.edu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfWo6Br3fp8W"
      },
      "source": [
        "## Part 5: The Polar Express (Muon Variant)\n",
        "\n",
        "In this section, we will implement the 2025  [**Polar Express**](https://arxiv.org/abs/2505.16932) method, a state-of-the-art algorithm for computing the polar decomposition within the Muon optimizer.\n",
        "\n",
        "Standard Muon typically relies on Newton-Schulz iteration to orthogonalize the update matrix. Newton-Schulz uses a static polynomial at every step (usually degree 3: $p(X) = 1.5X - 0.5X X^\\top X$). While effective asymptotically, it suffers from slow initial convergence 3and can be unstable in low precision (like `bfloat16`) because intermediate singular values may overshoot the range $[-1, 1]$, causing sign flips.\n",
        "\n",
        "**Polar Express** improves this by using a dynamic schedule of polynomial coefficients that changes at each iteration $k$. These coefficients are pre-calculated offline by solving a minimax optimization problem, ensuring the polynomial optimally approximates the sign function over the matrix's spectrum at that specific step.\n",
        "\n",
        "This approach guarantees the fastest possible worst-case convergence (super-exponential) while maintaining GPU efficiency. The update typically uses a degree-5 polynomial implemented efficiently with nested matrix multiplications:\n",
        "$$X_{k+1} = X_k (\\alpha_k I + X_k^\\top X_k (\\beta_k I + \\gamma_k X_k^\\top X_k))$$\n",
        "Where:\n",
        "- $X_k$ is the current approximate polar factor.\n",
        "- $\\alpha_k, \\beta_k, \\gamma_k$ are the optimal coefficients for step $k$, pre-computed to maximize spectral steering speed while preventing numerical instability in `bfloat16`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4thywJMLGTmX"
      },
      "source": [
        "### **Question 8: Implementation**\n",
        "Implement the `PolarExpress` subroutine below. We have provided the helper function `optimal_composition` (which calculates the coefficients). Your task is to implement the iterative update loop using efficient matrix multiplication.\n",
        "\n",
        "**Note:** We have provided the `MuonPolarExpress` optimizer class and `muon_polar_update` function for you. You only need to complete the `polar_express_orthogonalize` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI_Xrwp5GTmX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from itertools import repeat\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from itertools import repeat\n",
        "from math import inf, sqrt\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTJGvuMrGTmX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2025 Noah Amsel\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "\"\"\"\n",
        "\n",
        "def optimal_quintic(l, u):\n",
        "    assert 0 <= l <= u\n",
        "    if 1 - 5e-6 <= l / u:\n",
        "        # Above this threshold, the equioscillating polynomials\n",
        "        # is numerically equal to...\n",
        "        return (15/8)/u, (-10/8)/(u**3), (3/8)/(u**5)\n",
        "    # This initialization becomes exact as l -> u\n",
        "    q = (3*l + 1) / 4\n",
        "    r = (l + 3) / 4\n",
        "    E, old_E = inf, None\n",
        "    while not old_E or abs(old_E - E) > 1e-15:\n",
        "        old_E = E\n",
        "        LHS = np.array([\n",
        "            [l, l**3, l**5, 1],\n",
        "            [q, q**3, q**5, -1],\n",
        "            [r, r**3, r**5, 1],\n",
        "            [u, u**3, u**5, -1],\n",
        "        ])\n",
        "        a, b, c, E = np.linalg.solve(LHS, np.ones(4))\n",
        "        q, r = np.sqrt((-3*b + np.array([-1, 1]) *\n",
        "                        sqrt(9*b**2 - 20*a*c)) / (10*c))\n",
        "    return float(a), float(b), float(c)\n",
        "\n",
        "target_slope = 0\n",
        "def obj(l):\n",
        "    a, b, c = optimal_quintic(l, 1)\n",
        "    total = (a+b+c)\n",
        "    a /= total; b /= total; c /= total\n",
        "    local_argmin = np.sqrt((-3*b + sqrt(9*b**2 - 20*a*c)) / (10*c))\n",
        "    local_min = a*local_argmin + b*local_argmin**3 + c*local_argmin**5\n",
        "    return local_min / local_argmin - target_slope\n",
        "\n",
        "\n",
        "def optimal_composition(l, num_iters, safety_factor_eps=0, cushion=0):\n",
        "    u = 1\n",
        "    assert 0 <= l <= u\n",
        "    safety_factor = 1 + safety_factor_eps\n",
        "    coefficients = []\n",
        "    for iter in range(num_iters):\n",
        "        a, b, c = optimal_quintic(max(l, cushion*u), u)\n",
        "        if cushion*u > l:\n",
        "            # Due to cushioning, this may be centered around 1 with\n",
        "            # respect to 0.024*u, u. Recenter it around 1 with respect\n",
        "            # to l, u, meaning find c so that 1 - c*p(l) = c*p(u) - 1:\n",
        "            pl = a*l + b*l**3 + c*l**5\n",
        "            pu = a*u + b*u**3 + c*u**5\n",
        "            rescaler = 2/(pl + pu)\n",
        "            a *= rescaler; b *= rescaler; c *= rescaler\n",
        "        # Optionally incorporate safety factor here:\n",
        "        if iter < num_iters - 1:  # don't apply to last polynomial\n",
        "            a /= safety_factor; b /= safety_factor**3; c /= safety_factor**5\n",
        "        coefficients.append((a, b, c))\n",
        "        l = a*l + b*l**3 + c*l**5\n",
        "        u = 2 - l\n",
        "    return coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1k6vph0GTmX"
      },
      "outputs": [],
      "source": [
        "# Compute the optimal polynomial coefficients for Polar Express.\n",
        "# (This logic is pre-solved for you to focus on the optimizer step)\n",
        "# We use l=1e-3 as a conservative lower bound estimate for singular values\n",
        "\n",
        "coeffs_list = optimal_composition(l=1e-3, num_iters=10, safety_factor_eps=1e-2, cushion=0.02)\n",
        "\n",
        "\n",
        "# --- TODO: Implement Polar Express ---\n",
        "def polar_express_orthogonalize(G: torch.Tensor, steps: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Applies the Polar Express iterative method to orthogonalize matrix G.\n",
        "    \"\"\"\n",
        "    assert G.ndim >= 2\n",
        "    X = G.bfloat16()  # for speed\n",
        "\n",
        "    # Efficiency Trick 1: Work with the smaller dimension\n",
        "    if G.size(-2) > G.size(-1): X = X.mT\n",
        "\n",
        "    # Pre-conditioning: Normalize spectral norm estimate\n",
        "    X = X / (X.norm(dim=(-2, -1), keepdim=True) * 1.01 + 1e-7)\n",
        "\n",
        "    # Prepare coefficients\n",
        "    hs = coeffs_list[:steps] + list(repeat(coeffs_list[-1], steps - len(coeffs_list)))\n",
        "\n",
        "    for a, b, c in hs:\n",
        "        ################################################################################\n",
        "        # TODO: Implement the update X <- a*X + b*X^3 + c*X^5\n",
        "        #\n",
        "        # Efficiency Trick 2: Reuse the Gram matrix A = X @ X.mT\n",
        "        #\n",
        "        # Note: X^3 = (X @ X.mT) @ X = A @ X\n",
        "        #       X^5 = (X @ X.mT) @ (X @ X.mT) @ X = (A @ A) @ X\n",
        "        ################################################################################\n",
        "        # ___________ YOUR CODE HERE ____________\n",
        "        X = ...\n",
        "        ################################################################################\n",
        "\n",
        "    # Restore orientation if we transposed earlier\n",
        "    if G.size(-2) > G.size(-1): X = X.mT\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HHpKpKYGTmX"
      },
      "outputs": [],
      "source": [
        "# --- Provided: MuonPolarExpress Optimizer Wrapper ---\n",
        "def muon_polar_update(grad, momentum, beta=0.95, ns_iters=5):\n",
        "    momentum.lerp_(grad, 1 - beta)  # momentum = beta * momentum + (1-beta) * grad\n",
        "    update = momentum.clone()\n",
        "\n",
        "    original_shape = update.shape\n",
        "    reshaped = False\n",
        "    if update.ndim > 2:\n",
        "        reshaped = True\n",
        "        update = update.view(update.size(0), -1)\n",
        "\n",
        "    # Use Polar Express instead of Newton Schulz to orthogonalize\n",
        "    update = polar_express_orthogonalize(update, ns_iters)\n",
        "\n",
        "    d_out, d_in = update.shape\n",
        "    update = update * torch.sqrt(torch.tensor(max(1, d_out / d_in)))\n",
        "\n",
        "    if reshaped:\n",
        "        update = update.view(original_shape)\n",
        "\n",
        "    return update\n",
        "\n",
        "class MuonPolarExpress(optim.Optimizer):\n",
        "    def __init__(self, params, lr=0.01, beta=0.95, ns_iters=5, weight_decay=0):\n",
        "        defaults = dict(lr=lr, beta=beta, ns_iters=ns_iters, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            beta = group['beta']\n",
        "            ns_iters = group['ns_iters']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None: continue\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "                if 'momentum' not in state:\n",
        "                    state['momentum'] = torch.zeros_like(grad)\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    p.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                if p.ndim >= 2:\n",
        "                    # Call the MuonPolar specific update\n",
        "                    update = muon_polar_update(grad, state['momentum'], beta=beta, ns_iters=ns_iters)\n",
        "                    p.add_(update, alpha=-lr)\n",
        "                else:\n",
        "                    momentum = state['momentum']\n",
        "                    momentum.mul_(beta).add_(grad)\n",
        "                    p.add_(momentum, alpha=-lr)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmSTOlkFGTmX"
      },
      "source": [
        "## Part 6: The Lion Optimizer\n",
        "\n",
        "[**Lion**](https://arxiv.org/abs/2302.06675) (EvoLved Sign Momentum) is a memory-efficient optimizer discovered through symbolic program search. Unlike AdamW, which tracks both momentum (first moment) and variance (second moment), Lion only tracks momentum, halving the memory footprint of the optimizer state.\n",
        "\n",
        "Lion relies on the sign operation to calculate updates, ensuring that all parameter updates have a uniform magnitude determined solely by the learning rate.\n",
        "\n",
        "Let $\\theta$ be the parameters, $g_t$ be the gradient, $\\eta$ be the learning rate, and $\\lambda$ be the weight decay coefficient. The update process typically performs decoupled weight decay first, followed by the parameter update:\n",
        "\n",
        "1. Decoupled Weight Decay:$$\\theta_t \\leftarrow \\theta_{t-1} (1 - \\eta \\lambda)$$\n",
        "2. Calculate Update Candidate (Interpolation of Gradient and Momentum):$$c_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t$$\n",
        "3. Update Parameters (using Sign):$$\\theta_t \\leftarrow \\theta_t - \\eta \\cdot \\text{sign}(c_t)$$\n",
        "4. Update Momentum (EMA):$$m_t = \\beta_2 m_{t-1} + (1-\\beta_2)g_t$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNR7LJR5GTmX"
      },
      "source": [
        "### **Question 10: Implement Lion**\n",
        "Fill in the missing lines in the `Lion` optimizer class below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7SxGYDrGTmX"
      },
      "outputs": [],
      "source": [
        "class Lion(optim.Optimizer):\n",
        "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):\n",
        "        if not 0.0 <= lr: raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None: continue\n",
        "\n",
        "                # Extract parameters\n",
        "                grad = p.grad\n",
        "                lr = group['lr']\n",
        "                wd = group['weight_decay']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state = self.state[p]\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "\n",
        "                exp_avg = state['exp_avg']\n",
        "\n",
        "                ################################################################################\n",
        "                # TODO: Implement the Lion Update Step\n",
        "                #\n",
        "                # 1. Perform Decoupled Weight Decay\n",
        "                #    Update 'p' in-place.\n",
        "                #    Math: p = p * (1 - lr * wd)\n",
        "                #\n",
        "                # 2. Compute the Update Candidate (c_t)\n",
        "                #    This uses the current gradient and the PREVIOUS momentum (exp_avg).\n",
        "                #    Math: c_t = beta1 * exp_avg + (1 - beta1) * grad\n",
        "                #\n",
        "                # 3. Apply the Parameter Update using Sign\n",
        "                #    Update 'p' in-place.\n",
        "                #    Math: p = p - lr * sign(c_t)\n",
        "                #\n",
        "                # 4. Update the Momentum Buffer (exp_avg)\n",
        "                #    Update 'exp_avg' in-place using the current gradient.\n",
        "                #    Math: exp_avg = beta2 * exp_avg + (1 - beta2) * grad\n",
        "                ################################################################################\n",
        "\n",
        "                # ___________ YOUR CODE HERE ____________\n",
        "\n",
        "                ################################################################################\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDwoXfPeGTmX"
      },
      "source": [
        "## Part 7: Evaluating MuonPolarExpress and Lion\n",
        "\n",
        "Run the code below to evaluate your new optimizers (**MuonPolarExpress**, **Lion**) against the (calibrated) baselines (**Muon**, **AdamW**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2IYOk8VGTmX"
      },
      "outputs": [],
      "source": [
        "# Define optimizers to compare\n",
        "optimizers_dict = {\n",
        "    \"MuonPolarExpress\": lambda params: MuonPolarExpress(params, lr=1e-2, weight_decay=1e-1),\n",
        "    \"MuonNewtonSchulz\": lambda params: Muon(params, lr=1e-2, weight_decay=1e-4),\n",
        "\n",
        "    \"Lion\": lambda params: Lion(params, lr=5e-4, weight_decay=0),\n",
        "    \"AdamW\": lambda params: torch.optim.AdamW(params, lr=1e-3, weight_decay=0),\n",
        "    \"SGD\": lambda params: torch.optim.SGD(params, lr=1e-2, momentum=0.9, weight_decay=1e-4),\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device = torch.device(\"cuda:1\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "results = {}  # store loss curves\n",
        "accuracy_results = {}  # store accuracy curves\n",
        "\n",
        "# --- Train for each optimizer ---\n",
        "for opt_name, opt_fn in optimizers_dict.items():\n",
        "    print(f\"\\n--- Training with {opt_name} ---\")\n",
        "    model = CIFAR10CNN().to(device)  # re-init model each time\n",
        "    optimizer = opt_fn(model.parameters())\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    epoch_times = []\n",
        "\n",
        "    for epoch in range(1, 6):  # Train for 5 epochs\n",
        "        model.train()\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0\n",
        "\n",
        "        for i, (x, y) in enumerate(train_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Print progress\n",
        "            if (i+1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch}/5], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "        epoch_times.append(epoch_duration)\n",
        "\n",
        "        # Evaluate\n",
        "        test_acc = evaluate(model, test_loader)\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "        accuracies.append(test_acc)\n",
        "\n",
        "        print(f\"{opt_name} | Epoch {epoch}, avg loss: {avg_loss:.4f}, test acc: {test_acc:.2f}%, time: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "    results[opt_name] = losses\n",
        "    accuracy_results[opt_name] = accuracies\n",
        "\n",
        "    # Calculate and print total training time\n",
        "    total_time = sum(epoch_times)\n",
        "    print(f\"{opt_name} | Total training time: {total_time:.2f} seconds\")\n",
        "\n",
        "# --- Plot results ---\n",
        "# 1. Loss vs Epoch\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "for opt_name, losses in results.items():\n",
        "    plt.plot(range(1, len(losses)+1), losses, label=opt_name, marker=\"o\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.title(\"Training Loss vs Epoch for Different Optimizers\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 2. Accuracy vs Epoch\n",
        "plt.subplot(1, 2, 2)\n",
        "for opt_name, accuracies in accuracy_results.items():\n",
        "    plt.plot(range(1, len(accuracies)+1), accuracies, label=opt_name, marker=\"o\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Test Accuracy vs Epoch for Different Optimizers\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTk9M3M4GTmY"
      },
      "source": [
        "### **Question 11**\n",
        "Based on the plots and best validation accuracy:\n",
        "1.  How does **MuonPolarExpress** compare to standard **Muon**? Did the optimized polynomials provide better stability or faster initial convergence?\n",
        "2.  How did **Lion** compare to **AdamW**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikx4y3-NGTmY"
      },
      "source": [
        "## Part 8 (Optional): Hyperparameter sweeps (MuonPolarExpress & Lion)\n",
        "\n",
        "To ensure that we are making fair comparisons, we will sweep over both learning rate and weight decay for both MuonPolarExpress and Lion.\n",
        "\n",
        "Running the sweep should take less than 1 hour total on a GPU runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBcX4VhsGTmY"
      },
      "outputs": [],
      "source": [
        "# Define the optimizers and their hyperparameter search spaces\n",
        "# Grid to search\n",
        "optimizers_to_sweep_pe_and_lion = {\n",
        "    # \"MuonPolarExpress\": {\n",
        "    #     \"optimizer_fn\": lambda params, lr, weight_decay: MuonPolarExpress(params, lr=lr, weight_decay=weight_decay),\n",
        "    #     \"lr_values\": torch.linspace(1e-1, 1e-3, 5),\n",
        "    #     \"weight_decay_values\": torch.linspace(0, 1e-4, 5)\n",
        "    # },\n",
        "    \"Lion\": {\n",
        "        \"optimizer_fn\": lambda params, lr, weight_decay: Lion(params, lr=lr, weight_decay=weight_decay),\n",
        "        \"lr_values\": torch.linspace(1e-3, 1e-4, 6),\n",
        "        \"weight_decay_values\": torch.linspace(0, 0.3, 3)\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "results_pe_lion = {}\n",
        "\n",
        "print(\"Starting Hyperparameter Sweep...\")\n",
        "\n",
        "# Perform the hyperparameter sweep for each optimizer\n",
        "for opt_name, opt_info in optimizers_to_sweep_pe_and_lion.items():\n",
        "    print(f\"\\n--- Hyperparameter sweep for {opt_name} ---\")\n",
        "    optimizer_fn = opt_info[\"optimizer_fn\"]\n",
        "    lr_values = opt_info[\"lr_values\"]\n",
        "    weight_decay_values = opt_info[\"weight_decay_values\"]\n",
        "\n",
        "    results_pe_lion[opt_name] = {}\n",
        "\n",
        "    for lr in lr_values:\n",
        "        for weight_decay in weight_decay_values:\n",
        "            print(f\"Training with lr={lr}, weight_decay={weight_decay}\")\n",
        "            accuracy = train_and_evaluate(optimizer_fn, lr, weight_decay)\n",
        "            results_pe_lion[opt_name][(lr, weight_decay)] = accuracy\n",
        "            print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Print the results and best hyperparameters for each optimizer\n",
        "print(\"\\n--- Hyperparameter Sweep Results ---\")\n",
        "for opt_name, opt_results in results_pe_lion.items():\n",
        "    print(f\"\\n{opt_name}:\")\n",
        "    for (lr, weight_decay), accuracy in opt_results.items():\n",
        "        print(f\"  (lr={lr}, weight_decay={weight_decay}): {accuracy:.2f}%\")\n",
        "\n",
        "    best_params = max(opt_results, key=opt_results.get)\n",
        "    print(f\"Best hyperparameters for {opt_name}: (lr={best_params[0]}, weight_decay={best_params[1]}) with validation accuracy {opt_results[best_params]:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
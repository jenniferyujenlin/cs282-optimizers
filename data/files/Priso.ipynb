{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fantiflex/Modular_Manifold_Muon/blob/main/Fantine_q_coding_muon_solutions_ManifoldMuOn%2BLion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "7t6tuD6vGUsz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Intro\n",
        "\n",
        "In this problem, we will implement key components of the Muon optimizer in PyTorch. Then,  we will compare Muon against SGD and AdamW empirically to demonstrate the benefits of Muon.\n",
        "\n",
        "We will train a sample CNN architecture on CIFAR10 image data (initialized below)."
      ],
      "metadata": {
        "id": "ssD7ZMwsRHTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sample CNN for CIFAR-10\n",
        "class CIFAR10CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Dataset & loader with augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Increase batch size for more realistic training\n",
        "batch_size = 128\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "test_ds = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "fp4WlumrKZMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6694528a-3f77-42d7-a8ff-e20c730f9fb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Implementing Newton-Schulz\n",
        "\n",
        "The core of Muon is using matrix orthogonalization on a typical gradient update matrix. After orthogonalization:\n",
        "* Singular values become more uniform\n",
        "* Updates act across all directions in parameter space\n",
        "* The neural network can utilize its full parameter capacity (because parameters are all receiving nontrivial gradients)\n",
        "\n",
        "In Muon, matrix orthogonalization is done using Newton-Schulz. Newton-Schulz relies on iteration using an odd matrix polynomial. In this problem, we consider a very simple cubic polynomial, but many odd polynomials can be used (state-of-the-art implementations use specific, tuned quintic polynomial).\n",
        "\n",
        "**Complete the following code** to implement Newton-Schulz, using the matrix polynomial in the comments.\n"
      ],
      "metadata": {
        "id": "TjpGbmJRTPaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newton_schulz_orthogonalize(X: torch.Tensor, num_iters: int):\n",
        "    \"\"\"\n",
        "    Apply Newton-Schulz iterations to approximate orthogonalization.\n",
        "\n",
        "    This function applies the polynomial f(X) = (3X - X^3)/2 repeatedly to a normalized matrix,\n",
        "    which gradually forces all singular values to 1 while preserving singular vectors.\n",
        "\n",
        "    Args:\n",
        "      X (torch.Tensor): Input matrix to orthogonalize\n",
        "      num_iters (int): Number of Newton-Schulz iterations\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: Orthogonalized matrix\n",
        "    \"\"\"\n",
        "    dtype = X.dtype\n",
        "    # Use bfloat16 for potential speed/memory savings during the iterations\n",
        "    X = X.bfloat16()\n",
        "    # Recall from prior homeworks that we can transpose the matrix to speed up computation.\n",
        "    transposed = False\n",
        "    if X.size(-2) < X.size(-1):\n",
        "        transposed = True\n",
        "        X = X.mT\n",
        "\n",
        "    # Ensure spectral norm is at most sqrt(3)\n",
        "    norm = torch.linalg.norm(X, dim=(-2, -1), keepdim=True)\n",
        "    X = torch.div(X, norm + 1e-7) * (3**0.5)\n",
        "\n",
        "    ################################################################################\n",
        "    # Apply Newton-Schulz iterations using the cubic polynomial\n",
        "    for _ in range(num_iters):\n",
        "        X = (3 * X - torch.matmul(torch.matmul(X, X.mT), X)) / 2\n",
        "    ################################################################################\n",
        "    ################################################################################\n",
        "\n",
        "    if transposed:\n",
        "        X = X.mT\n",
        "\n",
        "    return X.to(dtype)"
      ],
      "metadata": {
        "id": "Mm5I-C5HIJQt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1\n",
        "\n",
        "Notice that in the above implementation, we scale the spectral norm to be at most sqrt(3). **Can you explain why we choose this particular scaling?**\n",
        "\n",
        "*(Hint: Inspect the roots of the cubic polynomial. What is the connection between the roots and the convergence properties of the singular values? You can refer to Discussion 4 for the answer)*"
      ],
      "metadata": {
        "id": "7YEjXmzqS_Ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Implementing Muon Update\n",
        "\n",
        "Now, we implement the update in a Muon optimizer. Given parameter matrix $W$ with momentum matrix $M$, the pseudocode for the Muon update proceeds as follows:\n",
        "\n",
        "```\n",
        "d_out, d_in = M.shape\n",
        "\n",
        "# Apply Newton-Schulz orthogonalization\n",
        "M ← newton_schulz_orthogonalize(M, ns_iters)\n",
        "        \n",
        "# Apply muP scaling factor for consistent update magnitude\n",
        "M ← M · sqrt(max(1, d_out / d_in))\n",
        "```\n",
        "\n",
        "Then, the Muon update is used later to update the parameters W:\n",
        "```\n",
        "# Update the parameter matrix\n",
        "W  ← W - lr * M\n",
        "```\n",
        "\n",
        "**Complete the following code** to implement the Muon update following the above pseudocode."
      ],
      "metadata": {
        "id": "YKyN6DSfaA7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def muon_update(grad, momentum, beta=0.95, ns_iters=5):\n",
        "    momentum.lerp_(grad, 1 - beta)  # momentum = beta * momentum + (1-beta) * grad\n",
        "    update = momentum.clone()\n",
        "\n",
        "    # If the parameter is a convolutional kernel, then flatten to a 2D matrix\n",
        "    original_shape = update.shape\n",
        "    reshaped = False\n",
        "    if update.ndim > 2:\n",
        "        reshaped = True\n",
        "        update = update.view(update.size(0), -1)\n",
        "\n",
        "    ################################################################################\n",
        "    # Apply orthogonalization\n",
        "    update = newton_schulz_orthogonalize(update, num_iters=ns_iters)\n",
        "    # Apply muP scaling\n",
        "    update.mul_(max(1, update.size(-2) / update.size(-1))**0.5)\n",
        "\n",
        "    ################################################################################\n",
        "    ################################################################################\n",
        "\n",
        "    # Restore shape if needed\n",
        "    if reshaped:\n",
        "        update = update.view(original_shape)\n",
        "\n",
        "    return update\n",
        "\n",
        "\n",
        "class Muon(optim.Optimizer):\n",
        "    def __init__(self, params, lr=0.01, beta=0.95, ns_iters=5,\n",
        "                weight_decay=0):\n",
        "\n",
        "        defaults = dict(lr=lr, beta=beta, ns_iters=ns_iters,\n",
        "                        weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            beta = group['beta']\n",
        "            ns_iters = group['ns_iters']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                # Get state for this parameter\n",
        "                state = self.state[p]\n",
        "                # Initialize momentum buffer if it doesn't exist\n",
        "                if 'momentum' not in state:\n",
        "                    state['momentum'] = torch.zeros_like(grad)\n",
        "\n",
        "                # Apply weight decay directly to parameters (AdamW style)\n",
        "                if weight_decay != 0:\n",
        "                    p.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                # Apply newton_schulz if parameter is a matrix\n",
        "                if p.ndim >= 2:\n",
        "                    update = muon_update(grad, state['momentum'],\n",
        "                                         beta=beta, ns_iters=ns_iters)\n",
        "                    # Apply update to parameters\n",
        "                    p.add_(update, alpha=-lr)\n",
        "                else:\n",
        "                    # For non-matrix parameters, i.e. bias, use standard momentum update\n",
        "                    momentum = state['momentum']\n",
        "                    momentum.mul_(beta).add_(grad)\n",
        "                    p.add_(momentum, alpha=-lr)\n",
        "\n",
        "        return None"
      ],
      "metadata": {
        "id": "k-m7S4cJnKx2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "\n",
        "Note that Muon requires that parameters are 2D matrices of shape $d_{out} \\times d_{in}$. However, we know that parameters that are convolutional kernels have shape $c_{out} \\times c_{in} \\times k \\times k$ where $c$ denotes number of channels and $k$ is kernel size.\n",
        "\n",
        "Modern implementations of convolutional layers will transform an input image $\\mathbf{x}$ of shape $c_{in} \\times h \\times w$ to $\\mathbf{x}'$ such that each column has size $c_{in} \\cdot k \\cdot k$ and corresponds to one flattened \"receptive field\" of the image (or one patch of the image that a convolutional filter passes over to compute one pixel in the output).\n",
        "\n",
        "Given this fact, **how do we modify the convolutional kernel into a $d_{out} \\times d_{in}$ matrix $C$ such that the output of the convolutional layer can be expressed as $C \\mathbf{x}'$**.\n"
      ],
      "metadata": {
        "id": "F6V1-eIIsPrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Empirical Evaluation of Muon\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Now, we'll train the CNN network on the CIFAR10 dataset using our Muon implementation, comparing performance on the test set against other popular optimizers in SGD and AdamW.\n",
        "\n",
        "First, in addition to SGD and AdamW, we consider two additional baseline optimizers that will help us better interpret our results:\n",
        "\n",
        "\n",
        "*   MuonSVD: Rather than using Newton-Schulz, we orthogonalize the momentum using SVD on the momentum matrix $M = U\\Sigma V^T$ and computing $UV^T$.\n",
        "*   AdamWMuP: We add the muP scaling on top of the AdamW optimizer. This is meant the help us better understand how much of the Muon performance is due to the orthogonalization step, and how much is simply from the muP scaling.\n",
        "\n",
        "The cell below implements these two additional optimizers. You do not have to implement anything.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wOa1BoaWeJ1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def muon_update_svd(grad, momentum, beta=0.95):\n",
        "    momentum.lerp_(grad, 1 - beta)  # momentum = beta * momentum + (1-beta) * grad\n",
        "    update = momentum.clone()\n",
        "\n",
        "    # If the parameter is a convolutional kernel, then flatten to a 2D matrix\n",
        "    original_shape = update.shape\n",
        "    reshaped = False\n",
        "    if update.ndim > 2:\n",
        "        reshaped = True\n",
        "        update = update.view(update.size(0), -1)\n",
        "\n",
        "    # Orthogonalization via SVD - specify full_matrices=False for reduced SVD\n",
        "    U, _, Vh = torch.linalg.svd(update, full_matrices=False)\n",
        "    update = torch.matmul(U, Vh)\n",
        "\n",
        "    # Apply muP scaling\n",
        "    update.mul_(max(1, update.size(-2) / update.size(-1))**0.5)\n",
        "\n",
        "    # Restore shape if needed\n",
        "    if reshaped:\n",
        "        update = update.view(original_shape)\n",
        "\n",
        "    return update\n",
        "\n",
        "\n",
        "class MuonSVD(optim.Optimizer):\n",
        "    def __init__(self, params, lr=0.01, beta=0.95, weight_decay=0):\n",
        "\n",
        "        defaults = dict(lr=lr, beta=beta, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            beta = group['beta']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                # Get state for this parameter\n",
        "                state = self.state[p]\n",
        "                # Initialize momentum buffer if it doesn't exist\n",
        "                if 'momentum' not in state:\n",
        "                    state['momentum'] = torch.zeros_like(grad)\n",
        "\n",
        "                # Apply weight decay directly to parameters (AdamW style)\n",
        "                if weight_decay != 0:\n",
        "                    p.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                # Apply newton_schulz if parameter is a matrix\n",
        "                if p.ndim >= 2:\n",
        "                    update = muon_update_svd(grad, state['momentum'], beta=beta)\n",
        "                    # Apply update to parameters\n",
        "                    p.add_(update, alpha=-lr)\n",
        "                else:\n",
        "                    # For non-matrix parameters, i.e. bias, use standard momentum update\n",
        "                    momentum = state['momentum']\n",
        "                    momentum.mul_(beta).add_(grad)\n",
        "                    p.add_(momentum, alpha=-lr)\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "class AdamWMuP(optim.Optimizer):\n",
        "    def __init__(self,  params, lr=0.01, betas=(0.9, 0.999), weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0: # Initialization\n",
        "                    state[\"step\"] = torch.tensor(0.0)\n",
        "                    state['momentum'] = torch.zeros_like(p)\n",
        "                    state['variance'] = torch.zeros_like(p)\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    p.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                state['step'] += 1\n",
        "                m = state['momentum']\n",
        "                m.lerp_(grad, 1 - group[\"betas\"][0])\n",
        "                v = state['variance']\n",
        "                v.lerp_(grad**2, 1 - group[\"betas\"][1])\n",
        "\n",
        "                m_hat = m / (1 - group[\"betas\"][0]**state['step'])\n",
        "                v_hat = v / (1 - group[\"betas\"][1]**state['step'])\n",
        "                u = m_hat / (torch.sqrt(v_hat) + 1e-16)\n",
        "\n",
        "                if p.ndim >= 2:\n",
        "                    # If the parameter is a convolutional kernel, then flatten to a 2D matrix\n",
        "                    original_shape = u.shape\n",
        "                    reshaped = False\n",
        "                    if u.ndim > 2:\n",
        "                        u = u.view(u.shape[0], -1)  # keep first dim, flatten the rest\n",
        "                        reshaped = True\n",
        "\n",
        "                    u.mul_(max(1, u.size(-2) / u.size(-1))**0.5)\n",
        "\n",
        "                    # Unflatten back to convolutional kernel\n",
        "                    if reshaped:\n",
        "                        u = u.view(original_shape)\n",
        "\n",
        "                p.add_(u, alpha=-lr)\n",
        "\n",
        "        return None"
      ],
      "metadata": {
        "id": "Ix6NZCLkF7qK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If you performed hyperparameter sweeping (optional part below), then replace the default hyperparameters with the values you found from your sweep. Then, run the following cell to investigate how good Muon is relative to other baseline optimizers. The cell should take less than 10 minutes if you use a GPU runtime on Colab."
      ],
      "metadata": {
        "id": "nd48iWs2EoF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define the optimizers you want to compare ---\n",
        "optimizers_dict = {\n",
        "    \"Muon\": lambda params: Muon(params, lr=1e-2, weight_decay=0),\n",
        "    \"SGD\": lambda params: torch.optim.SGD(params, lr=1e-2, momentum=0.9, weight_decay=1e-4),\n",
        "    \"AdamW\": lambda params: torch.optim.AdamW(params, lr=1e-3, weight_decay=1e-3),\n",
        "    \"MuonSVD\": lambda params: MuonSVD(params, lr=1e-2, weight_decay=0),\n",
        "    \"AdamWMuP\": lambda params: AdamWMuP(params, lr=1e-3, weight_decay=1e-3),\n",
        "\n",
        "    #to uncomment when Manifold Muon is fully implemented:\n",
        "    \"ManifoldMuon\": lambda params: ManifoldMuon(params, lr=1e-2, beta=0.95, weight_decay=0),\n",
        "    #to uncomment in part 6when Manifold Muon is fully implemented:\n",
        "    \"Lion\":         lambda params: Lion(params, lr=3e-4, betas=(0.9, 0.99), weight_decay=3e-3),\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "results = {}  # store loss curves\n",
        "accuracy_results = {}  # store accuracy curves\n",
        "\n",
        "# --- Train for each optimizer ---\n",
        "for opt_name, opt_fn in optimizers_dict.items():\n",
        "    print(f\"\\n--- Training with {opt_name} ---\")\n",
        "    model = CIFAR10CNN().to(device)  # re-init model each time\n",
        "    optimizer = opt_fn(model.parameters())\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    epoch_times = []\n",
        "\n",
        "    for epoch in range(1, 6):  # Train for 5 epochs\n",
        "        model.train()\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0\n",
        "\n",
        "        for i, (x, y) in enumerate(train_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Print progress\n",
        "            if (i+1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch}/5], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "        epoch_times.append(epoch_duration)\n",
        "\n",
        "        # Evaluate\n",
        "        test_acc = evaluate(model, test_loader)\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "        accuracies.append(test_acc)\n",
        "\n",
        "        print(f\"{opt_name} | Epoch {epoch}, avg loss: {avg_loss:.4f}, test acc: {test_acc:.2f}%, time: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "    results[opt_name] = losses\n",
        "    accuracy_results[opt_name] = accuracies\n",
        "\n",
        "    # Calculate and print total training time\n",
        "    total_time = sum(epoch_times)\n",
        "    print(f\"{opt_name} | Total training time: {total_time:.2f} seconds\")\n",
        "\n",
        "# --- Plot results ---\n",
        "# 1. Loss vs Epoch\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "for opt_name, losses in results.items():\n",
        "    plt.plot(range(1, len(losses)+1), losses, label=opt_name, marker=\"o\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.title(\"Training Loss vs Epoch for Different Optimizers\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 2. Accuracy vs Epoch\n",
        "plt.subplot(1, 2, 2)\n",
        "for opt_name, accuracies in accuracy_results.items():\n",
        "    plt.plot(range(1, len(accuracies)+1), accuracies, label=opt_name, marker=\"o\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Test Accuracy vs Epoch for Different Optimizers\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c2SVu7okFWVE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f08f690-4ede-408e-8196-70b5c92242a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training with Muon ---\n",
            "Epoch [1/5], Step [100/391], Loss: 1.5381\n",
            "Epoch [1/5], Step [200/391], Loss: 1.3171\n",
            "Epoch [1/5], Step [300/391], Loss: 1.2939\n",
            "Muon | Epoch 1, avg loss: 1.5055, test acc: 64.89%, time: 19.90 seconds\n",
            "Epoch [2/5], Step [100/391], Loss: 1.2361\n",
            "Epoch [2/5], Step [200/391], Loss: 1.1402\n",
            "Epoch [2/5], Step [300/391], Loss: 0.9197\n",
            "Muon | Epoch 2, avg loss: 1.0308, test acc: 70.96%, time: 20.00 seconds\n",
            "Epoch [3/5], Step [100/391], Loss: 0.8802\n",
            "Epoch [3/5], Step [200/391], Loss: 0.9294\n",
            "Epoch [3/5], Step [300/391], Loss: 0.6503\n",
            "Muon | Epoch 3, avg loss: 0.8464, test acc: 74.74%, time: 19.01 seconds\n",
            "Epoch [4/5], Step [100/391], Loss: 0.7283\n",
            "Epoch [4/5], Step [200/391], Loss: 0.8111\n",
            "Epoch [4/5], Step [300/391], Loss: 0.5790\n",
            "Muon | Epoch 4, avg loss: 0.7216, test acc: 79.17%, time: 20.03 seconds\n",
            "Epoch [5/5], Step [100/391], Loss: 0.6710\n",
            "Epoch [5/5], Step [200/391], Loss: 0.4724\n",
            "Epoch [5/5], Step [300/391], Loss: 0.6222\n",
            "Muon | Epoch 5, avg loss: 0.6458, test acc: 80.50%, time: 20.51 seconds\n",
            "Muon | Total training time: 99.45 seconds\n",
            "\n",
            "--- Training with SGD ---\n",
            "Epoch [1/5], Step [100/391], Loss: 1.9508\n",
            "Epoch [1/5], Step [200/391], Loss: 1.8186\n",
            "Epoch [1/5], Step [300/391], Loss: 1.6476\n",
            "SGD | Epoch 1, avg loss: 1.8458, test acc: 47.54%, time: 19.01 seconds\n",
            "Epoch [2/5], Step [100/391], Loss: 1.4449\n",
            "Epoch [2/5], Step [200/391], Loss: 1.3133\n",
            "Epoch [2/5], Step [300/391], Loss: 1.3852\n",
            "SGD | Epoch 2, avg loss: 1.4615, test acc: 52.87%, time: 18.54 seconds\n",
            "Epoch [3/5], Step [100/391], Loss: 1.4021\n",
            "Epoch [3/5], Step [200/391], Loss: 1.5568\n",
            "Epoch [3/5], Step [300/391], Loss: 1.3347\n",
            "SGD | Epoch 3, avg loss: 1.2795, test acc: 60.44%, time: 18.08 seconds\n",
            "Epoch [4/5], Step [100/391], Loss: 1.1812\n",
            "Epoch [4/5], Step [200/391], Loss: 1.1557\n",
            "Epoch [4/5], Step [300/391], Loss: 1.1917\n",
            "SGD | Epoch 4, avg loss: 1.1489, test acc: 63.31%, time: 16.47 seconds\n",
            "Epoch [5/5], Step [100/391], Loss: 1.2848\n",
            "Epoch [5/5], Step [200/391], Loss: 1.1396\n",
            "Epoch [5/5], Step [300/391], Loss: 1.1851\n",
            "SGD | Epoch 5, avg loss: 1.0747, test acc: 66.03%, time: 16.54 seconds\n",
            "SGD | Total training time: 88.64 seconds\n",
            "\n",
            "--- Training with AdamW ---\n",
            "Epoch [1/5], Step [100/391], Loss: 1.6812\n",
            "Epoch [1/5], Step [200/391], Loss: 1.6122\n",
            "Epoch [1/5], Step [300/391], Loss: 1.5396\n",
            "AdamW | Epoch 1, avg loss: 1.6483, test acc: 54.02%, time: 16.74 seconds\n",
            "Epoch [2/5], Step [100/391], Loss: 1.0900\n",
            "Epoch [2/5], Step [200/391], Loss: 1.5178\n",
            "Epoch [2/5], Step [300/391], Loss: 1.2864\n",
            "AdamW | Epoch 2, avg loss: 1.2797, test acc: 62.19%, time: 16.90 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3\n",
        "\n",
        "**Which optimizer performed best between Muon, SGD, and AdamW?** Also **copy the resulting plots** into the submission as well."
      ],
      "metadata": {
        "id": "HCOdMfEFfHmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4\n",
        "\n",
        "Compare the loss curves between Muon and MuonSVD. **Are the results expected? Explain why.**"
      ],
      "metadata": {
        "id": "DqoNyPtLMf3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 5\n",
        "\n",
        "The Muon optimizer contains two key differences: (1) orthogonalization of the momentum, and (2) muP scaling of the momentum. **Between orthogonalization and muP scaling, which seemed to matter more?** Reference the loss curves to justify your answer.  "
      ],
      "metadata": {
        "id": "_RVJi_lXM6JD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 6 (Optional)\n",
        "\n",
        "Our implementation of Newton-Schulz is suboptimal in the polynomial used for convergence.\n",
        "The community has developed quintic polynomials that converge faster while still being efficient. **Implement an improved Newton-Schulz and compare. Comment on the speed advantage of the improved Muon relative to the MuonSVD.**\n",
        "\n",
        "*(Hint: You can modify the number of iterations by setting the ns_iters parameter in the Muon optimizer)*"
      ],
      "metadata": {
        "id": "XyAeAbB2bssc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4 (Optional): Hyperparameter sweeps\n",
        "\n",
        "To ensure that we are making fair comparisons, we will sweep over both learning rate and weight decay for both Muon and AdamW (the likely strongest competing optimizer). We choose these two parameters because empirically, they have the greatest effect on training.\n",
        "\n",
        "Running the sweep should take less than 1 hour total on a GPU runtime."
      ],
      "metadata": {
        "id": "oZ9TX-EqwdfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "def train_and_evaluate(optimizer_fn, lr, weight_decay, num_epochs=5):\n",
        "    \"\"\"Trains and evaluates the CIFAR10CNN with a given optimizer and hyperparameters.\"\"\"\n",
        "    model = CIFAR10CNN().to(device)\n",
        "    optimizer = optimizer_fn(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    best_accuracy = 0\n",
        "\n",
        "    # Split training data into training and validation sets\n",
        "    train_size = int(0.8 * len(train_ds))\n",
        "    val_size = len(train_ds) - train_size\n",
        "    train_dataset, val_dataset = random_split(train_ds, [train_size, val_size])\n",
        "\n",
        "    train_loader_split = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        for i, (x, y) in enumerate(train_loader_split):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        val_acc = evaluate(model, val_loader)\n",
        "        best_accuracy = max(best_accuracy, val_acc)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return best_accuracy\n",
        "\n",
        "# Define the optimizers and their hyperparameter search spaces\n",
        "optimizers_to_sweep = {\n",
        "    \"Muon\": {\n",
        "        \"optimizer_fn\": lambda params, lr, weight_decay: Muon(params, lr=lr, weight_decay=weight_decay),\n",
        "        \"lr_values\": [1e-2, 5e-3, 1e-3, 5e-4],\n",
        "        \"weight_decay_values\": [0, 1e-4, 1e-3]\n",
        "    },\n",
        "    \"AdamW\": {\n",
        "        \"optimizer_fn\": lambda params, lr, weight_decay: torch.optim.AdamW(params, lr=lr, weight_decay=weight_decay),\n",
        "        \"lr_values\": [1e-2, 5e-3, 1e-3, 5e-4],\n",
        "        \"weight_decay_values\": [0, 1e-4, 1e-3]\n",
        "    }\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Perform the hyperparameter sweep for each optimizer\n",
        "for opt_name, opt_info in optimizers_to_sweep.items():\n",
        "    print(f\"\\n--- Hyperparameter sweep for {opt_name} ---\")\n",
        "    optimizer_fn = opt_info[\"optimizer_fn\"]\n",
        "    lr_values = opt_info[\"lr_values\"]\n",
        "    weight_decay_values = opt_info[\"weight_decay_values\"]\n",
        "\n",
        "    results[opt_name] = {}\n",
        "\n",
        "    for lr in lr_values:\n",
        "        for weight_decay in weight_decay_values:\n",
        "            print(f\"Training with lr={lr}, weight_decay={weight_decay}\")\n",
        "            accuracy = train_and_evaluate(optimizer_fn, lr, weight_decay)\n",
        "            results[opt_name][(lr, weight_decay)] = accuracy\n",
        "            print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Print the results and best hyperparameters for each optimizer\n",
        "print(\"\\n--- Hyperparameter Sweep Results ---\")\n",
        "for opt_name, opt_results in results.items():\n",
        "    print(f\"\\n{opt_name}:\")\n",
        "    for (lr, weight_decay), accuracy in opt_results.items():\n",
        "        print(f\"  (lr={lr}, weight_decay={weight_decay}): {accuracy:.2f}%\")\n",
        "\n",
        "    best_params = max(opt_results, key=opt_results.get)\n",
        "    print(f\"Best hyperparameters for {opt_name}: (lr={best_params[0]}, weight_decay={best_params[1]}) with validation accuracy {opt_results[best_params]:.2f}%\")"
      ],
      "metadata": {
        "id": "XLfdpF5h_ReG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "d12b162e-c85a-4599-9a0d-a40eb7fe0006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Hyperparameter sweep for Muon ---\n",
            "Training with lr=0.01, weight_decay=0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2276487585.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweight_decay_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training with lr={lr}, weight_decay={weight_decay}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation Accuracy: {accuracy:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2276487585.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(optimizer_fn, lr, weight_decay, num_epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 7 (Optional)\n",
        "\n",
        "**What were the best choices for hyperparameters for Muon? What about for AdamW?**"
      ],
      "metadata": {
        "id": "PVQE4jRzDkU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Part 5 : Using manifold to constrain our learning**\n",
        "We seek to solve the inner Manifold MuOn dual step efficiently and reliably using plain dual ascent. This part is hugely inspired by Jérémy Bernstein's paper \"Modular Manifold\".\n"
      ],
      "metadata": {
        "id": "Sm2uk-QExB4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A bit of context :** Manifold Muon relies on the idea that parameters should be constrained to live on a manifold. In the Modular Manifolds post, Bernstein focuses on the Stiefel manifold: the set of matrices whose columns are orthonormal and therefore have unit condition number. Constraining weights to Stiefel keeps them *“well-conditioned”* throughout training: none of the singular values can blow up or collapse to zero, so the layer neither explodes nor kills directions in the input space.\n",
        "\n",
        "To optimize on this manifold, Manifold Muon does not just take a standard gradient step and project back. Instead, it explicitly searches for the steepest descent direction inside the tangent space of the manifold, under a spectral-norm constraint on the update.\n",
        "\n",
        "Concretely, for a weight matrix W and gradient G, manifold Muon solves a small convex problem: find an update A that  \n",
        "\n",
        "1.   is tangent to the Stiefel manifold at W\n",
        "2.   has bounded spectral norm\n",
        "3.   decreases the loss as fast as possible\n",
        "\n",
        "\n",
        "The “modular” part comes from how this is organized at the network level. Bernstein treats each layer (or block) as a module with three pieces of structure:\n",
        "\n",
        "\n",
        "1.   a forward map f(w,x)\n",
        "2.   a manifold where it lives (here Stiefel manifold)\n",
        "3.   a norm used to measure the size of the update (here spectral norm)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T7pLJjqA0x-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spectral norm: how “strong” is a linear layer?\n",
        "\n",
        "Before we talk about manifolds or fancy optimization, we need a way to measure **how aggressive** a linear layer can be.\n",
        "Given a weight matrix (W), the **spectral norm** tells us the maximum factor by which (W) can stretch any input vector of unit length. In other words, it is a quantitative measure of the “strength” or **worst-case amplification** of the layer. In this lab, we will use the spectral norm to control both the size of weight matrices and the size of the updates we apply to them, which is a key ingredient behind the Muon and manifold Muon optimizers.\n",
        "\n",
        "\n",
        "**Implement the function spectral_norm(W)**"
      ],
      "metadata": {
        "id": "3zVZWsryJfEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def spectral_norm(W: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute the spectral norm of a matrix W.\n",
        "    Args:\n",
        "        W: 2D tensor of shape (m, n).\n",
        "    Returns:\n",
        "        A scalar tensor: the largest singular value of W.\n",
        "    \"\"\"\n",
        "    ################################################################################\n",
        "    # TODO: YOUR CODE HERE\n",
        "    U, S, Vt = np.linalg.svd(W, full_matrices=False)\n",
        "    ################################################################################\n",
        "    ################################################################################\n",
        "    return float(S.max())\n"
      ],
      "metadata": {
        "id": "B-TpXdOvJlLN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix sign: keep the direction, normalize the strength\n",
        "\n",
        "Once we know how to measure the “strength” of a matrix with the spectral norm, the next step is to **control** that strength. The matrix sign operator does exactly this. Given a matrix \\(M\\), we can factor it as $M = U S V^\\top$, where \\(S\\) contains the singular values. The matrix sign\n",
        "\n",
        "\n",
        "$\\mathrm{msign}(M) = U V^\\top$\n",
        "\n",
        "keeps the same singular directions (\\(U\\) and \\(V\\)) but replaces all singular values by 1. In other words, it preserves **where** the matrix pushes vectors, but normalizes **how much** it can stretch them. This gives us a convenient way to build updates whose spectral norm is exactly 1, and then scale them by a learning rate $\\eta$. In the simplified manifold Muon step later in the lab, we will use `msign` to turn a gradient into a **direction of steepest descent with controlled size** in spectral norm.\n",
        "\n",
        "\n",
        "**Implement the function msign(M)**"
      ],
      "metadata": {
        "id": "wpjuKCk7KgOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def msign(M: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Matrix sign of M based on its SVD: M = U S V^T -> U V^T.\n",
        "    The result has spectral norm 1 (up to numerical error).\n",
        "    Args:\n",
        "        M: 2D tensor of shape (m, n).\n",
        "    Returns:\n",
        "        M_sign: 2D tensor of shape (m, n) with all singular values ≈ 1.\n",
        "    \"\"\"\n",
        "    ################################################################################\n",
        "    # TODO: YOUR CODE HERE\n",
        "    U, S, Vt = torch.linalg.svd(M, full_matrices=False)\n",
        "    M_sign = U @ Vt\n",
        "    ################################################################################\n",
        "    ################################################################################\n",
        "    return M_sign\n"
      ],
      "metadata": {
        "id": "txdXf0GBK7cV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Projection on Stiefel**\n",
        "\n",
        "Implement the function project_to_stiefel(W) that computes the SVD of W and returns its projection on the Stiefel manifold."
      ],
      "metadata": {
        "id": "nxCrA-Po3INl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def project_to_stiefel(W):\n",
        "    '''\n",
        "    Project a 2D weight matrix W onto the (column) Stiefel manifold\n",
        "    W is the weigths matrix, size n_in x n_out\n",
        "    using the SVD: W = U S V^T  ->  W_proj = U V^T.\n",
        "    '''\n",
        "    ################################################################################\n",
        "    # TODO: YOUR CODE HERE\n",
        "    U, S, Vh = torch.linalg.svd(W, full_matrices=False)\n",
        "    W_proj = U @ Vh\n",
        "    ################################################################################\n",
        "    ################################################################################\n",
        "    return W_proj"
      ],
      "metadata": {
        "id": "24jbs0853xMP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A simplified manifold Muon step on the Stiefel manifold\n",
        "\n",
        "Now that we can (i) measure the strength of a matrix with the spectral norm, (ii) normalize a matrix using the matrix sign, and (iii) project weights onto the Stiefel manifold, we can put everything together into a **simplified manifold Muon update**.\n",
        "\n",
        "We consider a linear layer with weight matrix \\(W\\) constrained to the Stiefel manifold (its columns are orthonormal). Given the Euclidean gradient $G = \\partial \\mathcal{L} / \\partial W$, we will:\n",
        "\n",
        "1. Project the gradient onto the **tangent space** of the Stiefel manifold (Riemannian gradient).\n",
        "2. Normalize this tangent gradient using the **matrix sign**, so that the update has controlled spectral norm.\n",
        "3. Take a step of size `lr` in that direction.\n",
        "4. Retract back onto the Stiefel manifold using our `project_to_stiefel` function.\n",
        "\n",
        "This gives us a toy version of a *manifold Muon* step: an update that both respects the manifold constraint on the weights and has a bounded “strength” in spectral norm.\n",
        "\n",
        "**Implement this manifold_muon_step function**\n"
      ],
      "metadata": {
        "id": "Wdm0201qLzjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def manifold_muon_step(W: torch.Tensor,\n",
        "                       G: torch.Tensor,\n",
        "                       lr: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    One simplified 'manifold Muon' step on the Stiefel manifold.\n",
        "    Steps:\n",
        "    1. Compute the Riemannian (tangent) gradient on the Stiefel manifold:\n",
        "         G_tan = G - W sym(W^T G),\n",
        "       where sym(A) = 0.5 * (A + A^T).\n",
        "    2. Normalize this tangent gradient with `msign` to bound its\n",
        "       spectral norm (Muon-style).\n",
        "    3. Take a step of size `lr` in that direction.\n",
        "    4. Retract back onto the Stiefel manifold using `project_to_stiefel`.\n",
        "    Args:\n",
        "        W: current weight matrix on (or near) the Stiefel manifold, shape (m, n)\n",
        "        G: Euclidean gradient dL/dW, same shape as W\n",
        "        lr: learning rate (float)\n",
        "    Returns:\n",
        "        W_new: updated weight matrix on the Stiefel manifold (m, n)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        ################################################################################\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # ---- 1. Riemannian (tangent) gradient on Stiefel ----\n",
        "        A = W.T @ G\n",
        "        sym_A = 0.5 * (A + A.T)\n",
        "        G_tan = G - W @ sym_A\n",
        "        # ---- 2. Normalize with msign (spectral norm control) ----\n",
        "        # Turn the tangent gradient into a direction with spectral norm ~ 1\n",
        "        G_muon = msign(G_tan)\n",
        "        # ---- 3. Take a step in that direction ----\n",
        "        W_update = W - lr * G_muon\n",
        "        # ---- 4. Retract back onto Stiefel ----\n",
        "        W_new = project_to_stiefel(W_update)\n",
        "        ################################################################################\n",
        "        ################################################################################\n",
        "    return W_new\n",
        "\n",
        "def riemannian_grad_stiefel(W_mat: torch.Tensor,\n",
        "                            G_mat: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Riemannian (tangent) gradient on the Stiefel manifold.\n",
        "\n",
        "    W_mat: (m, n), current weight matrix\n",
        "    G_mat: (m, n), Euclidean gradient dL/dW\n",
        "\n",
        "    Returns:\n",
        "        G_tan: (m, n), tangent gradient\n",
        "    \"\"\"\n",
        "    # A = W^T G\n",
        "    A = W_mat.T @ G_mat\n",
        "    # sym(A) = (A + A^T) / 2\n",
        "    sym_A = 0.5 * (A + A.T)\n",
        "    # Tangent gradient\n",
        "    G_tan = G_mat - W_mat @ sym_A\n",
        "    return G_tan\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def manifold_muon_update(W: torch.Tensor,\n",
        "                         grad: torch.Tensor,\n",
        "                         momentum: torch.Tensor,\n",
        "                         beta: float = 0.95) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    One manifold-Muon-style *direction* for a single parameter tensor W.\n",
        "\n",
        "    Steps:\n",
        "    1. Flatten W and grad to a (m, n) matrix if needed.\n",
        "    2. Compute the Riemannian gradient on the Stiefel manifold.\n",
        "    3. Apply momentum in the tangent space (exponential moving average).\n",
        "    4. Apply msign to bound the spectral norm of the update.\n",
        "\n",
        "    Args:\n",
        "        W: parameter tensor (>= 2D for matrix/conv kernels)\n",
        "        grad: gradient tensor, same shape as W\n",
        "        momentum: momentum buffer, same shape as W\n",
        "        beta: momentum hyperparameter (0 <= beta < 1)\n",
        "\n",
        "    Returns:\n",
        "        update: tensor with same shape as W\n",
        "    \"\"\"\n",
        "    original_shape = W.shape\n",
        "\n",
        "    # ---- 1. Flatten to (m, n) if needed ----\n",
        "    reshaped = False\n",
        "    if W.ndim > 2:\n",
        "        reshaped = True\n",
        "        m = W.size(0)\n",
        "        W_mat   = W.view(m, -1)\n",
        "        G_mat   = grad.view_as(W_mat)\n",
        "        M_mat   = momentum.view_as(W_mat)\n",
        "    else:\n",
        "        W_mat = W\n",
        "        G_mat = grad\n",
        "        M_mat = momentum\n",
        "    # ---- 2. Riemannian gradient on Stiefel ----\n",
        "    G_tan = riemannian_grad_stiefel(W_mat, G_mat)\n",
        "    # ---- 3. Momentum in tangent space ----\n",
        "    M_mat.lerp_(G_tan, 1 - beta)\n",
        "    # ---- 4. Matrix sign to bound spectral norm ----\n",
        "    update_mat = msign(M_mat) # produces a direction with spectral norm ≈ 1\n",
        "\n",
        "    # ---- 5. Restore original shape if needed ----\n",
        "    if reshaped:\n",
        "        update = torch.zeros_like(W)\n",
        "        update.view_as(update_mat).copy_(update_mat)\n",
        "    else:\n",
        "        update = update_mat\n",
        "\n",
        "    return update"
      ],
      "metadata": {
        "id": "e0GTX-07MCpr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ManifoldMuon(optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Simplified manifold Muon optimizer:\n",
        "\n",
        "    - For matrix-like parameters (ndim >= 2), we:\n",
        "        * compute a manifold-Muon direction (tangent + msign),\n",
        "        * take an update step,\n",
        "        * project back to the Stiefel manifold.\n",
        "\n",
        "    - For vector-like parameters (bias, etc.), we fall back to\n",
        "      standard momentum.\n",
        "\n",
        "    It follows the same style as MuonSVD.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=0.01, beta=0.95, weight_decay=0.0):\n",
        "        defaults = dict(lr=lr, beta=beta, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            # optional closure for line-search style methods\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            beta = group['beta']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "\n",
        "                # State for this parameter\n",
        "                state = self.state[p]\n",
        "                if 'momentum' not in state:\n",
        "                    # Momentum buffer has same shape as parameter\n",
        "                    state['momentum'] = torch.zeros_like(p)\n",
        "\n",
        "                momentum = state['momentum']\n",
        "\n",
        "                # AdamW-style decoupled weight decay\n",
        "                if weight_decay != 0:\n",
        "                    p.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                # ---- Matrix-like parameters: manifold Muon on Stiefel ----\n",
        "                if p.ndim >= 2:\n",
        "                    # Compute update direction in the tangent space\n",
        "                    update = manifold_muon_update(\n",
        "                        W=p,\n",
        "                        grad=grad,\n",
        "                        momentum=momentum,\n",
        "                        beta=beta,\n",
        "                    )\n",
        "\n",
        "                    # Gradient step\n",
        "                    p.add_(update, alpha=-lr)\n",
        "\n",
        "                    # Project back onto Stiefel (on the flattened view)\n",
        "                    if p.ndim > 2:\n",
        "                        m = p.size(0)\n",
        "                        W_mat = p.view(m, -1)\n",
        "                        W_proj = project_to_stiefel(W_mat)\n",
        "                        W_mat.copy_(W_proj)\n",
        "                    else:\n",
        "                        W_proj = project_to_stiefel(p)\n",
        "                        p.copy_(W_proj)\n",
        "\n",
        "                # ---- 1D parameters: standard momentum update ----\n",
        "                else:\n",
        "                    momentum.mul_(beta).add_(grad)\n",
        "                    p.add_(momentum, alpha=-lr)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UWvxQQRyNhB8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modify the ploting block so that you can compare Manifold MuOn with other optimizers.**"
      ],
      "metadata": {
        "id": "EEczYYcChqR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Which optimizer achieves the lowest training loss after 5 epochs?"
      ],
      "metadata": {
        "id": "bOJTv6YohylV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9** : Compare Muon vs MuonSVD.\n",
        "\n",
        "Which one converges faster in terms of training loss?\n",
        "\n",
        "Which one achieves better test accuracy?\n",
        "How might the SVD-based update help explain this behavior?"
      ],
      "metadata": {
        "id": "z0lEZDq9h6P8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 6 - Lion: a sign-based momentum optimizer**\n",
        "In this section, we introduce **Lion** (“EvoLved Sign Momentum”), an optimizer discovered by symbolic program search (Chen et al., 2023). Instead of adapting per-parameter learning rates like AdamW, Lion keeps only a single momentum vector and uses the **sign** of a blended gradient–momentum direction to update the weights. This means every parameter receives an update with the same magnitude (scaled by the learning rate); only the **direction** differs.\n",
        "\n",
        "Lion also uses decoupled weight decay, similar to AdamW, but theory shows that this can be interpreted as solving a constrained problem where the parameters are kept inside an ℓ∞ ball, preventing any coordinate from growing too large. Compared to AdamW, Lion is more memory efficient (no second-moment buffer) and often prefers **smaller learning rates** and **larger weight decay**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3FFcuAGacZA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Lion(optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Keeps a single momentum buffer and updates parameters using the sign of a blended gradient–momentum direction.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=3e-4, betas=(0.9, 0.99), weight_decay=3e-3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            params: iterable of parameters or parameter groups.\n",
        "            lr: learning rate (typically 3–10x smaller than AdamW).\n",
        "            betas: (beta1, beta2) coefficients for the two EMAs.\n",
        "            weight_decay: decoupled weight decay coefficient.\n",
        "        \"\"\"\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        beta1, beta2 = betas\n",
        "        if not 0.0 <= beta1 < 1.0:\n",
        "            raise ValueError(f\"Invalid beta1: {beta1}\")\n",
        "        if not 0.0 <= beta2 < 1.0:\n",
        "            raise ValueError(f\"Invalid beta2: {beta2}\")\n",
        "\n",
        "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group[\"lr\"]\n",
        "            beta1, beta2 = group[\"betas\"]\n",
        "            weight_decay = group[\"weight_decay\"]\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "\n",
        "                # Decoupled weight decay (AdamW-style)\n",
        "                if weight_decay != 0.0:\n",
        "                    p.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                # State init\n",
        "                state = self.state[p]\n",
        "                if \"exp_avg\" not in state:\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
        "\n",
        "                m = state[\"exp_avg\"]\n",
        "\n",
        "                # 1) Build blended direction and take a sign step\n",
        "                #    update_dir ~ beta1 * m + (1 - beta1) * grad\n",
        "                update_dir = m * beta1 + grad * (1.0 - beta1)\n",
        "                p.add_(update_dir.sign(), alpha=-lr)\n",
        "\n",
        "                # 2) Update momentum with beta2\n",
        "                m.mul_(beta2).add_(grad, alpha=1.0 - beta2)\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "2WNQN0NDctaS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10 :** Briefly explain how Lion's update is different from AdamW in terms of:\n",
        "\n",
        "\n",
        "\n",
        "*   number of state tensors per parameter\n",
        "*   use (or non-use) of the gradient magnitude.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DZKgqtYKeQtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 11** : Uncomment the Lion contribution to the optimizer dictionnary. Compare Lion and AdamW:\n",
        "\n",
        "\n",
        "\n",
        "*   Which optimizer reaches lower training loss after 5 epochs?\n",
        "*   Which optimizer achieves higher test accuracy?\n",
        "*   Does Lion seem to converge faster early in training, slower, or about the same?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LGpqrC-Pej3p"
      }
    }
  ]
}
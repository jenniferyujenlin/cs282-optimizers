{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scZxX6oEKzZn"
      },
      "source": [
        "## Maximal Update Parameterization\n",
        "\n",
        "In this problem, we will examine the training of a simple MLP with hidden layers of varying widths. We will then investigate the maximal update parameterization (muP) which will allow us to use a single global learning rate to jointly train layers of any width.\n",
        "\n",
        "Note: This homework question is new this year and it is messier than usual. We felt it was worth it to get it out so you can play with these new techniques. If you're feeling stuck, don't hesistate to ask questions on Ed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRp2k4Dbli_k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_sizes = [8, 16, 32, 64, 128], num_classes=10):\n",
        "        super(MLP, self).__init__()\n",
        "        all_hidden_sizes = [input_size] + hidden_sizes + [num_classes]\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(all_hidden_sizes)-1):\n",
        "            self.layers.append(nn.Linear(all_hidden_sizes[i], all_hidden_sizes[i+1]))\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        activations = []\n",
        "        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 28*28)\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = self.sigmoid(layer(x))\n",
        "            activations.append(x)\n",
        "        x = self.layers[-1](x)\n",
        "        activations = activations[1:]\n",
        "        return x, [a.detach() for a in activations]\n",
        "\n",
        "# Load MNIST data\n",
        "(train_images, train_labels), (valid_images, valid_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "train_images = train_images.astype(np.float32) / 255.0\n",
        "valid_images = valid_images.astype(np.float32) / 255.0\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_images = torch.from_numpy(train_images)\n",
        "train_labels = torch.from_numpy(train_labels).long()\n",
        "valid_images = torch.from_numpy(valid_images)\n",
        "valid_labels = torch.from_numpy(valid_labels).long()\n",
        "\n",
        "def rms(x, dim):\n",
        "    return torch.sqrt(torch.mean(x**2, dim=dim))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j68v19tfX5Vq"
      },
      "outputs": [],
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "from typing import Any\n",
        "class SimpleAdam(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Any,\n",
        "        lr: float = 1e-1,\n",
        "        b1: float = 0.9,\n",
        "        b2: float = 0.999,\n",
        "    ):\n",
        "        defaults = dict(lr=lr, b1=b1, b2=b2,)\n",
        "        super(SimpleAdam, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0: # Initialization\n",
        "                    state[\"step\"] = torch.tensor(0.0)\n",
        "                    state['momentum'] = torch.zeros_like(p)\n",
        "                    state['variance'] = torch.zeros_like(p)\n",
        "\n",
        "                state['step'] += 1\n",
        "                m = state['momentum']\n",
        "                m.lerp_(grad, 1-group[\"b1\"])\n",
        "                v = state['variance']\n",
        "                v.lerp_(grad**2, 1-group[\"b2\"])\n",
        "\n",
        "                m_hat = m / (1 - group[\"b1\"]**state['step'])\n",
        "                v_hat = v / (1 - group[\"b2\"]**state['step'])\n",
        "                u = m_hat / (torch.sqrt(v_hat) + 1e-16)\n",
        "\n",
        "                p.add_(u, alpha=-group['lr'])\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "XIYSY34qX80r",
        "outputId": "87e8d0cd-8806-47c9-f102-f1cd96d38506"
      },
      "outputs": [],
      "source": [
        "batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "def train_one_step(mlp=MLP, hiddens=[8, 16, 64, 64, 64, 256, 256, 1024], optimizer=SimpleAdam, label=\"Adam\", lr=0.01):\n",
        "    model = mlp(hidden_sizes=hiddens).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer(model.parameters(), lr=lr)\n",
        "\n",
        "    prev_activations = None\n",
        "    for i in range(2):\n",
        "        images_batch = train_images[batch_idx]\n",
        "        labels_batch = train_labels[batch_idx]\n",
        "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, activations = model(images_batch)\n",
        "        loss = criterion(outputs, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i > 0:\n",
        "            print([a.shape for a in activations])\n",
        "            activation_deltas = [a - pa for a, pa in zip(activations, prev_activations)]\n",
        "            activation_deltas_rms = [torch.mean(rms(a, dim=-1)) for a in activation_deltas]\n",
        "        prev_activations = activations\n",
        "\n",
        "    # plot deltas\n",
        "    deltas = np.array(activation_deltas_rms)\n",
        "    fig, axs = plt.subplots(1, figsize=(8, 4))\n",
        "    axs.set_title(f'RMS of activation deltas per layer ({label})')\n",
        "    axs.set_xlabel('Hidden Size of activation')\n",
        "    axs.bar(np.arange(deltas.shape[0]), deltas)\n",
        "    axs.set_xticks(np.arange(deltas.shape[0]))\n",
        "    axs.set_xticklabels(hiddens[1:])\n",
        "    plt.show()\n",
        "train_one_step(optimizer=SimpleAdam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykmT0CjoLaCH"
      },
      "source": [
        "## a. Examining the norms of a heterogenous MLP.\n",
        "\n",
        "Run the above cell, which trains a neural network for a single gradient step, then examines the effect of that step on the resulting activations. What are the dimensions of each layer in the neural network? \n",
        "\n",
        "*Answer:* \n",
        "\n",
        "How does the dimensionality of the layer affect the RMS norm of the activation deltas?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "Change the widths of some of your neural network layers, and recreate the plot -- did the RMS values change as expected?\n",
        "\n",
        "*Answer:*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "s0-GhF6eNN5I",
        "outputId": "8d21a6b9-8027-49a6-c6b5-a1843b1b50e5"
      },
      "outputs": [],
      "source": [
        "# TODO: Call some plotting code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOzosCNjMlMZ"
      },
      "source": [
        "## b. Examining the norms of the updates to the weights.\n",
        "\n",
        "In the provided code above, we plotted the change in norms of the *activation vectors*. Now, you will examine the change in the weights themselves. Create a version of the above function that runs a single gradient step, then for each dense layer plot:\n",
        "- The *Frobenius* norm of the update.\n",
        "- The *spectral* norm of the update.\n",
        "- The *RMS-RMS induced norm* of the update.\n",
        "\n",
        "Which one of these norms correlates the most with the RMS norms of the activations?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "You should calculate your updates as `new_dense_parameter - old_dense_parameter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "lOp2ISs2NfA9",
        "outputId": "db77a082-8aeb-4328-8185-5325d85d0eb8"
      },
      "outputs": [],
      "source": [
        "### Solution\n",
        "batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "def train_one_step_matrices(mlp=MLP, hiddens=[8, 16, 64, 64, 64, 256, 256, 1024], optimizer=SimpleAdam, label=\"Adam\", lr=0.01):\n",
        "    model = mlp(hidden_sizes=hiddens).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer(model.parameters(), lr=lr)\n",
        "\n",
        "    old_params = [p.detach().clone() for p in model.parameters()]\n",
        "\n",
        "    for i in range(1):\n",
        "        images_batch = train_images[batch_idx]\n",
        "        labels_batch = train_labels[batch_idx]\n",
        "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, activations = model(images_batch)\n",
        "        loss = criterion(outputs, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    new_params = [p.detach().clone() for p in model.parameters()]\n",
        "    delta_params = [new_p - old_p for new_p, old_p in zip(new_params, old_params)]\n",
        "\n",
        "    frob_norms = []\n",
        "    spectral_norms = []\n",
        "    induced_norms = []\n",
        "    p_shapes = []\n",
        "\n",
        "    for p in delta_params:\n",
        "        if len(p.shape) == 2:\n",
        "            ### TODO: Log the respective norms.\n",
        "            pass\n",
        "            ###\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "    axs[0].set_title(f'Frobenius norm of update per layer ({label})')\n",
        "    axs[0].set_xlabel('Hidden Size of layer')\n",
        "    axs[0].bar(np.arange(len(frob_norms)), frob_norms)\n",
        "    axs[0].set_xticks(np.arange(len(frob_norms)), p_shapes, rotation=45, ha='right')\n",
        "\n",
        "    axs[1].set_title(f'Spectral norm of update per layer ({label})')\n",
        "    axs[1].set_xlabel('Hidden Size of layer')\n",
        "    axs[1].bar(np.arange(len(spectral_norms)), spectral_norms)\n",
        "    axs[1].set_xticks(np.arange(len(spectral_norms)), p_shapes, rotation=45, ha='right')\n",
        "\n",
        "    axs[2].set_title(f'Induced norm of update per layer ({label})')\n",
        "    axs[2].set_xlabel('Hidden Size of layer')\n",
        "    axs[2].bar(np.arange(len(induced_norms)), induced_norms)\n",
        "    axs[2].set_xticks(np.arange(len(induced_norms)), p_shapes, rotation=45, ha='right')\n",
        "    plt.show()\n",
        "train_one_step_matrices(optimizer=SimpleAdam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## c. Lion: A Sign-based Optimizer\n",
        "\n",
        "In this section, you will implement a simplified **Lion**-style optimizer and compare its behavior to `SimpleAdam` on the same MLP setup as above.\n",
        "\n",
        "The full Lion optimizer (Chen et al., 2023) is described in the paper:\n",
        "\n",
        "- Lion: Sign-based optimizer with memory. [[arXiv:2302.06675](https://arxiv.org/abs/2302.06675)]\n",
        "\n",
        "In our simplified version, we keep an exponential-moving-average momentum buffer\n",
        "$\\mathbf{m}_t$ and use only its **sign** to form the update:\n",
        "\n",
        "$$\n",
        "\\mathbf{m}_t = \\beta_1 \\mathbf{m}_{t-1} + (1 - \\beta_1)\\, \\mathbf{g}_t,\\\\\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\, \\operatorname{sign}(\\mathbf{m}_t),\n",
        "$$\n",
        "\n",
        "where $\\mathbf{g}_t$ is the gradient at step $t$, $\\beta_1$ is the momentum\n",
        "parameter, and $\\eta$ is the learning rate.\n",
        "\n",
        "We will:\n",
        "\n",
        "1. Implement a `SimpleLion` optimizer class, similar to `SimpleAdam`.\n",
        "2. Use `train_one_step` to compare Lion and Adam in terms of RMS activation deltas per layer.\n",
        "3. Perform a small hyperparameter sweep over Lion's learning rate and momentum using the provided code.\n",
        "\n",
        "Below, fill in the TODOs in the `SimpleLion` class. Then follow the instructions in the subsequent markdown cells to run the experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "from typing import Any\n",
        "\n",
        "class SimpleLion(Optimizer):\n",
        "    \"\"\"Simplified Lion-style optimizer.\n",
        "\n",
        "    This version keeps an exponential-moving-average momentum buffer m\n",
        "    and applies sign-based updates using that momentum.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Any,\n",
        "        lr: float = 1e-1,\n",
        "        b1: float = 0.9,\n",
        "    ):\n",
        "        defaults = dict(lr=lr, b1=b1)\n",
        "        super(SimpleLion, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr = group[\"lr\"]\n",
        "            b1 = group[\"b1\"]\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:  # initialization\n",
        "                    state[\"momentum\"] = torch.zeros_like(p)\n",
        "\n",
        "                m = state[\"momentum\"]\n",
        "                ################################################################\n",
        "                # TODO: Implement the Lion-style step in three steps:           #\n",
        "                #   1) Update the momentum buffer m using grad and b1.         #\n",
        "                #   2) Compute an update tensor u based on sign(m).            #\n",
        "                #   3) Apply u to p with learning rate lr.                     #\n",
        "                ################################################################\n",
        "                # YOUR CODE HERE                                               #\n",
        "                ################################################################\n",
        "                pass\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### c.1 Comparing activation deltas for Adam vs Lion\n",
        "\n",
        "Once you have implemented `SimpleLion`, compare how **activation deltas** behave under Adam vs Lion.\n",
        "\n",
        "1. Re-run `train_one_step` with `optimizer=SimpleLion` and the same hidden sizes and learning rate as the Adam experiment:\n",
        "\n",
        "   ```python\n",
        "   train_one_step(optimizer=SimpleLion, label=\"Lion\", lr=0.01)\n",
        "   ```\n",
        "\n",
        "2. On the same figure (or side-by-side), compare the RMS of activation deltas per layer for:\n",
        "\n",
        "   - `SimpleAdam` (from part a)\n",
        "   - `SimpleLion` (this part)\n",
        "\n",
        "In your written submission, briefly answer:\n",
        "\n",
        "- Which layers show the largest change under Lion vs Adam?\n",
        "- Does Lion tend to make activation deltas more uniform or more peaked across layers?\n",
        "- How might this behavior be related to using only the **sign** of the momentum in the update?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### c.2 Hyperparameter sweep for Lion\n",
        "\n",
        "Next, run the **hyperparameter sweep** for Lion using the code cell directly below.\n",
        "\n",
        "- The sweep considers learning rates `lr` in `[0.001, 0.003, 0.01, 0.03]` and momentum parameters `b1` in `[0.8, 0.9, 0.95]` for a 3-layer MLP of width 64.\n",
        "- The code will report the mean validation loss for each `(lr, b1)` pair and plot the results.\n",
        "\n",
        "In your written submission, report one reasonably good `(lr, b1)` pair and comment (very briefly) on how sensitive Lion seems to be to the learning rate and momentum in this setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter sweep for Lion\n",
        "\n",
        "# This cell sweeps over lr and b1 for SimpleLion on a 3-layer MLP of width 64,\n",
        "# reports the mean validation loss, and plots the results.\n",
        "\n",
        "def train_with_lr_lion(hiddens=[64, 64, 64], lr=0.01, b1=0.9):\n",
        "    \"\"\"Train a 3-layer MLP with SimpleLion and return mean valid loss.\n",
        "\n",
        "    This mirrors `train_with_lr` but exposes both lr and b1.\n",
        "    \"\"\"\n",
        "    torch.manual_seed(4)\n",
        "    np.random_seed(4)\n",
        "    model = MLP(hidden_sizes=hiddens).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = SimpleLion(model.parameters(), lr=lr, b1=b1)\n",
        "    losses = []\n",
        "\n",
        "    for i in range(100):\n",
        "        batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "\n",
        "        images_batch = train_images[batch_idx]\n",
        "        labels_batch = train_labels[batch_idx]\n",
        "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(images_batch)\n",
        "        loss = criterion(outputs, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_valid, _ = model(valid_images)\n",
        "            valid_losses = criterion(outputs_valid, valid_labels)\n",
        "            losses.append(valid_losses.item())\n",
        "\n",
        "    return np.mean(np.array(losses)[-5:])\n",
        "\n",
        "\n",
        "lrs = [0.001, 0.003, 0.01, 0.03]\n",
        "b1s = [0.8, 0.9, 0.95]\n",
        "results = np.zeros((len(b1s), len(lrs)))\n",
        "\n",
        "for i, b1 in enumerate(b1s):\n",
        "    for j, lr in enumerate(lrs):\n",
        "        results[i, j] = train_with_lr_lion(hiddens=[64, 64, 64], lr=lr, b1=b1)\n",
        "\n",
        "print(\"Mean validation loss for each (b1, lr):\")\n",
        "for i, b1 in enumerate(b1s):\n",
        "    for j, lr in enumerate(lrs):\n",
        "        print(f\"b1={b1}, lr={lr}: loss={results[i, j]:.3f}\")\n",
        "\n",
        "# Plot results: loss vs lr, one curve per b1\n",
        "fig, ax = plt.subplots(1, figsize=(8, 4))\n",
        "for i, b1 in enumerate(b1s):\n",
        "    ax.plot(np.arange(len(lrs)), results[i], marker='o', label=f\"b1={b1}\")\n",
        "ax.set_title(\"Lion: mean validation loss vs learning rate\")\n",
        "ax.set_xlabel(\"learning rate\")\n",
        "ax.set_ylabel(\"mean validation loss\")\n",
        "ax.set_xticks(np.arange(len(lrs)))\n",
        "ax.set_xticklabels(lrs)\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYleV00ERYKW"
      },
      "source": [
        "## d. Implementing muP\n",
        "\n",
        "We will now implement muP scaling. Modify the starter code below to set a per-layer learning rate such that the resulting RMS activation-deltas are uniform scale, regardless of the layer widths. Plot the resulting activation-deltas on at least two sets of widths.\n",
        "\n",
        "Note: Even with the correct scaling, the first 2-3 activation-deltas may have a lower norm than the rest. Can you think of a reason why this might be the case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "Kq8g0H4bYF2W",
        "outputId": "2f14eb42-fb8d-4f7f-f469-b3fdcb34064f"
      },
      "outputs": [],
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "from typing import Any\n",
        "class SimpleAdamMuP(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Any,\n",
        "        lr: float = 1e-1,\n",
        "        b1: float = 0.9,\n",
        "        b2: float = 0.999,\n",
        "    ):\n",
        "        defaults = dict(lr=lr, b1=b1, b2=b2,)\n",
        "        super(SimpleAdamMuP, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0: # Initialization\n",
        "                    state[\"step\"] = torch.tensor(0.0)\n",
        "                    state['momentum'] = torch.zeros_like(p)\n",
        "                    state['variance'] = torch.zeros_like(p)\n",
        "\n",
        "                state['step'] += 1\n",
        "                m = state['momentum']\n",
        "                m.lerp_(grad, 1-group[\"b1\"])\n",
        "                v = state['variance']\n",
        "                v.lerp_(grad**2, 1-group[\"b2\"])\n",
        "\n",
        "                m_hat = m / (1 - group[\"b1\"]**state['step'])\n",
        "                v_hat = v / (1 - group[\"b2\"]**state['step'])\n",
        "                u = m_hat / (torch.sqrt(v_hat) + 1e-16)\n",
        "\n",
        "                ############################\n",
        "                ### Todo: Adjust the per-layer learning rate scaling factor so per-layer RMS activation deltas are constant.\n",
        "                ### Hint for part f: The following tricks will help you retain performance when using muP scaling.\n",
        "                ###  - Treat biases as a hidden layer with size (d_out, 1). You will need to use a fudge-factor of around 0.01 -- we want to keep the change in bias terms low.\n",
        "                ###  - For the input layer, a fudge factor of 10 appears to help.\n",
        "                ###  - For the output layer, we find it is best to ignore the muP scaling, and instead use a fixed learning rate (e.g. 0.003).\n",
        "                ############################\n",
        "                lr = group['lr']\n",
        "                pass\n",
        "                ############################\n",
        "                ############################\n",
        "\n",
        "                p.add_(u, alpha=-lr)\n",
        "        return None\n",
        "train_one_step(optimizer=SimpleAdamMuP, lr=2, label=\"Adam MuP\", hiddens=[8, 16, 64, 64, 64, 256, 256, 1024])\n",
        "train_one_step(optimizer=SimpleAdamMuP, lr=2, label=\"Adam MuP\", hiddens=[8, 16, 32, 64, 128, 256, 512, 1024])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GznnkrDAjV1e"
      },
      "source": [
        "## e. Per-Weight Multipliers\n",
        "\n",
        "An alternative way to implement muP is to adjust the *network graph* itself, rather than the optimizer. Implement this below, and recreate the above uniformly-scaled graph when using the *Adam* (not muP) optimizer. We have disabled biases to simplify the problem.\n",
        "\n",
        "Why is multiplying the output of a layer by a constant the same as adjusting the learning-rate of that layer (when using Adam or SignGD)?\n",
        "\n",
        "*Answer:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "9XueS5-OjoMh",
        "outputId": "0e730124-22e4-41ec-9417-ba2604613de6"
      },
      "outputs": [],
      "source": [
        "class ScaledMLP(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_sizes = [8, 16, 32, 64, 128], num_classes=10):\n",
        "        super(ScaledMLP, self).__init__()\n",
        "        all_hidden_sizes = [input_size] + hidden_sizes + [num_classes]\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(all_hidden_sizes)-1):\n",
        "            self.layers.append(nn.Linear(all_hidden_sizes[i], all_hidden_sizes[i+1], bias=False))\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        activations = []\n",
        "        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 28*28)\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = layer(x)\n",
        "            ## TODO\n",
        "            pass\n",
        "            ##\n",
        "            x = self.sigmoid(x)\n",
        "            activations.append(x)\n",
        "        x = self.layers[-1](x)\n",
        "        activations = activations[1:]\n",
        "        return x, [a.detach() for a in activations]\n",
        "\n",
        "train_one_step(mlp=ScaledMLP, optimizer=SimpleAdam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHHRVpQMiEvS"
      },
      "source": [
        "## f. Hyperparameter Transfer\n",
        "\n",
        "Run the following code, which will perform a sweep over learning rates for 3-layer MLPs of increasing width using Adam. How does the optimal learning rate change as the network increases in size?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "In the second cell, we will instead use the muP optimizer you implemented. How does the optimal learning rate work now? You should aim to show that there is a single global learning rate that works on a majority of widths. The 256-width network should achieve a loss of 0.5, comparable to Adam.\n",
        "\n",
        "*Answer:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_z3UhD8Ytts"
      },
      "outputs": [],
      "source": [
        "valid_idx = np.random.randint(0, len(train_images), size=64)\n",
        "valid_images = train_images[valid_idx]\n",
        "valid_labels = train_labels[valid_idx]\n",
        "valid_images, valid_labels = valid_images.to(device), valid_labels.to(device)\n",
        "\n",
        "\n",
        "def train_with_lr(hiddens=[64, 64, 64], optimizer=SimpleAdam, lr=0.01):\n",
        "    torch.manual_seed(4)\n",
        "    np.random.seed(4)\n",
        "    model = MLP(hidden_sizes=hiddens).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer(model.parameters(), lr=lr)\n",
        "    losses = []\n",
        "\n",
        "    for i in range(100):\n",
        "        batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "\n",
        "        images_batch = train_images[batch_idx]\n",
        "        labels_batch = train_labels[batch_idx]\n",
        "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(images_batch)\n",
        "        loss = criterion(outputs, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_valid, _ = model(valid_images)\n",
        "            valid_losses = criterion(outputs_valid, valid_labels)\n",
        "            losses.append(valid_losses.item())\n",
        "\n",
        "    return np.mean(np.array(losses)[-5:])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "dEIPwU8SZtIz",
        "outputId": "c9ac0405-54a6-427f-f810-345d91a9ea29"
      },
      "outputs": [],
      "source": [
        "all_widths = [4, 8, 16, 32, 64, 128, 256]\n",
        "all_lrs = [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0]\n",
        "adam_results = np.zeros((len(all_widths), len(all_lrs)))\n",
        "for wi, width in enumerate(all_widths):\n",
        "    for lri, lr in enumerate(all_lrs):\n",
        "        loss = train_with_lr(hiddens=[width, width, width], lr=lr)\n",
        "        adam_results[wi, lri] = loss\n",
        "\n",
        "fig, axs = plt.subplots(1, figsize=(8, 4))\n",
        "axs.set_title(f'Loss per learning rate (Adam)')\n",
        "for wi, width in enumerate(all_widths):\n",
        "    axs.plot(np.arange(len(all_lrs)), adam_results[wi], label=f'Width: {width}')\n",
        "axs.set_xticks(np.arange(len(all_lrs)))\n",
        "axs.set_xticklabels(all_lrs)\n",
        "axs.set_ylim(bottom=0, top=3)\n",
        "axs.legend()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "cTHetldFdMOP",
        "outputId": "d90a7f8b-7191-4b1a-e44b-4c4cce3db41d"
      },
      "outputs": [],
      "source": [
        "all_widths = [4, 8, 16, 32, 64, 128, 256]\n",
        "all_lrs = [0.03, 0.1, 0.3, 1.0, 3.0, 10.0]\n",
        "mup_results = np.zeros((len(all_widths), len(all_lrs)))\n",
        "for wi, width in enumerate(all_widths):\n",
        "    for lri, lr in enumerate(all_lrs):\n",
        "        loss = train_with_lr(hiddens=[width, width, width], lr=lr, optimizer=SimpleAdamMuP)\n",
        "        mup_results[wi, lri] = loss\n",
        "\n",
        "fig, axs = plt.subplots(1, figsize=(8, 4))\n",
        "axs.set_title(f'Loss per learning rate (muP)')\n",
        "for wi, width in enumerate(all_widths):\n",
        "    axs.plot(np.arange(len(all_lrs)), mup_results[wi], label=f'Width: {width}')\n",
        "axs.set_xticks(np.arange(len(all_lrs)))\n",
        "axs.set_xticklabels(all_lrs)\n",
        "axs.set_ylim(bottom=0, top=3)\n",
        "axs.legend()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaqzanKomMTx"
      },
      "source": [
        "## g. Shampoo and Orthogonalization\n",
        "\n",
        "In lecture, we discussed a simplified version of the Shampoo update, which can be viewed as *orthogonalizing* the update to a dense layer. In the following code block, implement this simplified Shampoo update:\n",
        "\n",
        "$$\n",
        "momentum \\rightarrow U \\Sigma V^T. \\qquad update = UV^T.\n",
        "$$\n",
        "\n",
        "Feel free to use linear algebra functions such as `torch.linalg.svd`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUgzUqQ9nALJ"
      },
      "outputs": [],
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "from typing import Any\n",
        "class SimpleShampoo(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Any,\n",
        "        lr: float = 1e-1,\n",
        "        b1: float = 0.9,\n",
        "    ):\n",
        "        defaults = dict(lr=lr, b1=b1)\n",
        "        super(SimpleShampoo, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0: # Initialization\n",
        "                    state[\"step\"] = torch.tensor(0.0)\n",
        "                    state['momentum'] = torch.zeros_like(p)\n",
        "\n",
        "                state['step'] += 1\n",
        "                m = state['momentum']\n",
        "                m.lerp_(grad, 1-group[\"b1\"])\n",
        "\n",
        "                ############ TODO\n",
        "                if len(m.shape) == 1:\n",
        "                    u = m # Ignore biases for this question, it's not important.\n",
        "                else:\n",
        "                    pass\n",
        "                #############\n",
        "                p.add_(u, alpha=-group['lr'])\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIoTUgUYmvJt"
      },
      "source": [
        "Now, we will examine the relationship between the Frobenius norm and the Spectral norm for Adam vs. Shampoo. Plot these norms using your code from part **b**. What relationship do you see? Can you come up for a reason why this makes sense?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "Bonus: How should we scale the Shampoo update so the *induced RMS-RMS norm* is equal? Implement this change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2pcGr3WloaGw",
        "outputId": "0eb1e796-4529-4f1b-aae7-8fb91a5c9be4"
      },
      "outputs": [],
      "source": [
        "train_one_step_matrices(optimizer=SimpleAdam)\n",
        "train_one_step_matrices(optimizer=SimpleShampoo, label=\"Shampoo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## h. SOAP-style Optimizer\n",
        "\n",
        "SOAP is a matrix-whitening optimizer closely related to Shampoo. For more details,\n",
        "see for example:\n",
        "\n",
        "- *Understanding SOAP from the Perspective of Gradient Whitening*. [[arXiv:2509.22938](https://arxiv.org/abs/2509.22938)]\n",
        "\n",
        "In this part, you will implement a simple **SOAP-style** optimizer that:\n",
        "\n",
        "1. Orthogonalizes the momentum using SVD, like Shampoo.\n",
        "2. Rescales the orthogonal update so that its **Frobenius norm matches** that of the original momentum.\n",
        "\n",
        "Concretely, for a matrix momentum $m$ at step $t$:\n",
        "\n",
        "$$\n",
        " m_t \\xrightarrow{\\text{SVD}} U_t \\Sigma_t V_t^T, \\quad\n",
        " u_{0,t} = U_t V_t^T, \\quad\n",
        " u_t = u_{0,t} \\cdot \\frac{\\lVert m_t \\rVert_F}{\\lVert u_{0,t} \\rVert_F}.\n",
        "$$\n",
        "\n",
        "Below, fill in the TODOs in the `SimpleSOAP` class. Then you will compare its\n",
        "update norms to those of Adam and Shampoo using your `train_one_step_matrices` helper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "from typing import Any\n",
        "\n",
        "class SimpleSOAP(Optimizer):\n",
        "    \"\"\"Simplified SOAP-style optimizer.\n",
        "\n",
        "    Uses Shampoo-style orthogonalization (via SVD) and then rescales the\n",
        "    orthogonal update so that its Frobenius norm matches that of the\n",
        "    original momentum.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Any,\n",
        "        lr: float = 1e-1,\n",
        "        b1: float = 0.9,\n",
        "    ):\n",
        "        defaults = dict(lr=lr, b1=b1)\n",
        "        super(SimpleSOAP, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr = group[\"lr\"]\n",
        "            b1 = group[\"b1\"]\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:  # initialization\n",
        "                    state[\"step\"] = torch.tensor(0.0)\n",
        "                    state[\"momentum\"] = torch.zeros_like(p)\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "                m = state[\"momentum\"]\n",
        "                m.lerp_(grad, 1 - b1)\n",
        "\n",
        "                if len(m.shape) == 1:\n",
        "                    # For biases, just use the momentum directly\n",
        "                    u = m\n",
        "                else:\n",
        "                    ################################################################\n",
        "                    # TODO: Implement the SOAP-style matrix update:                #\n",
        "                    #   1) Compute SVD of m                                       #\n",
        "                    #   2) Form u0 = U @ Vh                                       #\n",
        "                    #   3) Rescale u0 so that ||u||_F = ||m||_F                   #\n",
        "                    ################################################################\n",
        "                    # YOUR CODE HERE                                               #\n",
        "                    ################################################################\n",
        "                    pass\n",
        "                p.add_(u, alpha=-lr)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### h.1 Comparing update norms for Adam, Shampoo, and SOAP\n",
        "\n",
        "Use your `train_one_step_matrices` helper to compare update norms across optimizers.\n",
        "\n",
        "1. Run the following calls (you may want to reuse the same `batch_idx` for fairness):\n",
        "\n",
        "   ```python\n",
        "   train_one_step_matrices(optimizer=SimpleAdam, label=\"Adam\")\n",
        "   train_one_step_matrices(optimizer=SimpleShampoo, label=\"Shampoo\")\n",
        "   train_one_step_matrices(optimizer=SimpleSOAP, label=\"SOAP\")\n",
        "   ```\n",
        "\n",
        "2. For each optimizer, inspect the bar plots of:\n",
        "\n",
        "   - Frobenius norm of the update per layer.\n",
        "   - Spectral norm of the update per layer.\n",
        "   - RMS-RMS induced norm of the update per layer.\n",
        "\n",
        "In your written submission, briefly discuss:\n",
        "\n",
        "- How do the **Frobenius norms** of SOAP compare to those of Adam and Shampoo?\n",
        "- How do the **spectral norms** compare?\n",
        "- Which optimizer (Adam, Shampoo, SOAP) seems to give update norms that best match the **activation-delta RMS** patterns from part (a)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### h.2 Small learning-rate comparison for Shampoo vs SOAP\n",
        "\n",
        "In this part, you will run the cell below, which performs a\n",
        "small learning-rate sweep for `SimpleShampoo` and `SimpleSOAP` on a 3-layer MLP\n",
        "of width 64.\n",
        "\n",
        "The code will:\n",
        "- Sweep learning rates `lr` in `[0.001, 0.003, 0.01, 0.03]` for each optimizer.\n",
        "- Report the mean validation loss for each `(optimizer, lr)` pair.\n",
        "\n",
        "In your written submission, briefly answer:\n",
        "- Which learning-rate range works reasonably well for **Shampoo**?\n",
        "- Which range works well for **SOAP**?\n",
        "- Does preserving the Frobenius norm in SOAP appear to make learning-rate tuning\n",
        "  easier or harder compared to Shampoo?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Small learning-rate comparison for Shampoo vs SOAP\n",
        "\n",
        "# This cell sweeps over lr for SimpleShampoo and SimpleSOAP on a 3-layer MLP\n",
        "# of width 64, and reports the mean validation loss for each.\n",
        "\n",
        "\n",
        "def train_with_lr_opt(hiddens=[64, 64, 64], optimizer_cls=SimpleShampoo, lr=0.01):\n",
        "    torch.manual_seed(4)\n",
        "    np.random.seed(4)\n",
        "    model = MLP(hidden_sizes=hiddens).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer_cls(model.parameters(), lr=lr)\n",
        "    losses = []\n",
        "\n",
        "    for i in range(100):\n",
        "        batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "\n",
        "        images_batch = train_images[batch_idx]\n",
        "        labels_batch = train_labels[batch_idx]\n",
        "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(images_batch)\n",
        "        loss = criterion(outputs, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_valid, _ = model(valid_images)\n",
        "            valid_losses = criterion(outputs_valid, valid_labels)\n",
        "            losses.append(valid_losses.item())\n",
        "\n",
        "    return np.mean(np.array(losses)[-5:])\n",
        "\n",
        "\n",
        "lrs = [0.001, 0.003, 0.01, 0.03]\n",
        "optims = [SimpleShampoo, SimpleSOAP]\n",
        "\n",
        "for opt_cls in optims:\n",
        "    print(f\"Results for {opt_cls.__name__}:\")\n",
        "    for lr in lrs:\n",
        "        loss = train_with_lr_opt(hiddens=[64, 64, 64], optimizer_cls=opt_cls, lr=lr)\n",
        "        print(f\"  lr={lr}: mean valid loss = {loss:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

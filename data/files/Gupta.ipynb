{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS182: Deep Neural Networks - Assignment on CNNs and Advanced Optimizers\n",
    "\n",
    "**UC Berkeley - Fall 2025**\n",
    "\n",
    "In this assignment, you will:\n",
    "1. Build a standard training-validation loop for CNN models **(Part a - 20 points)**\n",
    "2. Implement and understand the Newton-Schulz (NS) iteration used in modern optimizers **(Part b - 40 points)**\n",
    "3. Implement the Lion optimizer and compare its performance with AdamW on ResNet18/CIFAR-10 **(Part c - 40 points)**\n",
    "\n",
    "**Important Notes:**\n",
    "- This notebook is designed to run on Google Colab with GPUs\n",
    "- All random seeds are set to 42 for reproducibility across runs  \n",
    "- PyTorch 2.0+ recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch version and upgrade if necessary\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# If version is less than 2.9, upgrade\n",
    "if torch.__version__ < '2.9':\n",
    "    !pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # for multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Random seed set to: {SEED}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Visualization\n",
    "\n",
    "We'll use the CIFAR-10 dataset, which consists of 60,000 32x32 color images in 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for CIFAR-10\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "\n",
    "# Split training set into train and validation\n",
    "# Using 40,000 train / 10,000 validation to match test set size\n",
    "train_size = 40000\n",
    "val_size = 10000\n",
    "\n",
    "# Use generator with fixed seed for reproducible split\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "trainset, valset = random_split(trainset, [train_size, val_size], generator=generator)\n",
    "\n",
    "print(f\"Training samples: {len(trainset)}\")\n",
    "print(f\"Validation samples: {len(valset)}\")\n",
    "print(f\"Test samples: {len(testset)}\")\n",
    "print(f\"\\nNote: Validation and test sets have equal size ({val_size} samples each)\")\n",
    "\n",
    "# CIFAR-10 classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training images\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get some random training images\n",
    "temp_loader = DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "dataiter = iter(temp_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    ax.imshow(np.transpose(images[idx].numpy() / 2 + 0.5, (1, 2, 0)))\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part (a): Build a Standard Training-Validation Loop\n",
    "\n",
    "Your task is to implement a general `train_validation_loop` function that can be used to train any model with any optimizer. This function will be reused in later parts of the assignment.\n",
    "\n",
    "**Requirements:**\n",
    "- The function should accept: model, train_loader, val_loader, optimizer, criterion, num_epochs, and device\n",
    "- Track and return both training and validation losses and accuracies for each epoch\n",
    "- Use tqdm for progress bars during training\n",
    "- Save the best model based on validation accuracy\n",
    "- Return a dictionary containing training history and the best model state\n",
    "\n",
    "**Deliverables:**\n",
    "1. Complete implementation of `train_validation_loop` function\n",
    "2. Test it with a simple CNN (provided below) on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN for testing (provided)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_loop(model, train_loader, val_loader, optimizer, criterion, \n",
    "                         num_epochs, device):\n",
    "    \"\"\"\n",
    "    General training and validation loop for PyTorch models.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        optimizer: PyTorch optimizer\n",
    "        criterion: Loss function\n",
    "        num_epochs: Number of epochs to train\n",
    "        device: Device to train on (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - 'train_losses': List of training losses per epoch\n",
    "            - 'val_losses': List of validation losses per epoch\n",
    "            - 'train_accs': List of training accuracies per epoch\n",
    "            - 'val_accs': List of validation accuracies per epoch\n",
    "            - 'best_model_state': State dict of best model (based on val accuracy)\n",
    "            - 'best_val_acc': Best validation accuracy achieved\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Initialize tracking variables\n",
    "    # - Lists to store losses and accuracies\n",
    "    # - Best validation accuracy and best model state\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # TODO: Implement training phase\n",
    "        # 1. Set model to training mode\n",
    "        # 2. Iterate through train_loader with tqdm\n",
    "        # 3. For each batch:\n",
    "        #    - Move data to device\n",
    "        #    - Zero gradients\n",
    "        #    - Forward pass\n",
    "        #    - Compute loss\n",
    "        #    - Backward pass\n",
    "        #    - Optimizer step\n",
    "        #    - Track running loss and accuracy\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # TODO: Implement validation phase\n",
    "        # 1. Set model to evaluation mode\n",
    "        # 2. A short step regarding gradient computation (what is it and why?)\n",
    "        # 3. Iterate through val_loader\n",
    "        # 4. Compute validation loss and accuracy\n",
    "        # 5. Save best model if current val accuracy is best so far\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # TODO: Print epoch statistics\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Return results dictionary\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation with SimpleCNN\n",
    "train_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(valset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize model, optimizer, and criterion\n",
    "simple_model = SimpleCNN().to(device)\n",
    "optimizer = torch.optim.Adam(simple_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train for a few epochs to test\n",
    "results = train_validation_loop(\n",
    "    model=simple_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    num_epochs=5,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results['train_losses'], label='Train Loss')\n",
    "ax1.plot(results['val_losses'], label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results['train_accs'], label='Train Acc')\n",
    "ax2.plot(results['val_accs'], label='Val Acc')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best Validation Accuracy: {results['best_val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Newton-Schulz (NS) Coefficients Visualization\n",
    "\n",
    "Before we dive into implementing optimizers, let's understand the Newton-Schulz iteration and how different coefficient sets affect the convergence behavior.\n",
    "\n",
    "### Background: What is Newton-Schulz and Why Orthogonalization Matters\n",
    "\n",
    "The Newton-Schulz iteration is a method for computing the **inverse square root** of a matrix, $X^{-1/2}$. The iteration uses a polynomial approximation:\n",
    "\n",
    "$$X_{k+1} = X_k(\\alpha I + \\beta X_k^2 + \\gamma X_k^4)$$\n",
    "\n",
    "**Why do we want the inverse square root?** Because when applied to a matrix, it produces an **orthogonal matrix** (or approximately orthogonal after sufficient iterations).\n",
    "\n",
    "### The Power of Orthogonalization in Optimization\n",
    "\n",
    "Orthogonal matrices have special properties that are crucial for stable and efficient neural network training:\n",
    "\n",
    "1. **Preserve Gradient Norms**: Orthogonal transformations preserve the magnitude of vectors. This prevents gradients from vanishing or exploding as they backpropagate through layers.\n",
    "\n",
    "2. **Balanced Learning Directions**: In the weight matrix, different directions (singular vectors) can have vastly different scales. Orthogonalization \"evens out\" these scales, ensuring all feature dimensions learn at comparable rates.\n",
    "\n",
    "3. **Condition Number Control**: The condition number of an orthogonal matrix is always 1 (optimal). This leads to better-conditioned optimization problems that converge more reliably.\n",
    "\n",
    "4. **Prevents Feature Collapse**: Without orthogonalization, weight matrices can develop redundant or highly correlated features. Orthogonalization encourages diverse, independent feature representations.\n",
    "\n",
    "### How Muon Uses Newton-Schulz\n",
    "\n",
    "In the Muon optimizer, the Newton-Schulz iteration is applied to **precondition the gradients**:\n",
    "- Instead of using gradients directly, Muon orthogonalizes them\n",
    "- This ensures gradient updates are well-balanced across all parameter dimensions\n",
    "- The NS coefficients control how aggressively this orthogonalization is performed\n",
    "\n",
    "### Two Coefficient Sets with Different Trade-offs\n",
    "\n",
    "1. **Aggressive (Keller) Coefficients**: α ≈ 3.4445, β ≈ -4.7750, γ ≈ 2.0315\n",
    "   - **Does not converge** (coefficients don't sum to 1)\n",
    "   - High initial slope (~3.44) aggressively inflates small singular values\n",
    "   - Pushes the matrix rapidly toward the orthogonal approximation range\n",
    "   - Best for: LLM pretraining and speedruns where you want maximum speed\n",
    "   - Trades mathematical convergence for rapid feature amplification\n",
    "   - Risk: May overshoot the orthogonal target, causing instability\n",
    "\n",
    "2. **Stable (Taylor) Coefficients**: α = 1.875, β = -1.25, γ = 0.375\n",
    "   - **Convergent** (coefficients sum to exactly 1)\n",
    "   - Gentle slope (1.875) for stable orthogonalization\n",
    "   - Iteratively refines the matrix toward perfect orthogonality\n",
    "   - Best for: Fine-tuning and stability-critical tasks\n",
    "   - Focuses on convergence and preserving learned structure\n",
    "   - Guarantees: Will eventually reach an orthogonal matrix if run indefinitely\n",
    "\n",
    "### The Orthogonalization Process\n",
    "\n",
    "The visualization below shows how these coefficients affect **singular values** over multiple NS iterations:\n",
    "- **Singular values** measure the \"stretch\" along different directions in the matrix\n",
    "- An orthogonal matrix has all singular values equal to 1\n",
    "- NS iterations gradually transform the singular value spectrum toward this ideal\n",
    "- Different coefficients change how aggressively this transformation happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part (b): Implement Newton-Schulz Iteration\n",
    "\n",
    "Now that you understand the theory, let's implement the Newton-Schulz iteration function.\n",
    "\n",
    "The Newton-Schulz iteration computes: $$X_{k+1} = X_k(\\alpha I + \\beta X_k^2 + \\gamma X_k^4)$$\n",
    "\n",
    "Your task is to implement this iteration step and visualize how different coefficient sets affect the convergence behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ns_iteration(X, alpha, beta, gamma):\n",
    "    \"\"\"\n",
    "    Perform one Newton-Schulz iteration step.\n",
    "    \n",
    "    Formula: X_{k+1} = X_k * (alpha * I + beta * X_k^2 + gamma * X_k^4)\n",
    "    \n",
    "    Args:\n",
    "        X: Current matrix (torch.Tensor)\n",
    "        alpha: Coefficient for identity term\n",
    "        beta: Coefficient for X^2 term  \n",
    "        gamma: Coefficient for X^4 term\n",
    "    \n",
    "    Returns:\n",
    "        X_{k+1}: Updated matrix after one NS iteration\n",
    "    \"\"\"\n",
    "    # TODO: Implement Newton-Schulz iteration\n",
    "    # 1. Compute X^2 = X @ X\n",
    "    # 2. Compute X^4 = X^2 @ X^2\n",
    "    # 3. Create identity matrix I with same shape as X\n",
    "    # 4. Compute polynomial: alpha * I + beta * X^2 + gamma * X^4\n",
    "    # 5. Return X @ polynomial\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def track_singular_values(A, coeffs, num_iters):\n",
    "    \"\"\"\n",
    "    Track singular values through multiple NS iterations.\n",
    "    \n",
    "    Args:\n",
    "        A: Initial matrix (torch.Tensor)\n",
    "        coeffs: Tuple of (alpha, beta, gamma) coefficients\n",
    "        num_iters: Number of iterations to perform\n",
    "    \n",
    "    Returns:\n",
    "        List of numpy arrays containing singular values at each iteration\n",
    "    \"\"\"\n",
    "    # TODO: Implement singular value tracking\n",
    "    # 1. Clone the input matrix A to X\n",
    "    # 2. Create empty list sv_history to store singular values\n",
    "    # 3. Loop num_iters times:\n",
    "    #    a. Compute SVD of X using torch.linalg.svd\n",
    "    #    b. Extract singular values S and convert to numpy, append to sv_history\n",
    "    #    c. Update X using ns_iteration with the given coefficients\n",
    "    # 4. Return sv_history\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation with both coefficient sets\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Define coefficient sets\n",
    "aggressive_coeffs = (3.4445, -4.7750, 2.0315)  # Keller coefficients\n",
    "stable_coeffs = (1.875, -1.25, 0.375)  # Taylor series coefficients\n",
    "\n",
    "# Create a random test matrix (using same seed for reproducibility)\n",
    "torch.manual_seed(SEED)\n",
    "n = 50\n",
    "A = torch.randn(n, n)\n",
    "A = A @ A.T + 0.1 * torch.eye(n)\n",
    "\n",
    "# Normalize to have singular values in [0, 1]\n",
    "U, S, Vt = torch.linalg.svd(A)\n",
    "S_normalized = S / S.max()\n",
    "A_normalized = U @ torch.diag(S_normalized) @ Vt\n",
    "\n",
    "num_iterations = 10\n",
    "\n",
    "# Track singular values for both coefficient sets\n",
    "aggressive_sv = track_singular_values(A_normalized, aggressive_coeffs, num_iterations)\n",
    "stable_sv = track_singular_values(A_normalized, stable_coeffs, num_iterations)\n",
    "\n",
    "print(\"Newton-Schulz iterations completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animated visualization showing NS iteration convergence\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "def animate(frame):\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    \n",
    "    # Aggressive coefficients\n",
    "    ax1.bar(range(len(aggressive_sv[frame])), aggressive_sv[frame], color='red', alpha=0.7)\n",
    "    ax1.set_ylim([0, max(aggressive_sv[-1].max(), stable_sv[-1].max()) * 1.1])\n",
    "    ax1.set_xlabel('Singular Value Index')\n",
    "    ax1.set_ylabel('Magnitude')\n",
    "    ax1.set_title(f'Aggressive Coefficients (Iteration {frame})')\n",
    "    ax1.axhline(y=1.0, color='black', linestyle='--', linewidth=2, alpha=0.5, label='Target')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    ax1.text(0.05, 0.95, 'Rapidly inflates small values\\nMay not converge', \n",
    "             transform=ax1.transAxes, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Stable coefficients\n",
    "    ax2.bar(range(len(stable_sv[frame])), stable_sv[frame], color='blue', alpha=0.7)\n",
    "    ax2.set_ylim([0, max(aggressive_sv[-1].max(), stable_sv[-1].max()) * 1.1])\n",
    "    ax2.set_xlabel('Singular Value Index')\n",
    "    ax2.set_ylabel('Magnitude')\n",
    "    ax2.set_title(f'Stable Coefficients (Iteration {frame})')\n",
    "    ax2.axhline(y=1.0, color='black', linestyle='--', linewidth=2, alpha=0.5, label='Target')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    ax2.text(0.05, 0.95, 'Gentle orthogonalization\\nConverges to equilibrium', \n",
    "             transform=ax2.transAxes, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=num_iterations, interval=500, repeat=True)\n",
    "plt.close()\n",
    "\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton-Schulz Analysis Questions\n",
    "\n",
    "Based on the visualization above, answer the following questions in the cell below:\n",
    "\n",
    "1. **Convergence Behavior**: Describe the difference in how singular values evolve for the aggressive (Keller) vs. stable (Taylor) coefficients. Which set brings singular values closer to 1.0 more quickly? Which set appears to converge to exactly 1.0?\n",
    "\n",
    "2. **Mathematical Connection**: The aggressive coefficients sum to α + β + γ ≈ 1.001 (approximately 1 but not exact), while the stable coefficients sum to exactly 1.0. How does this mathematical property relate to what you observe in the convergence behavior? Why might coefficients that sum to 1 guarantee convergence?\n",
    "\n",
    "3. **Overshoot vs. Stability**: In the aggressive coefficient visualization, do you observe any singular values that exceed 1.0 (overshoot the target)? What are the potential risks of this behavior when orthogonalizing gradient matrices in an optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWERS HERE:**\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Important Note: Muon's 2D Parameter Requirement\n",
    "\n",
    "Before moving on to Part (c), it's important to understand a key limitation of the Muon optimizer:\n",
    "\n",
    "**Muon requires EXACTLY 2D parameters**. According to the [PyTorch 2.9 documentation](https://docs.pytorch.org/docs/stable/generated/torch.optim.Muon.html):\n",
    "\n",
    "> \"Muon is an optimizer for 2D parameters of neural network hidden layers.\"\n",
    "\n",
    "The implementation checks: `if p.ndim != 2: raise ValueError(...)`\n",
    "\n",
    "**Parameter dimensionality in typical networks:**\n",
    "- **4D parameters**: Conv2d weights `[out_channels, in_channels, height, width]`\n",
    "- **2D parameters**: Linear layer weights `[out_features, in_features]`  \n",
    "- **1D parameters**: Biases, BatchNorm weights\n",
    "\n",
    "**Why this matters for CNNs:**\n",
    "Since most CNN parameters are 4D (convolutional layers), Muon cannot optimize them directly. This significantly limits Muon's applicability to pure CNN architectures like ResNet. While Muon excels at training Transformers and MLPs (which are dominated by 2D matrix multiplications), it's not well-suited for CNNs.\n",
    "\n",
    "**In the next section**, we'll implement the **Lion optimizer**, which works with parameters of any dimensionality and is much better suited for training CNNs like ResNet on CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part (c): Implement Lion Optimizer and Compare with AdamW\n",
    "\n",
    "In this section, you will:\n",
    "1. Implement the Lion optimizer from scratch based on the pseudocode\n",
    "2. Prepare ResNet18 for CIFAR-10 transfer learning (with ALL parameters trainable)\n",
    "3. Train ResNet18 with Lion using suggested hyperparameters\n",
    "4. Train ResNet18 with AdamW (SOTA baseline) for comparison\n",
    "5. Compare the performance of Lion vs AdamW\n",
    "\n",
    "### Lion Optimizer Background\n",
    "\n",
    "Lion (evoLved sIgn mOmeNtum) is a simple yet effective optimizer discovered through program search. Refer to the original paper: [Lion: Adversarial Learning with Momentum](https://arxiv.org/abs/2302.06675)\n",
    "\n",
    "The key update rules are:\n",
    "- Use the sign of the interpolation between momentum and gradient\n",
    "- Update momentum as EMA of gradients\n",
    "- Apply weight decay\n",
    "\n",
    "Unlike Muon, **Lion works with parameters of any dimensionality**, making it suitable for CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Implement Lion Optimizer\n",
    "\n",
    "Implement the Lion optimizer class following the algorithm from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lion(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Implements Lion optimizer.\n",
    "    \n",
    "    Based on the paper: https://arxiv.org/abs/2302.06675\n",
    "    \n",
    "    Args:\n",
    "        params: iterable of parameters to optimize\n",
    "        lr: learning rate (default: 1e-4)\n",
    "        beta1: coefficient for interpolation with momentum (default: 0.9)\n",
    "        beta2: coefficient for momentum EMA (default: 0.99)\n",
    "        weight_decay: weight decay coefficient (default: 0)\n",
    "    \n",
    "    Hint: Refer to PyTorch's Optimizer base class documentation and SGD source code\n",
    "    as examples for implementing custom optimizers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params, lr=1e-4, beta1=0.9, beta2=0.99, weight_decay=0.0):\n",
    "        # TODO: Validate hyperparameters and call parent constructor\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \n",
    "        Algorithm (from paper):\n",
    "        1. c_t = β₁ * m_{t-1} + (1 - β₁) * g_t  (interpolation)\n",
    "        2. θ_t = θ_{t-1} - η * (sign(c_t) + λ * θ_{t-1})  (param update)\n",
    "        3. m_t = β₂ * m_{t-1} + (1 - β₂) * g_t  (momentum update)\n",
    "        \n",
    "        Hint: Use self.state[p] to store per-parameter state (like momentum).\n",
    "        Refer to torch.sign() for the sign function.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # TODO: Implement the three steps above\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of Lion implementation\n",
    "# This will fail until you implement the Lion class above\n",
    "test_model = SimpleCNN().to(device)\n",
    "test_optimizer = Lion(test_model.parameters(), lr=1e-3)\n",
    "print(\"Lion optimizer created successfully!\")\n",
    "print(f\"Optimizer state: {test_optimizer.state_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare ResNet18 for CIFAR-10 Transfer Learning\n",
    "\n",
    "Now let's prepare a pretrained ResNet18 model for CIFAR-10. Since Lion works with parameters of any dimensionality, we can train ALL parameters (unlike Muon which only works with 2D parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "def prepare_resnet_for_cifar10(pretrained=True):\n",
    "    \"\"\"\n",
    "    Prepare ResNet18 for CIFAR-10 transfer learning.\n",
    "    \n",
    "    TODO: Complete this function\n",
    "    1. Load ResNet18 with pretrained ImageNet weights if pretrained=True\n",
    "       Hint: Use resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    2. Replace the final fully connected layer to output 10 classes instead of 1000\n",
    "       Hint: The final layer is model.fc, and it's a Linear layer\n",
    "       You'll need to check model.fc.in_features to get the input size\n",
    "    3. All parameters should remain trainable (this is the default, so no action needed)\n",
    "    \n",
    "    Args:\n",
    "        pretrained (bool): Whether to load ImageNet pretrained weights\n",
    "    \n",
    "    Returns:\n",
    "        model: Modified ResNet18 ready for CIFAR-10\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "test_resnet = prepare_resnet_for_cifar10(pretrained=True).to(device)\n",
    "print(f\"Final layer: {test_resnet.fc}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in test_resnet.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in test_resnet.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Hyperparameter Tuning for Lion\n",
    "\n",
    "Before comparing Lion with AdamW, we need to find the best hyperparameters for Lion. We'll perform a grid search over learning rate, batch size, and weight decay.\n",
    "\n",
    "**Search Space:**\n",
    "- Learning rates: [1e-4, 3e-4, 1e-3]\n",
    "- Batch sizes: [64, 128, 256]\n",
    "- Weight decays: [0.0, 0.01, 0.1]\n",
    "- Total configurations: 27 (3 × 3 × 3)\n",
    "- Training epochs per config: 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search space\n",
    "learning_rates = [1e-4, 3e-4, 1e-3]\n",
    "batch_sizes = [64, 128, 256]\n",
    "weight_decays = [0.0, 0.01, 0.1]\n",
    "\n",
    "# TODO: Implement hyperparameter grid search for Lion\n",
    "# 1. Create an empty list to store results: results_grid = []\n",
    "# 2. Calculate total_configs = len(learning_rates) * len(batch_sizes) * len(weight_decays)\n",
    "# 3. Create nested loops over learning_rates, batch_sizes, and weight_decays\n",
    "# 4. For each configuration:\n",
    "#    a. Print configuration number and hyperparameters\n",
    "#    b. Create DataLoaders with the current batch_size\n",
    "#    c. Initialize a fresh ResNet18 model using prepare_resnet_for_cifar10()\n",
    "#    d. Create Lion optimizer with current lr and weight_decay (use default betas)\n",
    "#    e. Train for 15 epochs using train_validation_loop\n",
    "#    f. Store results in a dictionary with keys: 'lr', 'batch_size', 'weight_decay', 'best_val_acc'\n",
    "#    g. Append to results_grid\n",
    "# 5. After all configs, print \"Hyperparameter search complete!\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Note: This will take significant time (~1-2 hours on GPU)\n",
    "# For testing, you can reduce the search space or num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Hyperparameter Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Analyze hyperparameter search results\n",
    "# 1. Create a DataFrame from results_grid\n",
    "# 2. Sort by 'best_val_acc' in descending order\n",
    "# 3. Display the top 5 configurations\n",
    "# 4. Extract the best configuration (first row after sorting)\n",
    "# 5. Print the best hyperparameters\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(f\"\\nBest Configuration:\")\n",
    "print(f\"Learning Rate: {best_config['lr']}\")\n",
    "print(f\"Batch Size: {best_config['batch_size']}\")\n",
    "print(f\"Weight Decay: {best_config['weight_decay']}\")\n",
    "print(f\"Best Val Acc: {best_config['best_val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter effects\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Learning rate effect\n",
    "lr_grouped = df_results.groupby('lr')['best_val_acc'].mean()\n",
    "axes[0].bar(range(len(lr_grouped)), lr_grouped.values)\n",
    "axes[0].set_xticks(range(len(lr_grouped)))\n",
    "axes[0].set_xticklabels([f'{lr:.0e}' for lr in lr_grouped.index])\n",
    "axes[0].set_xlabel('Learning Rate')\n",
    "axes[0].set_ylabel('Avg Val Accuracy (%)')\n",
    "axes[0].set_title('Effect of Learning Rate')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Batch size effect\n",
    "bs_grouped = df_results.groupby('batch_size')['best_val_acc'].mean()\n",
    "axes[1].bar(range(len(bs_grouped)), bs_grouped.values)\n",
    "axes[1].set_xticks(range(len(bs_grouped)))\n",
    "axes[1].set_xticklabels(bs_grouped.index)\n",
    "axes[1].set_xlabel('Batch Size')\n",
    "axes[1].set_ylabel('Avg Val Accuracy (%)')\n",
    "axes[1].set_title('Effect of Batch Size')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Weight decay effect\n",
    "wd_grouped = df_results.groupby('weight_decay')['best_val_acc'].mean()\n",
    "axes[2].bar(range(len(wd_grouped)), wd_grouped.values)\n",
    "axes[2].set_xticks(range(len(wd_grouped)))\n",
    "axes[2].set_xticklabels(wd_grouped.index)\n",
    "axes[2].set_xlabel('Weight Decay')\n",
    "axes[2].set_ylabel('Avg Val Accuracy (%)')\n",
    "axes[2].set_title('Effect of Weight Decay')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train with Best Lion Configuration\n",
    "\n",
    "Now let's train ResNet18 with the best hyperparameters we found for a full 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with best hyperparameters\n",
    "model_lion = prepare_resnet_for_cifar10(pretrained=True).to(device)\n",
    "\n",
    "# Create optimizer with best hyperparameters\n",
    "optimizer_lion = Lion(\n",
    "    model_lion.parameters(),\n",
    "    lr=best_config['lr'],\n",
    "    beta1=0.9,\n",
    "    beta2=0.99,\n",
    "    weight_decay=best_config['weight_decay']\n",
    ")\n",
    "\n",
    "# Create data loaders with best batch size\n",
    "train_loader_best = DataLoader(trainset, batch_size=int(best_config['batch_size']), \n",
    "                               shuffle=True, num_workers=2)\n",
    "val_loader_best = DataLoader(valset, batch_size=int(best_config['batch_size']), \n",
    "                             shuffle=False, num_workers=2)\n",
    "\n",
    "# Train for 20 epochs\n",
    "print(\"Training ResNet18 with best Lion hyperparameters...\")\n",
    "results_lion = train_validation_loop(\n",
    "    model=model_lion,\n",
    "    train_loader=train_loader_best,\n",
    "    val_loader=val_loader_best,\n",
    "    optimizer=optimizer_lion,\n",
    "    criterion=criterion,\n",
    "    num_epochs=20,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nLion (Best Config) - Best Validation Accuracy: {results_lion['best_val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Lion Model on Test Set\n",
    "\n",
    "Now let's evaluate the trained Lion model on the held-out test set to see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "def evaluate_on_test(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \n",
    "    TODO: Implement this function to evaluate a trained model on the test set.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_loader: DataLoader for test data\n",
    "        criterion: Loss function\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        test_loss: Average test loss\n",
    "        test_acc: Test accuracy (%)\n",
    "    \n",
    "    Hint: This should be similar to the validation phase in train_validation_loop:\n",
    "    1. Set model to evaluation mode\n",
    "    2. Do the same step regarding gradients that was done in validation phase\n",
    "    3. Iterate through test_loader with tqdm for progress tracking\n",
    "    4. For each batch:\n",
    "       - Move inputs and labels to device\n",
    "       - Forward pass\n",
    "       - Compute loss\n",
    "       - Track predictions and correct counts\n",
    "    5. Compute average loss and accuracy\n",
    "    6. Return test_loss and test_acc (as percentage)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Create test loader\n",
    "test_loader = DataLoader(testset, batch_size=int(best_config['batch_size']), \n",
    "                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Load best Lion model and evaluate\n",
    "model_lion.load_state_dict(results_lion['best_model_state'])\n",
    "lion_test_loss, lion_test_acc = evaluate_on_test(model_lion, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nLion Model - Test Set Performance:\")\n",
    "print(f\"Test Loss: {lion_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {lion_test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train ResNet18 with AdamW (Baseline)\n",
    "\n",
    "Now let's train the same model with AdamW, which is currently the most widely-used optimizer for deep learning. We'll use SOTA (state-of-the-art) hyperparameters for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model for AdamW training\n",
    "model_adamw = prepare_resnet_for_cifar10(pretrained=True).to(device)\n",
    "\n",
    "# AdamW hyperparameters (SOTA for ResNet transfer learning)\n",
    "optimizer_adamw = torch.optim.AdamW(\n",
    "    model_adamw.parameters(),\n",
    "    lr=3e-4,  # AdamW typically uses higher learning rates than Lion\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Use same batch size as Lion's best config for fair comparison\n",
    "# Note: train_loader_best and val_loader_best were created in the previous cell\n",
    "\n",
    "# Train for 20 epochs (same as Lion for fair comparison)\n",
    "print(\"Training ResNet18 with AdamW optimizer...\")\n",
    "results_adamw = train_validation_loop(\n",
    "    model=model_adamw,\n",
    "    train_loader=train_loader_best,\n",
    "    val_loader=val_loader_best,\n",
    "    optimizer=optimizer_adamw,\n",
    "    criterion=criterion,\n",
    "    num_epochs=20,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nAdamW - Best Validation Accuracy: {results_adamw['best_val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate AdamW Model on Test Set\n",
    "\n",
    "Let's also evaluate the trained AdamW model on the test set for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best AdamW model and evaluate on test set\n",
    "model_adamw.load_state_dict(results_adamw['best_model_state'])\n",
    "adamw_test_loss, adamw_test_acc = evaluate_on_test(model_adamw, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nAdamW Model - Test Set Performance:\")\n",
    "print(f\"Test Loss: {adamw_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {adamw_test_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Test Set Comparison:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Lion  - Test Acc: {lion_test_acc:.2f}%\")\n",
    "print(f\"AdamW - Test Acc: {adamw_test_acc:.2f}%\")\n",
    "print(f\"Difference: {abs(lion_test_acc - adamw_test_acc):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Lion vs AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Lion vs AdamW\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training Loss\n",
    "ax1.plot(results_lion['train_losses'], label='Lion', color='purple', linewidth=2)\n",
    "ax1.plot(results_adamw['train_losses'], label='AdamW', color='green', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "ax2.plot(results_lion['val_losses'], label='Lion', color='purple', linewidth=2)\n",
    "ax2.plot(results_adamw['val_losses'], label='AdamW', color='green', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Validation Loss Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Training Accuracy\n",
    "ax3.plot(results_lion['train_accs'], label='Lion', color='purple', linewidth=2)\n",
    "ax3.plot(results_adamw['train_accs'], label='AdamW', color='green', linewidth=2)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Accuracy (%)')\n",
    "ax3.set_title('Training Accuracy Comparison')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "ax4.plot(results_lion['val_accs'], label='Lion', color='purple', linewidth=2)\n",
    "ax4.plot(results_adamw['val_accs'], label='AdamW', color='green', linewidth=2)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Accuracy (%)')\n",
    "ax4.set_title('Validation Accuracy Comparison')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(f\"Lion    - Best Val Acc: {results_lion['best_val_acc']:.2f}%\")\n",
    "print(f\"AdamW   - Best Val Acc: {results_adamw['best_val_acc']:.2f}%\")\n",
    "print(f\"\\nDifference: {abs(results_lion['best_val_acc'] - results_adamw['best_val_acc']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Questions (Answer in the cell below):\n",
    "\n",
    "1. **Performance Comparison**: Which optimizer achieved better validation accuracy on CIFAR-10: Lion or AdamW? By how much?\n",
    "\n",
    "2. **Training Dynamics**: Describe the differences in training curves between Lion and AdamW. Which converged faster? Were there any notable differences in stability?\n",
    "\n",
    "3. **Hyperparameter Sensitivity**: What was the best learning rate found for Lion? How does it compare to AdamW's learning rate (3e-4)? Based on the algorithm differences (sign-based updates vs adaptive learning rates), why might Lion have different learning rate requirements?\n",
    "\n",
    "4. **Hyperparameter Effects**: Based on your grid search results, which hyperparameter (learning rate, batch size, or weight decay) had the most significant impact on Lion's performance? Why do you think this is? (Hint: Consider Lion's sign-based updates as 1-bit quantization of gradients, which benefits from larger batch sizes to reduce stochastic noise.)\n",
    "\n",
    "5. **Memory and Computation**: Consider the implementation details of both optimizers. Which one is more memory-efficient? Why? (Hint: Think about what state each optimizer needs to maintain.)\n",
    "\n",
    "6. **When to Use Each**: Based on your results and understanding of both optimizers, in what scenarios would you choose Lion over AdamW, and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWERS HERE:**\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. \n",
    "\n",
    "4. \n",
    "\n",
    "5. \n",
    "\n",
    "6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this assignment, you:\n",
    "1. Built a reusable training-validation loop for deep learning models **(Part a)**\n",
    "2. Understood Newton-Schulz coefficients and their role in optimization **(Part b)**\n",
    "3. Implemented the ns_iteration and track_singular_values functions to visualize how different NS coefficients affect convergence behavior **(Part b)**\n",
    "4. Learned about Muon's 2D parameter limitation and why it's not suitable for pure CNN architectures\n",
    "5. Implemented the Lion optimizer from scratch **(Part c)**\n",
    "6. Prepared ResNet18 for CIFAR-10 transfer learning with all parameters trainable **(Part c)**\n",
    "7. Performed systematic hyperparameter tuning for Lion optimizer using grid search **(Part c)**\n",
    "8. Compared the best Lion configuration vs AdamW performance on ResNet18/CIFAR-10 **(Part c)**\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Different optimizers have different strengths: Muon for Transformers/MLPs, Lion and AdamW for general use\n",
    "- Understanding the mathematical foundations (like Newton-Schulz iterations) helps explain optimizer behavior\n",
    "- Modern optimizers often make trade-offs between convergence speed, stability, and computational efficiency\n",
    "- Hyperparameter tuning is crucial for achieving optimal performance with any optimizer\n",
    "- Empirical comparison is essential for choosing the right optimizer for your specific task\n",
    "\n",
    "**Submission Instructions:**\n",
    "1. Ensure all code cells run without errors\n",
    "2. Answer all analysis questions\n",
    "3. Export this notebook as PDF\n",
    "4. Submit both .ipynb and .pdf files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
